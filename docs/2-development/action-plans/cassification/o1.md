Below is an example of how we could design a self-contained classification microservice with support for “hot-swappable” classification models. The microservice uses a service class to handle classification logic, while allowing us to register or unregister different models at runtime. This design supports both rule-based (pattern matching) approaches and machine-learning models.

---

## High-Level Overview

1. We have an abstract base class (BaseClassificationModel) that defines the interface for all classification models.  
2. We implement one or more concrete classes (PatternModel, MLModel, etc.) that comply with the base class.  
3. The ClassificationService manages a collection of active models. We can add or remove models at runtime.  
4. The microservice exposes endpoints (via FastAPI, for instance) for:  
   - Classifying a document.  
   - Adding a new model (swapping in).  
   - Removing a model (swapping out).  
   - Retrieving current model statuses.  

Below is one example of the code structure. Where relevant, we refer to the information gleaned from the “classification-microservice.md” and other references:

---

```python
# classification_microservice.py
from typing import Dict, List, Optional, Any
from abc import ABC, abstractmethod
from dataclasses import dataclass
from enum import Enum
from fastapi import FastAPI, APIRouter

###############################################################################
# DATA MODELS
###############################################################################

class BaseType(str, Enum):
    NEW = "NEW"
    RENEW = "RENEW"
    TONNAGE = "TONNAGE"
    CERT = "CERT"
    LABEL = "LABEL"

@dataclass
class Document:
    content: str
    metadata: Dict[str, Any]

@dataclass
class Classification:
    year: Optional[int]
    term: Optional[str]
    state: Optional[str]
    client: Optional[str]
    base_type: Optional[BaseType]
    description: Optional[str] = None
    confidence: float = 0.0

###############################################################################
# ABSTRACT BASE CLASS FOR MODELS
###############################################################################

class BaseClassificationModel(ABC):
    """
    Abstract base for classification models. Each model must:
      1. Provide a classify() method taking a Document.
      2. Return a Classification object.
      3. Provide a model_name() property for identification.
    """

    @property
    @abstractmethod
    def name(self) -> str:
        """
        A unique name or identifier for the model.
        """
        pass

    @abstractmethod
    async def classify(self, document: Document) -> Classification:
        """
        Classify the given document asynchronously.
        """
        pass

###############################################################################
# EXAMPLE MODEL IMPLEMENTATIONS
###############################################################################

class PatternModel(BaseClassificationModel):
    """
    An example rule-based model. Uses patterns or dictionary lookups to
    infer classification.
    """

    def __init__(self, name: str, patterns: Dict[str, Any]):
        self._name = name
        self.patterns = patterns

    @property
    def name(self) -> str:
        return self._name

    async def classify(self, document: Document) -> Classification:
        # Simplistic pattern-based classification
        base_type = None
        confidence = 0.0

        content_lower = document.content.lower()

        # For illustration, we look for the word "renewal"
        if "renewal" in content_lower:
            base_type = BaseType.RENEW
            confidence = 0.8
        elif "new" in content_lower and "registration" in content_lower:
            base_type = BaseType.NEW
            confidence = 0.8

        # Return minimal classification
        return Classification(
            year=None,
            term=None,
            state=None,
            client=None,
            base_type=base_type,
            confidence=confidence
        )


class MLModel(BaseClassificationModel):
    """
    An example machine-learning model. In production, we'd load a model
    (e.g., from a file or a model registry) and run inference here.
    """

    def __init__(self, name: str, model_path: str):
        self._name = name
        self.model_path = model_path
        # Pseudo-load ML model from path, e.g. self.model = load_model(model_path)

    @property
    def name(self) -> str:
        return self._name

    async def classify(self, document: Document) -> Classification:
        # Hypothetical ML inference
        base_type = BaseType.LABEL
        confidence = 0.9
        # ...
        return Classification(
            year=2025,
            term="Q1",
            state="ME",
            client="DemoCorp",
            base_type=base_type,
            confidence=confidence
        )

###############################################################################
# CLASSIFICATION SERVICE
###############################################################################

class ClassificationService:
    """
    Manages multiple classification models. Incoming classify requests
    are routed to all (or some) of the models, and the results can be combined.
    """

    def __init__(self):
        # Keep an ordered list or dict of models
        self.models: Dict[str, BaseClassificationModel] = {}

    def add_model(self, model: BaseClassificationModel) -> None:
        """
        Hot-swap: add a new model at runtime.
        """
        self.models[model.name] = model

    def remove_model(self, model_name: str) -> None:
        """
        Hot-swap: remove a model at runtime.
        """
        if model_name in self.models:
            del self.models[model_name]

    def list_models(self) -> List[str]:
        """
        List the names of all actively loaded models.
        """
        return list(self.models.keys())

    async def classify_document(self, document: Document) -> Classification:
        """
        For demonstration, we run all models and pick the classification
        with the highest confidence. In reality, we might do blending or
        aggregator logic.
        """
        best_classification = Classification(
            year=None, term=None, state=None, client=None, base_type=None, confidence=0.0
        )

        for model in self.models.values():
            result = await model.classify(document)
            if result.confidence > best_classification.confidence:
                best_classification = result

        return best_classification

###############################################################################
# FASTAPI SETUP
###############################################################################

app = FastAPI(title="Classification Microservice")

router = APIRouter()
service = ClassificationService()

@router.post("/add_model")
async def add_model(model_type: str, name: str, model_path_or_patterns: Dict[str, Any]):
    """
    Endpoint to register a new model at runtime. If model_type='pattern', we
    expect a patterns dictionary. If 'ml', we expect a model_path.
    """
    if model_type == "pattern":
        patterns = model_path_or_patterns
        new_model = PatternModel(name, patterns)
        service.add_model(new_model)
        return {"status": "ok", "msg": f"Pattern model '{name}' added"}
    elif model_type == "ml":
        path = model_path_or_patterns.get("path")
        new_model = MLModel(name, path)
        service.add_model(new_model)
        return {"status": "ok", "msg": f"ML model '{name}' added"}
    else:
        return {"status": "error", "msg": "Unknown model_type"}

@router.delete("/remove_model/{model_name}")
async def remove_model(model_name: str):
    """
    Endpoint to remove an active model by name.
    """
    service.remove_model(model_name)
    return {"status": "ok", "msg": f"Model '{model_name}' removed"}

@router.get("/list_models")
async def list_models():
    """
    Endpoint to list all currently loaded models.
    """
    return {"models": service.list_models()}

@router.post("/classify")
async def classify_document(doc: Document):
    """
    Classify an incoming document using whatever models are active.
    """
    result = await service.classify_document(doc)
    return {"classification": result}

# Attach the router to the main FastAPI app
app.include_router(router, prefix="/api", tags=["classification"])

```

---

## Usage Summary

1. Start the microservice (e.g., “uvicorn classification_microservice:app --reload”).  
2. Use the /api/add_model endpoint to register either a pattern-based or ML-based model.  
3. Use /api/classify with a document payload to perform classification.  
4. If we want to swap out a model, we can remove it at /api/remove_model/<model_name> and add a different one.  
5. Check all current models via /api/list_models.  

This simple architecture allows us to combine multiple models. We can pick whichever classification result is highest in confidence, or unify results in more advanced ways. By exposing the “add_model” and “remove_model” endpoints, we have a hot-swappable design for both pattern-based and machine-learning approaches in a single microservice.

---

We hope this example helps illustrate a clean, modular approach to building a classification microservice with hot-swappable models.


Below is a proposed integration refactor of our directory tree, grouped by functionality and placing any new integration-specific work under an “integrations/” module. This structure helps keep our code organized by domain, making it easier to scale. We have tried not to disturb the existing structure too heavily, but we place any future external integrations, including PaLIGemma or Google Drive, under a dedicated integrations folder.

```bash
src
├── __init__.py
├── api/
│   └── (API Endpoints and Routers)
├── auto_labeler.py
├── classifiers/
│   ├── __init__.py
│   ├── base.py           # Contains shared classifier interfaces
│   ├── pattern.py        # Rule-based classification
│   ├── ml.py             # ML-based classification
│   └── combined.py       # Combined pattern+ML approach
├── client/
│   ├── __init__.py
│   ├── attachment.py
│   ├── gmail.py
│   ├── gmail_client_README.md
│   ├── label.py
│   ├── message.py
│   └── query.py
├── config/
│   ├── __init__.py
│   └── patterns/
│       ├── __init__.py
│       ├── states.py
│       ├── documents.py
│       ├── clients.py
│       └── dates.py
├── console.py
├── integrations/
│   ├── __init__.py
│   ├── paligemma/
│   │   ├── __init__.py
│   │   ├── model_loader.py         # e.g. logic for loading & refreshing PaLIGemma models
│   │   └── inference_service.py    # calls for model inference
│   ├── google_drive/
│   │   ├── __init__.py
│   │   └── drive_service.py        # example for future expansions
│   └── # Additional integration modules as needed
├── logging/
│   └── logger.py
├── main.py
├── models/
│   ├── __init__.py
│   ├── paligemma/
│   └── classification/
│       ├── __init__.py
│       ├── document.py
│       ├── classification.py
│       └── enums.py
├── parsers/
│   ├── __init__.py
│   ├── base.py
│   ├── state/
│   │   ├── __init__.py
│   │   ├── maine.py
│   │   └── california.py
│   └── document_types/
│       ├── __init__.py
│       ├── registration.py
│       ├── renewal.py
│       └── tonnage.py
├── services/
│   ├── __init__.py
│   ├── attachment_service.py
│   ├── audit_service.py
│   ├── classification_service.py
│   ├── content_extraction_service.py
│   ├── email_processing_service.py
│   ├── notification_service.py
│   ├── security_service.py
│   ├── storage_service.py
│   └── validation_service.py
└── utils/
    ├── __init__.py
    ├── data_extractor.py
    ├── email_labeler.py
    ├── email_sampler.py
    └── main.py
```

### Why This Refactoring Helps

1. “integrations/” Folder. Collecting all external integrations in one place makes it clear where to look for third-party systems like PaLIGemma or Google Drive. It also keeps our core business logic and services from becoming cluttered with one-off integration code.  
2. Clear Domain Separation. We retain existing domain-based folders like “classifiers/,” “parsers/,” and “services/,” so we preserve a separation of concerns.  
3. Easier Onboarding. Future team members can readily identify where external connectors live, how to add or remove them, and how they interact with the rest of the codebase.  
4. Scalable Structure. If we decide to add new integrations (e.g., Slack notifications, AWS S3 storage, or other AI models), we can spin up new subfolders without cluttering existing functionality.

With this approach, we keep our main folder structure largely intact while providing a dedicated space for external integration logic, making overlapping responsibilities more organized and easier to maintain over time.
---
## Action Plan for Successful Integration

- [ ] **Review Architecture & Requirements**  
  • Confirm that our classification pipeline (content extraction → parallel classification → signal combination) meets the functional and performance needs.  
  • Document any critical dependencies or external services (e.g., Airtable, Google Drive).

- [ ] **Establish Parallel Processing Mechanisms**  
  • Ensure that our ML model and pattern matcher are asynchronously invoked with asyncio.  
  • Verify thread safety or concurrency details for any shared resources.

- [ ] **Implement and Refine Signal Combination**  
  • Formalize validation functions (e.g., validate_year, validate_state).  
  • Decide how to merge ML signals with pattern-based signals (simple confidence weighting, hierarchical rules, etc.).  
  • Determine final classification confidence thresholds (e.g., for auto-processing vs. manual review).

- [ ] **Integrate Routing Logic**  
  • Reference the newly combined classification object in route_document or equivalent.  
  • Incorporate relevant parser (e.g., get_parser(classification['type'])) and storage logic.  
  • Validate that logic for “airtable” sync or other external system updates works seamlessly.

- [ ] **Enhance Low-Confidence Handling**  
  • Implement a feedback loop for manual review outcomes.  
  • Update rules, patterns, or ML training data when review identifies repeated misclassification.

- [ ] **Configure Logging and Monitoring**  
  • Capture classification inputs, outputs, and confidence scores.  
  • Add monitoring for concurrency issues (e.g., dropped tasks).

- [ ] **Run Integration and Load Testing**  
  • Conduct performance tests with bulk or high-frequency document ingestion.  
  • Check concurrency impacts, troubleshooting any bottlenecks around shared data or model resource usage.

- [ ] **Prepare Documentation and Onboarding Materials**  
  • Document each step of the process (content extraction → parallel classification → routing).  
  • Provide a quick-reference guide for new teams to understand integration entry points.  
  • Clearly outline how to add or update classification rules and how to deploy model updates.

- [ ] **Schedule Iterative Improvements**  
  • Set up sprints or review cycles to refine classification signals.  
  • Engage in continuous improvement for pattern matching, ML model accuracy, and routing logic.  

By following these steps in order, we ensure the pipeline is robust, maintainable, and easily extendable for future classification tasks.
