This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.github/
  workflows/
    branch-check.yml
    tests.yml
presentation/
  public/
    file.svg
    globe.svg
    next.svg
    vercel.svg
    window.svg
  src/
    app/
      components/
        continuous-presentation.tsx
      globals.css
      layout.tsx
      page.tsx
  .gitignore
  eslint.config.mjs
  next.config.ts
  outline.md
  postcss.config.mjs
  README.md
  tailwind.config.ts
scripts/
  check_branch.py
src/
  api/
    routes/
      classification.py
    main.py
    README.md
  classifiers/
    __init__.py
    base.py
    docling.py
    domain_config.py
    factory.py
    gemini.py
  cli/
    cli_doc_labeler.py
    document_classifier.py
    document_labeler.py
  client/
    __init__.py
    attachment.py
    gmail_client_README.md
    gmail.py
    label.py
    message.py
    query.py
  logging/
    logger.py
  models/
    paligemma/
      __init__.py
      deploy.py
      predict.py
  parsers/
    gmail-message-class-existing-parsing-capabilities.md
    PARSING_STRATEGY.md
  services/
    audit_service.py
    classification_service.py
    content_extraction_service.py
    email_processing_service.py
    notification_service.py
    security_service.py
    storage_service.py
    validation_service.py
  utils/
    main.py
    version_checker.py
  __init__.py
  console.py
  modules_overview.md
tests/
  cli/
    test_cli_doc_labeler.py
  client/
    test_gmail.py
    test_message.py
  integration/
    test_classification_service.py
    test_classifier_integration.py
    test_labeled_documents.py
  mocks/
    docling.py
  models/
    paligemma/
      conftest.py
      test_predict.py
  services/
    test_audit_service.py
    test_classification_service.py
    test_content_extraction_service.py
    test_email_processing_service.py
    test_security_service.py
  test-simplegmail/
    test_query.py
  unit/
    test_base_classifier.py
    test_classifier_factory.py
    test_cli.py
    test_client_identification.py
    test_docling_classifier.py
    test_document_helpers.py
    test_domain_config.py
    test_gemini_classifier.py
  utils/
    document_helpers.py
  conftest.py
  main.py
  README.md
.codecov.yml
.env.example
.gitignore
.pre-commit-config.yaml
.python-version
.repomixignore
.sourcery.yaml
base.txt
dev.txt
Makefile
mypy.ini
plan.md
pytest.ini
requirements-dev.txt
requirements-test.txt
requirements.txt
test.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".github/workflows/branch-check.yml">
name: Branch Name Check

on:
  pull_request:
    branches: [ main, master, develop, staging ]

jobs:
  branch-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Check Branch Name
        run: |
          BRANCH_NAME=${GITHUB_HEAD_REF}
          if [[ ! $BRANCH_NAME =~ ^(feature|bugfix|test|docs|refactor)/GH-[0-9]+-[a-z0-9-]+$ ]]; then
            echo "❌ Branch name must match pattern: type/GH-number-description"
            exit 1
          fi
</file>

<file path=".github/workflows/tests.yml">
name: Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]

jobs:
  test:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'
      
    strategy:
      matrix:
        python-version: ['3.9', '3.10', '3.11']
    
    env:
      PROJECT_ID: gmail-ai-autolabel
      REGION: us-central1
      ENDPOINT_ID: 3233752757731065856
      GOOGLE_CLOUD_PROJECT: gmail-ai-autolabel
      VERTEX_AI_ENDPOINT: projects/gmail-ai-autolabel/locations/us-central1/endpoints/3233752757731065856
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Google Auth
      id: auth
      uses: google-github-actions/auth@v2
      with:
        credentials_json: '${{ secrets.GOOGLE_CREDENTIALS }}'
        create_credentials_file: true
        export_environment_variables: true
        cleanup_credentials: true
        service_account: 'github-actions@gmail-ai-autolabel.iam.gserviceaccount.com'
        
    - name: Set up Cloud SDK
      uses: google-github-actions/setup-gcloud@v2
      with:
        project_id: ${{ env.PROJECT_ID }}
        install_components: 'beta'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements/test.txt
        pip install -e .
    
    - name: Initialize Vertex AI
      run: |
        # Use the credentials file created by google-github-actions/auth
        python -c "
        import os
        from google.cloud import aiplatform
        
        aiplatform.init(
            project='${{ env.PROJECT_ID }}',
            location='${{ env.REGION }}'
        )
        print('Successfully initialized Vertex AI')
        "
        
    - name: Run tests
      env:
        GCLOUD_PROJECT: ${{ env.PROJECT_ID }}
        GOOGLE_CLOUD_PROJECT: ${{ env.PROJECT_ID }}
      run: |
        pytest tests/ --cov=src --cov-report=xml
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v5
      with:
        files: ./coverage.xml
        fail_ci_if_error: true
        token: ${{ secrets.CODECOV_TOKEN }}
        slug: ${{ github.repository }}
        
    - name: Cleanup
      if: always()
      run: |
        rm -f vertex_credentials.json
</file>

<file path="presentation/public/file.svg">
<svg fill="none" viewBox="0 0 16 16" xmlns="http://www.w3.org/2000/svg"><path d="M14.5 13.5V5.41a1 1 0 0 0-.3-.7L9.8.29A1 1 0 0 0 9.08 0H1.5v13.5A2.5 2.5 0 0 0 4 16h8a2.5 2.5 0 0 0 2.5-2.5m-1.5 0v-7H8v-5H3v12a1 1 0 0 0 1 1h8a1 1 0 0 0 1-1M9.5 5V2.12L12.38 5zM5.13 5h-.62v1.25h2.12V5zm-.62 3h7.12v1.25H4.5zm.62 3h-.62v1.25h7.12V11z" clip-rule="evenodd" fill="#666" fill-rule="evenodd"/></svg>
</file>

<file path="presentation/public/globe.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><g clip-path="url(#a)"><path fill-rule="evenodd" clip-rule="evenodd" d="M10.27 14.1a6.5 6.5 0 0 0 3.67-3.45q-1.24.21-2.7.34-.31 1.83-.97 3.1M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16m.48-1.52a7 7 0 0 1-.96 0H7.5a4 4 0 0 1-.84-1.32q-.38-.89-.63-2.08a40 40 0 0 0 3.92 0q-.25 1.2-.63 2.08a4 4 0 0 1-.84 1.31zm2.94-4.76q1.66-.15 2.95-.43a7 7 0 0 0 0-2.58q-1.3-.27-2.95-.43a18 18 0 0 1 0 3.44m-1.27-3.54a17 17 0 0 1 0 3.64 39 39 0 0 1-4.3 0 17 17 0 0 1 0-3.64 39 39 0 0 1 4.3 0m1.1-1.17q1.45.13 2.69.34a6.5 6.5 0 0 0-3.67-3.44q.65 1.26.98 3.1M8.48 1.5l.01.02q.41.37.84 1.31.38.89.63 2.08a40 40 0 0 0-3.92 0q.25-1.2.63-2.08a4 4 0 0 1 .85-1.32 7 7 0 0 1 .96 0m-2.75.4a6.5 6.5 0 0 0-3.67 3.44 29 29 0 0 1 2.7-.34q.31-1.83.97-3.1M4.58 6.28q-1.66.16-2.95.43a7 7 0 0 0 0 2.58q1.3.27 2.95.43a18 18 0 0 1 0-3.44m.17 4.71q-1.45-.12-2.69-.34a6.5 6.5 0 0 0 3.67 3.44q-.65-1.27-.98-3.1" fill="#666"/></g><defs><clipPath id="a"><path fill="#fff" d="M0 0h16v16H0z"/></clipPath></defs></svg>
</file>

<file path="presentation/public/next.svg">
<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 394 80"><path fill="#000" d="M262 0h68.5v12.7h-27.2v66.6h-13.6V12.7H262V0ZM149 0v12.7H94v20.4h44.3v12.6H94v21h55v12.6H80.5V0h68.7zm34.3 0h-17.8l63.8 79.4h17.9l-32-39.7 32-39.6h-17.9l-23 28.6-23-28.6zm18.3 56.7-9-11-27.1 33.7h17.8l18.3-22.7z"/><path fill="#000" d="M81 79.3 17 0H0v79.3h13.6V17l50.2 62.3H81Zm252.6-.4c-1 0-1.8-.4-2.5-1s-1.1-1.6-1.1-2.6.3-1.8 1-2.5 1.6-1 2.6-1 1.8.3 2.5 1a3.4 3.4 0 0 1 .6 4.3 3.7 3.7 0 0 1-3 1.8zm23.2-33.5h6v23.3c0 2.1-.4 4-1.3 5.5a9.1 9.1 0 0 1-3.8 3.5c-1.6.8-3.5 1.3-5.7 1.3-2 0-3.7-.4-5.3-1s-2.8-1.8-3.7-3.2c-.9-1.3-1.4-3-1.4-5h6c.1.8.3 1.6.7 2.2s1 1.2 1.6 1.5c.7.4 1.5.5 2.4.5 1 0 1.8-.2 2.4-.6a4 4 0 0 0 1.6-1.8c.3-.8.5-1.8.5-3V45.5zm30.9 9.1a4.4 4.4 0 0 0-2-3.3 7.5 7.5 0 0 0-4.3-1.1c-1.3 0-2.4.2-3.3.5-.9.4-1.6 1-2 1.6a3.5 3.5 0 0 0-.3 4c.3.5.7.9 1.3 1.2l1.8 1 2 .5 3.2.8c1.3.3 2.5.7 3.7 1.2a13 13 0 0 1 3.2 1.8 8.1 8.1 0 0 1 3 6.5c0 2-.5 3.7-1.5 5.1a10 10 0 0 1-4.4 3.5c-1.8.8-4.1 1.2-6.8 1.2-2.6 0-4.9-.4-6.8-1.2-2-.8-3.4-2-4.5-3.5a10 10 0 0 1-1.7-5.6h6a5 5 0 0 0 3.5 4.6c1 .4 2.2.6 3.4.6 1.3 0 2.5-.2 3.5-.6 1-.4 1.8-1 2.4-1.7a4 4 0 0 0 .8-2.4c0-.9-.2-1.6-.7-2.2a11 11 0 0 0-2.1-1.4l-3.2-1-3.8-1c-2.8-.7-5-1.7-6.6-3.2a7.2 7.2 0 0 1-2.4-5.7 8 8 0 0 1 1.7-5 10 10 0 0 1 4.3-3.5c2-.8 4-1.2 6.4-1.2 2.3 0 4.4.4 6.2 1.2 1.8.8 3.2 2 4.3 3.4 1 1.4 1.5 3 1.5 5h-5.8z"/></svg>
</file>

<file path="presentation/public/vercel.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1155 1000"><path d="m577.3 0 577.4 1000H0z" fill="#fff"/></svg>
</file>

<file path="presentation/public/window.svg">
<svg fill="none" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 16 16"><path fill-rule="evenodd" clip-rule="evenodd" d="M1.5 2.5h13v10a1 1 0 0 1-1 1h-11a1 1 0 0 1-1-1zM0 1h16v11.5a2.5 2.5 0 0 1-2.5 2.5h-11A2.5 2.5 0 0 1 0 12.5zm3.75 4.5a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5M7 4.75a.75.75 0 1 1-1.5 0 .75.75 0 0 1 1.5 0m1.75.75a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5" fill="#666"/></svg>
</file>

<file path="presentation/src/app/components/continuous-presentation.tsx">
'use client';

import React, { useEffect, useRef } from 'react';
import { Mail, Bot, Clock, AlertTriangle, Settings, CheckCircle, AlertCircle, ChevronDown } from 'lucide-react';
import { motion, useScroll, useTransform, useInView } from 'framer-motion';

const Header = () => {
  const { scrollY } = useScroll();
  const opacity = useTransform(scrollY, [0, 100], [0, 1]);
  
  return (
    <motion.div 
      style={{ opacity }}
      className="fixed top-0 left-0 right-0 bg-white/80 backdrop-blur-md z-50 border-b border-blue-100"
    >
      <nav className="max-w-6xl mx-auto p-4">
        <div className="flex justify-between items-center">
          <motion.div 
            initial={{ x: -20, opacity: 0 }}
            animate={{ x: 0, opacity: 1 }}
            className="text-xl font-bold bg-gradient-to-r from-blue-600 to-purple-600 text-transparent bg-clip-text"
          >
            Delta Gmail AI
          </motion.div>
          <div className="flex space-x-6">
            {['Challenge', 'Solution', 'How It Works', 'Impact', '2024-2025'].map((item) => (
              <motion.button
                key={item}
                whileHover={{ scale: 1.05 }}
                whileTap={{ scale: 0.95 }}
                className="text-blue-600 hover:text-blue-800 font-medium"
              >
                {item}
              </motion.button>
            ))}
          </div>
        </div>
      </nav>
    </motion.div>
  );
};

const ScrollIndicator = () => (
  <motion.div
    initial={{ opacity: 0, y: 10 }}
    animate={{ opacity: 1, y: 0 }}
    transition={{ delay: 2, duration: 1, repeat: Infinity }}
    className="absolute bottom-8 left-1/2 transform -translate-x-1/2 text-blue-500"
  >
    <ChevronDown className="w-8 h-8 animate-bounce" />
  </motion.div>
);

const IntroSection = () => {
  const ref = useRef(null);
  const isInView = useInView(ref, { once: true });

  return (
    <div className="min-h-screen bg-gradient-to-br from-background via-background to-background pt-20 p-8 flex items-center relative">
      <motion.div
        ref={ref}
        initial={{ opacity: 0 }}
        animate={isInView ? { opacity: 1 } : {}}
        transition={{ duration: 0.8 }}
        className="max-w-4xl mx-auto"
      >
        <div className="text-center mb-12">
          <motion.h1
            initial={{ y: 20, opacity: 0 }}
            animate={isInView ? { y: 0, opacity: 1 } : {}}
            transition={{ delay: 0.2, duration: 0.8 }}
            className="text-5xl font-bold bg-gradient-to-r from-blue-600 to-purple-600 text-transparent bg-clip-text mb-4"
          >
            Delta Gmail AI Autolabel
          </motion.h1>
          <motion.p
            initial={{ y: 20, opacity: 0 }}
            animate={isInView ? { y: 0, opacity: 1 } : {}}
            transition={{ delay: 0.4, duration: 0.8 }}
            className="text-xl text-blue-700"
          >
            2024-2025: Transforming Email Management
          </motion.p>
        </div>

        <div className="grid grid-cols-1 md:grid-cols-2 gap-8 mb-12">
          <motion.div
            initial={{ x: -50, opacity: 0 }}
            animate={isInView ? { x: 0, opacity: 1 } : {}}
            transition={{ delay: 0.6, duration: 0.8 }}
            className="bg-white/80 backdrop-blur-sm rounded-2xl p-8 shadow-xl hover:shadow-2xl transition-shadow duration-300"
          >
            <h2 className="text-2xl font-semibold bg-gradient-to-r from-blue-600 to-indigo-600 text-transparent bg-clip-text mb-6">
              The Challenge
            </h2>
            <div className="space-y-6">
              <motion.div 
                className="flex items-start"
                whileHover={{ x: 5 }}
                transition={{ type: "spring", stiffness: 300 }}
              >
                <Mail className="w-6 h-6 text-blue-500 mr-3 mt-1 flex-shrink-0" />
                <p className="text-gray-700">
                  Managing registration emails for dozens of clients across 40 different state regulators
                </p>
              </motion.div>
              <motion.div 
                className="flex items-start"
                whileHover={{ x: 5 }}
                transition={{ type: "spring", stiffness: 300 }}
              >
                <Clock className="w-6 h-6 text-blue-500 mr-3 mt-1 flex-shrink-0" />
                <p className="text-gray-700">
                  Time-consuming manual processing and classification of emails
                </p>
              </motion.div>
              <motion.div 
                className="flex items-start"
                whileHover={{ x: 5 }}
                transition={{ type: "spring", stiffness: 300 }}
              >
                <AlertTriangle className="w-6 h-6 text-blue-500 mr-3 mt-1 flex-shrink-0" />
                <p className="text-gray-700">
                  Risk of missing critical updates and deadlines
                </p>
              </motion.div>
            </div>
          </motion.div>

          <motion.div
            initial={{ x: 50, opacity: 0 }}
            animate={isInView ? { x: 0, opacity: 1 } : {}}
            transition={{ delay: 0.8, duration: 0.8 }}
            className="bg-white/80 backdrop-blur-sm rounded-2xl p-8 shadow-xl hover:shadow-2xl transition-shadow duration-300"
          >
            <h2 className="text-2xl font-semibold bg-gradient-to-r from-purple-600 to-indigo-600 text-transparent bg-clip-text mb-6">
              2024-2025 Vision
            </h2>
            <div className="space-y-6">
              <motion.div 
                className="flex items-start"
                whileHover={{ x: 5 }}
                transition={{ type: "spring", stiffness: 300 }}
              >
                <Bot className="w-6 h-6 text-purple-500 mr-3 mt-1 flex-shrink-0" />
                <p className="text-gray-700">
                  Advanced AI-powered classification with 99% accuracy
                </p>
              </motion.div>
              <motion.div 
                className="flex items-start"
                whileHover={{ x: 5 }}
                transition={{ type: "spring", stiffness: 300 }}
              >
                <Mail className="w-6 h-6 text-purple-500 mr-3 mt-1 flex-shrink-0" />
                <p className="text-gray-700">
                  Automated labeling and processing of registration-related emails
                </p>
              </motion.div>
              <motion.div 
                className="flex items-start"
                whileHover={{ x: 5 }}
                transition={{ type: "spring", stiffness: 300 }}
              >
                <Clock className="w-6 h-6 text-purple-500 mr-3 mt-1 flex-shrink-0" />
                <p className="text-gray-700">
                  Save time and reduce headaches with smart automation
                </p>
              </motion.div>
            </div>
          </motion.div>
        </div>
      </motion.div>
      <ScrollIndicator />
    </div>
  );
};

const WorkflowSection = () => {
  const ref = useRef(null);
  const isInView = useInView(ref, { once: true });

  return (
    <div className="min-h-screen bg-gradient-to-br from-background via-background to-background pt-20 p-8 flex items-center">
      <motion.div
        ref={ref}
        initial={{ opacity: 0 }}
        animate={isInView ? { opacity: 1 } : {}}
        transition={{ duration: 0.8 }}
        className="max-w-6xl mx-auto"
      >
        <h2 className="text-3xl font-bold text-blue-900 mb-8 text-center">
          How It Works
        </h2>
        
        <div className="grid grid-cols-1 lg:grid-cols-2 gap-8">
          {/* Email Processing Flow */}
          <div className="bg-gray-50 rounded-lg p-6 shadow-lg">
            <h3 className="text-xl font-semibold text-blue-800 mb-4">
              Email Processing Flow
            </h3>
            <div className="text-sm text-gray-600 mb-4">
              From arrival to completion, here's how we handle your emails:
            </div>
            
            <div className="space-y-4">
              <div className="flex items-center p-3 bg-blue-50 rounded-lg">
                <Mail className="w-5 h-5 text-blue-600 mr-3 flex-shrink-0" />
                <div>
                  <div className="font-medium text-blue-900">Email Received</div>
                  <div className="text-sm text-blue-700">Initial security checks and validation</div>
                </div>
              </div>
              
              <div className="flex items-center p-3 bg-purple-50 rounded-lg">
                <Settings className="w-5 h-5 text-purple-600 mr-3 flex-shrink-0" />
                <div>
                  <div className="font-medium text-purple-900">Processing</div>
                  <div className="text-sm text-purple-700">Content extraction and analysis</div>
                </div>
              </div>
              
              <div className="flex items-center p-3 bg-indigo-50 rounded-lg">
                <AlertCircle className="w-5 h-5 text-indigo-600 mr-3 flex-shrink-0" />
                <div>
                  <div className="font-medium text-indigo-900">Classification</div>
                  <div className="text-sm text-indigo-700">AI-powered categorization and labeling</div>
                </div>
              </div>
              
              <div className="flex items-center p-3 bg-green-50 rounded-lg">
                <CheckCircle className="w-5 h-5 text-green-600 mr-3 flex-shrink-0" />
                <div>
                  <div className="font-medium text-green-900">Completion</div>
                  <div className="text-sm text-green-700">Storage and integration with Airtable</div>
                </div>
              </div>
            </div>
          </div>
          
          {/* Technical Flow */}
          <div className="bg-gray-50 rounded-lg p-6 shadow-lg">
            <h3 className="text-xl font-semibold text-blue-800 mb-4">
              Technical Architecture
            </h3>
            <div className="text-sm text-gray-600 mb-4">
              Behind the scenes, our system uses multiple specialized components:
            </div>
            
            <img 
              src="/api/placeholder/600/400" 
              alt="System Architecture Diagram" 
              className="w-full h-64 object-cover rounded-lg mb-4 bg-gray-100"
            />
            
            <div className="space-y-2 text-sm text-gray-700">
              <div className="font-medium">Key Components:</div>
              <ul className="list-disc pl-5 space-y-1">
                <li>Email Monitoring Service</li>
                <li>Content Processing Engine</li>
                <li>Classification System</li>
                <li>Integration Layer</li>
                <li>Storage & Database Systems</li>
              </ul>
            </div>
          </div>
        </div>
      </motion.div>
    </div>
  );
};

const ImpactSection = () => (
  <div className="min-h-screen bg-gradient-to-br from-background via-background to-background pt-20 p-8 flex items-center">
    <div className="max-w-4xl mx-auto">
      <h2 className="text-3xl font-bold text-emerald-900 mb-8 text-center">
        Real-World Impact
      </h2>
      
      <div className="grid grid-cols-1 md:grid-cols-2 gap-8">
        <div className="bg-white rounded-lg p-6 shadow-lg">
          <h3 className="text-xl font-semibold text-emerald-800 mb-4">
            Time Savings
          </h3>
          <div className="space-y-4">
            <div className="flex items-center p-3 bg-emerald-50 rounded-lg">
              <Clock className="w-5 h-5 text-emerald-600 mr-3 flex-shrink-0" />
              <div>
                <div className="font-medium text-emerald-900">Faster Processing</div>
                <div className="text-sm text-emerald-700">
                  Reduce email processing time from hours to minutes
                </div>
              </div>
            </div>
          </div>
        </div>

        <div className="bg-white rounded-lg p-6 shadow-lg">
          <h3 className="text-xl font-semibold text-emerald-800 mb-4">
            Better Organization
          </h3>
          <div className="space-y-4">
            <div className="flex items-center p-3 bg-emerald-50 rounded-lg">
              <CheckCircle className="w-5 h-5 text-emerald-600 mr-3 flex-shrink-0" />
              <div>
                <div className="font-medium text-emerald-900">Improved Accuracy</div>
                <div className="text-sm text-emerald-700">
                  Consistent classification and organized tracking
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
);

const NextStepsSection = () => (
  <div className="min-h-screen bg-gradient-to-br from-background via-background to-background pt-20 p-8 flex items-center">
    <div className="max-w-4xl mx-auto">
      <h2 className="text-3xl font-bold text-purple-900 mb-8 text-center">
        Next Steps
      </h2>
      
      <div className="bg-white rounded-lg p-6 shadow-lg">
        <div className="space-y-6">
          <div className="flex items-start">
            <div className="w-12 h-12 bg-purple-100 rounded-full flex items-center justify-center flex-shrink-0">
              <span className="text-2xl text-purple-600">1</span>
            </div>
            <div className="ml-4">
              <h3 className="text-lg font-semibold text-purple-900">Implementation</h3>
              <p className="text-gray-600">Initial setup and configuration of the system</p>
            </div>
          </div>
          
          <div className="flex items-start">
            <div className="w-12 h-12 bg-purple-100 rounded-full flex items-center justify-center flex-shrink-0">
              <span className="text-2xl text-purple-600">2</span>
            </div>
            <div className="ml-4">
              <h3 className="text-lg font-semibold text-purple-900">Training</h3>
              <p className="text-gray-600">Team training and system optimization</p>
            </div>
          </div>
          
          <div className="flex items-start">
            <div className="w-12 h-12 bg-purple-100 rounded-full flex items-center justify-center flex-shrink-0">
              <span className="text-2xl text-purple-600">3</span>
            </div>
            <div className="ml-4">
              <h3 className="text-lg font-semibold text-purple-900">Expansion</h3>
              <p className="text-gray-600">Adding more features and capabilities</p>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
);

const DeltaPresentation = () => {
  return (
    <div className="relative">
      <Header />
      <IntroSection />
      <WorkflowSection />
      <ImpactSection />
      <NextStepsSection />
    </div>
  );
};

export default DeltaPresentation;
</file>

<file path="presentation/src/app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  --foreground-rgb: 0, 0, 0;
  --background-start-rgb: 214, 219, 220;
  --background-end-rgb: 255, 255, 255;
}

body {
  margin: 0;
  padding: 0;
  min-height: 100vh;
}
</file>

<file path="presentation/src/app/layout.tsx">
import type { Metadata } from 'next'
import { Inter } from 'next/font/google'
import './globals.css'

const inter = Inter({ 
  subsets: ['latin'],
  variable: '--font-inter',
})

export const metadata: Metadata = {
  title: 'Delta Gmail AI - 2024-2025 Vision',
  description: 'Transforming Email Management with AI',
}

export default function RootLayout({
  children,
}: {
  children: React.ReactNode
}) {
  return (
    <html lang="en">
      <body className={`${inter.variable} font-sans`}>{children}</body>
    </html>
  )
}
</file>

<file path="presentation/src/app/page.tsx">
'use client';

import DeltaPresentation from './components/continuous-presentation';

export default function Home() {
  return (
    <main className="min-h-screen">
      <DeltaPresentation />
    </main>
  );
}
</file>

<file path="presentation/.gitignore">
# See https://help.github.com/articles/ignoring-files/ for more about ignoring files.

# dependencies
/node_modules
/.pnp
.pnp.*
.yarn/*
!.yarn/patches
!.yarn/plugins
!.yarn/releases
!.yarn/versions

# testing
/coverage

# next.js
/.next/
/out/

# production
/build

# misc
.DS_Store
*.pem

# debug
npm-debug.log*
yarn-debug.log*
yarn-error.log*
.pnpm-debug.log*

# env files (can opt-in for committing if needed)
.env*

# vercel
.vercel

# typescript
*.tsbuildinfo
next-env.d.ts
</file>

<file path="presentation/eslint.config.mjs">
import { dirname } from "path";
import { fileURLToPath } from "url";
import { FlatCompat } from "@eslint/eslintrc";

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

const compat = new FlatCompat({
  baseDirectory: __dirname,
});

const eslintConfig = [
  ...compat.extends("next/core-web-vitals", "next/typescript"),
];

export default eslintConfig;
</file>

<file path="presentation/next.config.ts">
import type { NextConfig } from "next";

const nextConfig: NextConfig = {
  /* config options here */
};

export default nextConfig;
</file>

<file path="presentation/outline.md">
# Delta Gmail AI Autolabel - Presentation Outline

## 1. The Challenge
- Managing dozens of clients with multiple products
- 40 different state regulators, each with unique processes
- High volume of critical communications
- Time-consuming manual processing
- Risk of missing important updates

## 2. The Vision
- An inbox that thinks like a human but works at machine speed
- Automated classification and organization
- Seamless integration with existing tools
- Time savings and reduced headaches

## 3. How It Works
- Email monitoring and security
- Smart content analysis
- Automatic classification
- Integration with Gmail, Gmelius, and Airtable
- Human oversight and verification

## 4. Key Benefits
- Faster processing of registration-related emails
- Reduced risk of missing critical communications
- Better organization and tracking
- Time saved on manual tasks
- Improved team collaboration

## 5. Real-World Impact
- Examples of automated workflows
- Time saved per email
- Improved accuracy in classification
- Better registration tracking

## 6. Next Steps
- Implementation timeline
- Training and adoption
- Continuous improvement
- Future enhancements

## 7. Q&A and Discussion
- Team feedback
- Questions and concerns
- Additional feature requests
</file>

<file path="presentation/postcss.config.mjs">
/** @type {import('postcss-load-config').Config} */
const config = {
  plugins: {
    tailwindcss: {},
  },
};

export default config;
</file>

<file path="presentation/README.md">
This is a [Next.js](https://nextjs.org) project bootstrapped with [`create-next-app`](https://nextjs.org/docs/app/api-reference/cli/create-next-app).

## Getting Started

First, run the development server:

```bash
npm run dev
# or
yarn dev
# or
pnpm dev
# or
bun dev
```

Open [http://localhost:3000](http://localhost:3000) with your browser to see the result.

You can start editing the page by modifying `app/page.tsx`. The page auto-updates as you edit the file.

This project uses [`next/font`](https://nextjs.org/docs/app/building-your-application/optimizing/fonts) to automatically optimize and load [Geist](https://vercel.com/font), a new font family for Vercel.

## Learn More

To learn more about Next.js, take a look at the following resources:

- [Next.js Documentation](https://nextjs.org/docs) - learn about Next.js features and API.
- [Learn Next.js](https://nextjs.org/learn) - an interactive Next.js tutorial.

You can check out [the Next.js GitHub repository](https://github.com/vercel/next.js) - your feedback and contributions are welcome!

## Deploy on Vercel

The easiest way to deploy your Next.js app is to use the [Vercel Platform](https://vercel.com/new?utm_medium=default-template&filter=next.js&utm_source=create-next-app&utm_campaign=create-next-app-readme) from the creators of Next.js.

Check out our [Next.js deployment documentation](https://nextjs.org/docs/app/building-your-application/deploying) for more details.
</file>

<file path="presentation/tailwind.config.ts">
import type { Config } from "tailwindcss";

export default {
  content: [
    "./src/pages/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/components/**/*.{js,ts,jsx,tsx,mdx}",
    "./src/app/**/*.{js,ts,jsx,tsx,mdx}",
  ],
  theme: {
    extend: {
      colors: {
        background: "var(--background)",
        foreground: "var(--foreground)",
        gradient: {
          start: "var(--background-start)",
          end: "var(--background-end)",
        },
      },
    },
  },
  plugins: [],
} satisfies Config;
</file>

<file path="scripts/check_branch.py">
#!/usr/bin/env python
import sys
import re
import subprocess


def get_current_branch():
    return (
        subprocess.check_output(["git", "rev-parse", "--abbrev-ref", "HEAD"])
        .decode("utf-8")
        .strip()
    )


def check_branch_name():
    branch = get_current_branch()

    # Protected branches
    protected = ["main", "master", "develop", "staging"]
    if branch in protected:
        print(f"❌ Cannot commit directly to {branch}")
        return 1

    # Branch pattern: type/GH-number-description
    pattern = r"^(feature|bugfix|test|docs|refactor)/GH-\d+-[a-z0-9-]+$"
    if not re.match(pattern, branch):
        print("❌ Branch name must match pattern: type/GH-number-description")
        print("Example: feature/GH-123-gmail-connectivity")
        return 1

    return 0


if __name__ == "__main__":
    sys.exit(check_branch_name())
</file>

<file path="src/api/routes/classification.py">
"""
Classification API Routes

Provides endpoints for document classification using configurable classifiers.
"""
from fastapi import APIRouter, File, UploadFile, HTTPException, Depends
from typing import Dict, List
import tempfile
import os
from pathlib import Path

from ...classifiers.base import BaseDocumentClassifier, ClassificationResult
from ...classifiers.factory import ClassifierFactory
from ...config.classifier_config import ClassifierConfig

router = APIRouter(prefix="/classify", tags=["classification"])

async def get_config() -> ClassifierConfig:
    """Get the current classifier configuration."""
    return ClassifierConfig()

async def get_classifier(config: ClassifierConfig = Depends(get_config)) -> BaseDocumentClassifier:
    """
    Dependency to get the configured classifier instance.
    
    Args:
        config: Classifier configuration settings
        
    Returns:
        Configured classifier instance
    """
    try:
        return ClassifierFactory.create_classifier(
            config.classifier_type,
            **config.get_classifier_kwargs()
        )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to initialize classifier: {str(e)}"
        )

@router.post("/", response_model=ClassificationResult)
async def classify_document(
    file: UploadFile = File(...),
    classifier: BaseDocumentClassifier = Depends(get_classifier),
    config: ClassifierConfig = Depends(get_config)
):
    """
    Classify a single document.
    
    Args:
        file: The document file to classify
        classifier: Document classifier implementation
        config: Classifier configuration
        
    Returns:
        ClassificationResult containing the document analysis
    """
    try:
        # Create a temporary file to store the upload
        with tempfile.NamedTemporaryFile(delete=False) as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_path = temp_file.name
        
        try:
            # Classify the document
            result = await classifier.classify_document(temp_path)
            return result
        finally:
            # Clean up temp file
            os.unlink(temp_path)
            
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/batch/", response_model=List[ClassificationResult])
async def classify_documents(
    files: List[UploadFile] = File(...),
    classifier: BaseDocumentClassifier = Depends(get_classifier),
    config: ClassifierConfig = Depends(get_config)
):
    """
    Classify multiple documents in batch.
    
    Args:
        files: List of document files to classify
        classifier: Document classifier implementation
        config: Classifier configuration
        
    Returns:
        List of ClassificationResults
    """
    temp_files = []
    try:
        # Create temporary files for all uploads
        for file in files:
            with tempfile.NamedTemporaryFile(delete=False) as temp_file:
                content = await file.read()
                temp_file.write(content)
                temp_files.append(temp_file.name)
        
        # Classify all documents
        results = await classifier.classify_batch(
            temp_files,
            max_concurrent=config.max_concurrent_requests
        )
        return results
    
    finally:
        # Clean up all temp files
        for temp_path in temp_files:
            try:
                os.unlink(temp_path)
            except:
                pass

@router.get("/info/current")
async def get_current_classifier_info(
    classifier: BaseDocumentClassifier = Depends(get_classifier)
) -> Dict:
    """
    Get information about the current classifier implementation.
    
    Returns:
        Dictionary with current classifier metadata
    """
    return classifier.get_classifier_info()

@router.get("/info/available")
async def list_available_classifiers() -> Dict[str, str]:
    """
    List all available classifier implementations.
    
    Returns:
        Dictionary mapping classifier names to their descriptions
    """
    return ClassifierFactory.list_available_classifiers()

@router.get("/config")
async def get_current_config(
    config: ClassifierConfig = Depends(get_config)
) -> Dict:
    """
    Get current classifier configuration.
    
    Returns:
        Dictionary of current configuration settings
    """
    return config.dict(exclude_none=True)
</file>

<file path="src/api/main.py">
"""
FastAPI Web Service for Document Classification
"""
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from .routes import classification

app = FastAPI(
    title="Document Classification API",
    description="API for classifying regulatory documents using configurable classifiers",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Modify in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Include our routes
app.include_router(classification.router)

@app.get("/health")
async def health_check():
    """Simple health check endpoint."""
    return {"status": "healthy"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="src/api/README.md">
# Document Classification API

This API service provides document classification capabilities using Google's Gemini Flash model. It can process regulatory documents (PDFs, images) and extract structured information about their content, type, and entities.

## Setup

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Set up environment variables:
Create a `.env` file with:
```
GOOGLE_API_KEY=your_gemini_api_key_here
```

3. Run the service:
```bash
python -m src.api.main
```

The service will start on `http://localhost:8000`

## API Endpoints

### 1. Classify Single Document
**POST** `/classify/`

Upload a single document for classification.

```bash
curl -X POST "http://localhost:8000/classify/" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@your_document.pdf"
```

### 2. Batch Classification
**POST** `/classify/batch/`

Upload multiple documents for classification.

```bash
curl -X POST "http://localhost:8000/classify/batch/" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "files=@document1.pdf" \
  -F "files=@document2.pdf"
```

### 3. Health Check
**GET** `/health/`

Check if the service is running.

```bash
curl "http://localhost:8000/health/"
```

## Response Format

The API returns JSON responses in the following format:

```json
{
    "file_path": "path/to/document",
    "success": true,
    "classification": {
        "document_type": "registration",
        "confidence": 0.85,
        "entities": {
            "companies": ["Company A", "Company B"],
            "products": ["Product X"],
            "states": ["CA", "NY"]
        },
        "key_fields": {
            "dates": ["2024-02-11"],
            "registration_numbers": ["REG123"],
            "amounts": ["$500"]
        },
        "metadata": {
            "has_tables": true,
            "entity_count": 4,
            "field_count": 3
        },
        "summary": "Document summary...",
        "flags": []
    }
}
```

## Error Handling

Errors are returned with appropriate HTTP status codes and messages:

```json
{
    "detail": "Error message describing what went wrong"
}
```

## Development

The API is built with:
- FastAPI for the web framework
- Google Gemini Flash for document processing
- Async processing for better performance
- Temporary file handling for uploads

## Security Notes

For production deployment:
1. Configure CORS appropriately in `main.py`
2. Use secure environment variable handling
3. Implement authentication/authorization
4. Add rate limiting
5. Configure proper logging
</file>

<file path="src/classifiers/__init__.py">
"""
Document Classification Package

Provides implementations of document classifiers and related utilities.
"""
from .base import BaseDocumentClassifier, ClassificationResult
from .docling import DoclingClassifier
from .gemini import GeminiClassifier
from .factory import ClassifierFactory

__all__ = [
    'BaseDocumentClassifier',
    'ClassificationResult',
    'DoclingClassifier',
    'GeminiClassifier',
    'ClassifierFactory'
]
</file>

<file path="src/classifiers/base.py">
"""
Base Classifier Interface

This module defines the base interface that all document classifiers must implement.
Supports hot-swapping of different classification implementations.
"""
from abc import ABC, abstractmethod
from typing import Dict, List, Union, Optional
from pathlib import Path
from pydantic import BaseModel

class ClassificationResult(BaseModel):
    """Standardized classification result model."""
    document_type: str | None
    client_code: str | None
    confidence: float
    entities: Dict[str, List[str]]
    key_fields: Dict[str, List[str]]
    metadata: Dict[str, Union[bool, int, float, str, List[str]]]
    summary: str | None
    flags: List[str]

class BaseDocumentClassifier(ABC):
    """Abstract base class for document classifiers."""
    
    @abstractmethod
    async def classify_document(self, 
                              source: Union[str, Path, bytes], 
                              source_type: str = "file",
                              metadata: Optional[Dict] = None) -> ClassificationResult:
        """
        Classify a single document from various sources.
        
        Args:
            source: The document source, which can be:
                   - A file path (str or Path) when source_type is "file"
                   - Raw document bytes when source_type is "bytes"
                   - Document text content when source_type is "text"
            source_type: Type of the source ("file", "bytes", or "text")
            metadata: Optional metadata about the source (e.g., email subject, sender)
            
        Returns:
            ClassificationResult containing the classification details
        """
        pass
    
    @abstractmethod
    async def classify_batch(self, 
                           sources: List[Union[str, Path, bytes]],
                           source_type: str = "file",
                           metadata: Optional[List[Dict]] = None,
                           max_concurrent: int = 5) -> List[ClassificationResult]:
        """
        Classify multiple documents from various sources.
        
        Args:
            sources: List of document sources (paths, bytes, or text)
            source_type: Type of the sources ("file", "bytes", or "text")
            metadata: Optional list of metadata dicts for each source
            max_concurrent: Maximum number of concurrent classifications
            
        Returns:
            List of ClassificationResults
        """
        pass
    
    @abstractmethod
    def get_classifier_info(self) -> Dict[str, str]:
        """
        Get information about the classifier implementation.
        
        Returns:
            Dictionary containing classifier metadata like:
            - name: Name of the classifier
            - version: Version of the classifier
            - description: Brief description of how it works
        """
        pass
</file>

<file path="src/classifiers/docling.py">
"""
Docling Document Classifier Implementation

Uses IBM's open-source Docling library for high-fidelity document parsing and classification.
Runs locally and preserves document structure.
"""
from typing import Dict, List, Optional, Union, Tuple
from pathlib import Path
import asyncio
import json
import io
from docling import DocProcessor, TableFormer  # Note: Package name may differ
from .base import BaseDocumentClassifier, ClassificationResult
from .domain_config import DomainConfig

class DoclingClassifier(BaseDocumentClassifier):
    """Document classifier using IBM's Docling library."""
    
    def __init__(self, model_path: Optional[str] = None, config_dir: Optional[Path] = None):
        """
        Initialize the Docling classifier.
        
        Args:
            model_path: Optional path to custom model weights
            config_dir: Optional path to configuration directory
        """
        # Initialize Docling components
        self.processor = DocProcessor(model_path=model_path)
        self.table_extractor = TableFormer()
        
        # Load domain configuration
        self.domain_config = DomainConfig(config_dir)
        
    async def classify_document(self, 
                              source: Union[str, Path, bytes],
                              source_type: str = "file",
                              metadata: Optional[Dict] = None) -> ClassificationResult:
        """Classify a single document using Docling."""
        try:
            # Process the document based on source type
            if source_type == "file":
                file_path = Path(source)
                if not file_path.exists():
                    raise FileNotFoundError(f"File not found: {file_path}")
                doc_result = await asyncio.get_event_loop().run_in_executor(
                    None, self._process_document, file_path
                )
            elif source_type == "bytes":
                # Process bytes directly
                doc_result = await asyncio.get_event_loop().run_in_executor(
                    None, self._process_bytes, source
                )
            elif source_type == "text":
                # Process text content directly
                doc_result = await asyncio.get_event_loop().run_in_executor(
                    None, self._process_text, source
                )
            else:
                raise ValueError(f"Unsupported source type: {source_type}")
            
            # Enhance classification with metadata if provided
            if metadata:
                doc_result = self._enhance_with_metadata(doc_result, metadata)
            
            # Convert Docling output to our standard format
            return self._convert_to_classification_result(doc_result)
            
        except Exception as e:
            raise Exception(f"Docling classification failed: {str(e)}")
            
    async def classify_batch(self, 
                           sources: List[Union[str, Path, bytes]],
                           source_type: str = "file",
                           metadata: Optional[List[Dict]] = None,
                           max_concurrent: int = 5) -> List[ClassificationResult]:
        """Classify multiple documents concurrently."""
        results = []
        metadata_list = metadata or [None] * len(sources)
        
        for i in range(0, len(sources), max_concurrent):
            batch_sources = sources[i:i + max_concurrent]
            batch_metadata = metadata_list[i:i + max_concurrent]
            tasks = [
                self.classify_document(source, source_type, md) 
                for source, md in zip(batch_sources, batch_metadata)
            ]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Handle any exceptions in the batch
            for result in batch_results:
                if isinstance(result, Exception):
                    results.append(self._create_error_result(str(result)))
                else:
                    results.append(result)
                    
        return results

    def _process_bytes(self, content: bytes) -> Dict:
        """Process document from bytes."""
        # Create a file-like object from bytes
        file_obj = io.BytesIO(content)
        doc = self.processor.process_file_object(file_obj)
        return self._extract_document_info(doc)
        
    def _process_text(self, content: str) -> Dict:
        """Process document from text content."""
        doc = self.processor.process_text(content)
        return self._extract_document_info(doc)
        
    def _process_document(self, file_path: Path) -> Dict:
        """Process document from file path."""
        doc = self.processor.process_document(file_path)
        return self._extract_document_info(doc)
        
    def _extract_document_info(self, doc) -> Dict:
        """Extract common document information."""
        # Get document text for pattern matching
        doc_text = doc.get_text()
        
        # Extract tables if present
        tables = []
        if doc.has_tables:
            tables = self.table_extractor.extract_tables(doc)
        
        # Use domain configuration to extract entities and information
        entities = self._extract_entities(doc, doc_text)
        key_fields = self._extract_key_fields(doc)
        
        # Determine document type using domain patterns
        doc_type, base_type = self.domain_config.get_document_type(doc_text)
        
        # Get related document types
        related_types = self.domain_config.get_related_documents(doc_type) if doc_type else []
        
        return {
            "document_type": doc_type,
            "base_type": base_type,
            "entities": entities,
            "key_fields": key_fields,
            "tables": tables,
            "summary": doc.get_summary(),
            "metadata": {
                "page_count": doc.page_count,
                "has_tables": bool(tables),
                "confidence": doc.extraction_confidence,
                "related_document_types": related_types,
                "product_categories": self.domain_config.get_product_categories(doc_text)
            }
        }
        
    def _enhance_with_metadata(self, doc_result: Dict, metadata: Dict) -> Dict:
        """Enhance classification results with provided metadata."""
        # Add metadata to existing result
        doc_result["metadata"].update({
            "source_metadata": metadata
        })
        
        # If we have email metadata, use it to improve classification
        if "email_subject" in metadata:
            # Try to extract additional entities from subject
            subject_entities = self._extract_entities_from_text(metadata["email_subject"])
            doc_result["entities"]["companies"].extend(subject_entities.get("companies", []))
            doc_result["entities"]["states"].extend(subject_entities.get("states", []))
            
        return doc_result
        
    def _extract_entities_from_text(self, text: str) -> Dict[str, List[str]]:
        """Extract entities from a text string."""
        return {
            "companies": [match[1] for match in self.domain_config.get_company_codes(text)],
            "states": self.domain_config.get_states(text)
        }
    
    def get_classifier_info(self) -> Dict[str, str]:
        """Get information about this classifier implementation."""
        return {
            "name": "Docling Classifier",
            "version": "1.0.0",
            "description": "Uses IBM's Docling library for high-fidelity local document processing",
            "provider": "IBM (Open Source)",
            "features": [
                "Local processing (no data leaves your system)",
                "High-accuracy layout preservation",
                "Superior table extraction via TableFormer",
                "No API costs or rate limits",
                "Domain-aware classification using regulatory configurations"
            ]
        }
    
    def _extract_entities(self, doc, doc_text: str) -> Dict[str, List[str]]:
        """Extract entities using both Docling and domain configuration."""
        # Get entities from Docling
        companies = doc.extract_entities(entity_type="ORG")
        products = doc.extract_entities(entity_type="PRODUCT")
        
        # Get states from domain configuration
        states = self.domain_config.get_states(doc_text)
        
        return {
            "companies": companies,
            "products": products,
            "states": states
        }
    
    def _extract_key_fields(self, doc) -> Dict[str, List[str]]:
        """Extract and validate key fields."""
        # Extract basic fields
        dates = doc.extract_dates()
        registration_numbers = doc.extract_patterns(r"REG-?\d+|LIC-?\d+")
        amounts = doc.extract_patterns(r"\$?\d+(?:,\d{3})*(?:\.\d{2})?")
        
        # Validate registration numbers against state-specific rules
        doc_text = doc.get_text()
        states = self.domain_config.get_states(doc_text)
        
        valid_numbers = []
        for number in registration_numbers:
            # If we have state context, validate against state rules
            if states:
                if any(self.domain_config.validate_registration_number(number, state) 
                      for state in states):
                    valid_numbers.append(number)
            else:
                # If no state context, include all numbers
                valid_numbers.append(number)
        
        return {
            "dates": dates,
            "registration_numbers": valid_numbers,
            "amounts": amounts
        }
    
    def _identify_client(self, doc_text: str, companies: List[str]) -> Tuple[str | None, float]:
        """
        Identify client code from document text and extracted companies.
        
        Returns:
            Tuple of (client_code, confidence)
        """
        # Check against known client patterns
        client_matches = []
        for code, patterns in self.domain_config.client_patterns.items():
            for pattern in patterns:
                if pattern.search(doc_text):
                    client_matches.append((code, 0.9))
                    
        # Check extracted company names against client database
        for company in companies:
            if client_code := self.domain_config.get_client_by_company(company):
                client_matches.append((client_code, 0.8))
                
        if not client_matches:
            return None, 0.0
            
        # Return highest confidence match
        return max(client_matches, key=lambda x: x[1])

    def _convert_to_classification_result(self, raw_result: Dict) -> ClassificationResult:
        """Convert Docling output to standardized ClassificationResult."""
        # Calculate confidence based on Docling's metrics and domain validation
        base_confidence = raw_result.get("metadata", {}).get("confidence", 0.0)
        domain_confidence = self._calculate_domain_confidence(raw_result)
        confidence = (base_confidence + domain_confidence) / 2
        
        # Generate any warning flags
        flags = self._generate_flags(raw_result, confidence)
        
        # Create metadata
        metadata = {
            **raw_result.get("metadata", {}),
            'classifier': self.get_classifier_info()['name'],
            'domain_confidence': domain_confidence
        }
        
        # Identify client
        client_code, client_confidence = self._identify_client(
            raw_result.get('summary', ''),
            raw_result.get('entities', {}).get('companies', [])
        )
        
        # Construct the result
        return ClassificationResult(
            document_type=raw_result.get("document_type"),
            client_code=client_code,
            confidence=min(confidence, client_confidence),
            entities=raw_result.get("entities", {}),
            key_fields=raw_result.get("key_fields", {}),
            metadata=metadata,
            summary=raw_result.get("summary"),
            flags=flags
        )
    
    def _calculate_domain_confidence(self, result: Dict) -> float:
        """Calculate confidence based on domain validation."""
        score = 0.0
        total_checks = 0
        
        # Check document type against known types
        if result.get("document_type"):
            score += 1
            total_checks += 1
        
        # Check if found states are valid
        if result.get("entities", {}).get("states"):
            score += 1
            total_checks += 1
        
        # Check if found product categories match our domain
        if result.get("metadata", {}).get("product_categories"):
            score += 1
            total_checks += 1
        
        # Check registration number validation
        if result.get("key_fields", {}).get("registration_numbers"):
            score += 1
            total_checks += 1
        
        return round(score / total_checks, 2) if total_checks > 0 else 0.0
    
    def _generate_flags(self, result: Dict, confidence: float) -> List[str]:
        """Generate warning flags based on classification results."""
        flags = []
        
        # Check confidence
        if confidence < 0.5:
            flags.append('LOW_CONFIDENCE')
            
        # Check for missing critical information
        if not result.get("document_type"):
            flags.append('MISSING_DOCUMENT_TYPE')
            
        # Check entities
        if not any(result.get("entities", {}).values()):
            flags.append('NO_ENTITIES_FOUND')
            
        # Check key fields
        key_fields = result.get("key_fields", {})
        if not key_fields.get("dates"):
            flags.append('NO_DATES_FOUND')
        if not key_fields.get("registration_numbers"):
            flags.append('NO_REGISTRATION_NUMBERS')
            
        # Add domain-specific flags
        if not result.get("metadata", {}).get("product_categories"):
            flags.append('NO_PRODUCT_CATEGORY_MATCH')
            
        return flags
    
    def _create_error_result(self, error_message: str) -> ClassificationResult:
        """Create a ClassificationResult for error cases."""
        return ClassificationResult(
            document_type=None,
            confidence=0.0,
            entities={'companies': [], 'products': [], 'states': []},
            key_fields={'dates': [], 'registration_numbers': [], 'amounts': []},
            metadata={
                'error': error_message,
                'classifier': self.get_classifier_info()['name']
            },
            summary=None,
            flags=['CLASSIFICATION_ERROR']
        )
</file>

<file path="src/classifiers/domain_config.py">
"""
Domain Configuration Loader

Loads and manages domain-specific configuration from YAML files.
"""
from typing import Dict, List, Optional, Tuple
from pathlib import Path
import yaml
import re
from src.utils.version_checker import VersionChecker
import logging

logger = logging.getLogger(__name__)

class DomainConfig:
    """Manages domain-specific configuration for document classification."""
    
    def __init__(self, config_dir: Optional[Path] = None):
        """
        Initialize domain configuration.
        
        Args:
            config_dir: Optional path to configuration directory
        """
        self.config_dir = config_dir or Path("config")
        
        # Initialize version checker
        self.version_checker = VersionChecker(self.config_dir)
        
        # Check config compatibility
        warnings = self.version_checker.check_compatibility()
        for warning in warnings:
            logger.warning(warning)
            
        # Load configurations
        self.regulatory_actions = self._load_yaml("regulatory_actions.yaml")
        self.product_categories = self._load_yaml("product_categories.yaml")
        self.state_specific = self._load_yaml("state_specific.yaml")
        self.validation_rules = self._load_yaml("validation_rules.yaml")
        self.relationships = self._load_yaml("relationships.yaml")
        self.state_patterns = self._load_yaml("state_patterns.yaml")
        self.company_codes = self._load_yaml("clients.yaml")
        self.document_types = self._load_yaml("document_types.yaml")
        
        # Load and process client data
        self.client_data = self._load_client_patterns()
        
        # Check for required migrations
        self._check_migrations()
        
        # Compile regex patterns
        self._compile_patterns()
        
    def _load_yaml(self, filename: str) -> Dict:
        """Load YAML configuration file."""
        try:
            with open(self.config_dir / filename) as f:
                config = yaml.safe_load(f) or {}
                
            # Check if migration is needed
            config_name = filename.replace(".yaml", "")
            if self.version_checker.needs_migration(config_name):
                migrations = self.version_checker.get_required_migrations(config_name)
                logger.warning(
                    f"Config {filename} needs migration. Required steps: {migrations}"
                )
                
            return config
        except FileNotFoundError:
            return {}
            
    def _check_migrations(self) -> None:
        """Check if any config files need migration."""
        for config_name in [
            "regulatory_actions",
            "product_categories",
            "document_types",
            "company_codes",
            "state_patterns"
        ]:
            if self.version_checker.needs_migration(config_name):
                migrations = self.version_checker.get_required_migrations(config_name)
                logger.warning(
                    f"Config {config_name} needs migration. Steps: {migrations}"
                )
            
    def _compile_patterns(self) -> None:
        """Compile regex patterns from configurations."""
        self.patterns = {
            "document_types": {},
            "products": {},
            "states": {},
            "companies": {}
        }
        
        # Compile patterns for document types from document_types.yaml
        for doc_type_id, doc_type in self.document_types.get("document_types", {}).items():
            if "patterns" in doc_type:
                for pattern in doc_type["patterns"]:
                    self.patterns["document_types"][doc_type_id] = re.compile(
                        pattern["regex"], re.IGNORECASE
                    )
                    
        # Compile patterns for products
        for category in self.product_categories.get("product_categories", {}).values():
            if "patterns" in category:
                for pattern in category["patterns"]:
                    self.patterns["products"][category["canonical_name"]] = re.compile(
                        pattern["regex"], re.IGNORECASE
                    )
                    
        # Compile state patterns from state_patterns.yaml
        for state_code, state_info in self.state_patterns.get("states", {}).items():
            if "patterns" in state_info:
                for pattern in state_info["patterns"]:
                    self.patterns["states"][state_code] = re.compile(
                        pattern["regex"], re.IGNORECASE
                    )
        
        # Compile company name patterns
        for code, company in self.client_data.items():
            # Create pattern from company name, aliases, and patterns
            patterns = []
            
            # Add company name
            if name := company.get("name"):
                patterns.append(re.escape(name))
                # Add partial name matches (for each word in the name)
                words = name.split()
                if len(words) > 1:  # Only add word patterns if name has multiple words
                    for word in words:
                        if len(word) > 3:  # Only match on words longer than 3 chars
                            patterns.append(re.escape(word))
            
            # Add aliases
            patterns.extend(re.escape(alias) for alias in company.get("aliases", []))
            
            # Add provided patterns
            patterns.extend(pattern for pattern in company.get("patterns", []))
            
            # Add code pattern
            patterns.append(rf"{code}\b")
            
            # Join all patterns with OR
            pattern = "|".join(f"({p})" for p in patterns)
            self.patterns["companies"][code] = re.compile(
                f"(?i)\\b({pattern})\\b"
            )
    
    def get_document_type(self, text: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Determine document type based on configured patterns.
        
        Args:
            text: Document text to analyze
            
        Returns:
            Tuple of (canonical_name, base_type) if found, else (None, None)
        """
        for doc_type_id, pattern in self.patterns["document_types"].items():
            if pattern.search(text):
                doc_type = self.document_types["document_types"][doc_type_id]
                return doc_type["canonical_name"], doc_type["base_type"]
        return None, None
    
    def get_product_categories(self, text: str) -> List[str]:
        """
        Extract product categories based on configured patterns.
        
        Args:
            text: Document text to analyze
            
        Returns:
            List of canonical product category names
        """
        categories = []
        for category, pattern in self.patterns["products"].items():
            if pattern.search(text):
                categories.append(category)
        return categories
    
    def get_states(self, text: str) -> List[str]:
        """
        Extract state references based on configured patterns.
        
        Args:
            text: Document text to analyze
            
        Returns:
            List of state codes
        """
        states = []
        for state_code, pattern in self.patterns["states"].items():
            if pattern.search(text):
                states.append(state_code)
        return states
    
    def get_company_codes(self, text: str) -> List[Tuple[str, str]]:
        """
        Extract company codes based on configured patterns.
        
        Args:
            text: Document text to analyze
            
        Returns:
            List of tuples (company_code, matched_name)
        """
        matches = []
        for code, pattern in self.patterns["companies"].items():
            match = pattern.search(text)
            if match:
                matches.append((code, match.group(0)))
        return matches
    
    def validate_registration_number(self, number: str, state: str) -> bool:
        """
        Validate a registration number against state-specific rules.
        
        Args:
            number: Registration number to validate
            state: State code
            
        Returns:
            Whether the number is valid for the state
        """
        rules = self.validation_rules.get("registration_numbers", {}).get(state, {})
        if not rules or "pattern" not in rules:
            return True  # No validation rule defined
            
        pattern = re.compile(rules["pattern"])
        return bool(pattern.match(number))
    
    def get_related_documents(self, doc_type: str) -> List[str]:
        """
        Get related document types based on relationships config.
        
        Args:
            doc_type: Document type to find relationships for
            
        Returns:
            List of related document types
        """
        relationships = self.relationships.get("document_relationships", {})
        return relationships.get(doc_type, [])
    
    def _load_client_patterns(self) -> Dict:
        """Load and compile client patterns."""
        client_patterns_file = self.config_dir / "clients.yaml"
        if not client_patterns_file.exists():
            logger.warning("Client patterns file not found")
            return {}
            
        with open(client_patterns_file) as f:
            config = yaml.safe_load(f)
            
        companies = config.get("companies", {})
        
        # Process each client to add patterns
        for code, data in companies.items():
            # Add standard patterns based on company name and code
            patterns = []
            
            # Add company name pattern
            if name := data.get("name"):
                patterns.append(re.escape(name))  # Exact company name
                
            # Add aliases patterns
            for alias in data.get("aliases", []):
                patterns.append(re.escape(alias))
                
            # Add client code pattern
            patterns.append(rf"{code}\b")  # Code with word boundary
            
            # Add email domain patterns
            for domain in data.get("domains", []):
                patterns.append(rf"@{re.escape(domain)}")
                
            # Store patterns with the client data
            data["patterns"] = patterns
            
        return companies

    def get_client_by_company(self, company_name: str) -> Optional[str]:
        """Get client code by exact company name match."""
        for code, data in self.client_data.items():
            if data.get("name") == company_name:
                return code
        return None
        
    def get_client_by_email_domain(self, email: str) -> Optional[str]:
        """Get client code by email domain."""
        domain = email.split("@")[-1].lower()
        for code, data in self.client_data.items():
            if domain in data.get("domains", []):
                return code
        return None
        
    def _identify_client(self, text: str) -> Tuple[Optional[str], float]:
        """Identify client from text with confidence score.
        
        Args:
            text: Text to analyze
            
        Returns:
            Tuple of (client_code, confidence_score)
        """
        logger.debug(f"Identifying client from text: {text}")
        
        # Check exact company name match first (highest confidence)
        for code, data in self.client_data.items():
            if data.get("name"):
                company_name = data["name"]
                if company_name == text.strip():  # Exact match
                    logger.debug(f"Found exact company name match: {code}")
                    return code, 1.0
                elif company_name in text:  # Contains full name
                    logger.debug(f"Found company name in text: {code}")
                    return code, 0.9
                    
        # Check aliases
        for code, data in self.client_data.items():
            for alias in data.get("aliases", []):
                if alias == text.strip():  # Exact alias match
                    logger.debug(f"Found exact alias match: {code}")
                    return code, 0.95
                elif alias in text:  # Contains alias
                    logger.debug(f"Found alias in text: {code}")
                    return code, 0.85
                    
        # Check email domains
        for code, data in self.client_data.items():
            for domain in data.get("domains", []):
                if f"@{domain}" in text.lower():
                    logger.debug(f"Found email domain match: {code}")
                    return code, 0.95
                    
        # Check code patterns (e.g., "EEA", "ARB")
        for code, data in self.client_data.items():
            if re.search(rf"\b{code}\b", text):
                logger.debug(f"Found code pattern match: {code}")
                return code, 0.8
                
        # Check compiled patterns for partial matches
        logger.debug("Checking compiled patterns:")
        for code, pattern in self.patterns["companies"].items():
            logger.debug(f"Pattern for {code}: {pattern.pattern}")
            if pattern.search(text):
                logger.debug(f"Found pattern match: {code}")
                return code, 0.75
                
        logger.debug("No match found")
        return None, 0.0
</file>

<file path="src/classifiers/factory.py">
"""
Classifier Factory

Manages the creation and configuration of document classifiers.
"""
from typing import Dict, Type
from .base import BaseDocumentClassifier
from .gemini import GeminiClassifier
from .docling import DoclingClassifier

class ClassifierFactory:
    """Factory for creating document classifier instances."""
    
    # Registry of available classifier implementations
    _registry: Dict[str, Type[BaseDocumentClassifier]] = {
        "gemini": GeminiClassifier,
        "docling": DoclingClassifier,
        # Add more implementations as they become available:
        # "openai": OpenAIClassifier,
        # "document_ai": GoogleDocumentAIClassifier,
    }
    
    @classmethod
    def register_classifier(cls, name: str, classifier_class: Type[BaseDocumentClassifier]) -> None:
        """
        Register a new classifier implementation.
        
        Args:
            name: Identifier for the classifier
            classifier_class: The classifier class to register
        """
        if not issubclass(classifier_class, BaseDocumentClassifier):
            raise ValueError(f"Classifier must implement BaseDocumentClassifier")
        cls._registry[name] = classifier_class
    
    @classmethod
    def create_classifier(cls, name: str, **kwargs) -> BaseDocumentClassifier:
        """
        Create an instance of the specified classifier.
        
        Args:
            name: Name of the classifier to create
            **kwargs: Configuration parameters for the classifier
            
        Returns:
            Configured classifier instance
            
        Raises:
            ValueError: If classifier name is not registered
        """
        if name not in cls._registry:
            raise ValueError(
                f"Unknown classifier: {name}. "
                f"Available classifiers: {list(cls._registry.keys())}"
            )
        
        classifier_class = cls._registry[name]
        return classifier_class(**kwargs)
    
    @classmethod
    def list_available_classifiers(cls) -> Dict[str, str]:
        """
        Get information about available classifier implementations.
        
        Returns:
            Dictionary mapping classifier names to their descriptions
        """
        info = {}
        for name, classifier_class in cls._registry.items():
            # Create a temporary instance to get info
            try:
                instance = classifier_class()
                info[name] = instance.get_classifier_info()["description"]
            except Exception as e:
                info[name] = f"(Configuration required) {str(e)}"
        return info
</file>

<file path="src/classifiers/gemini.py">
"""
Gemini Flash Document Classifier Implementation
"""
import os
from typing import Dict, List, Optional, Union, Tuple
import google.generativeai as genai
from pathlib import Path
import json
import asyncio
import io
import base64
import mimetypes
from .base import BaseDocumentClassifier, ClassificationResult
from .domain_config import DomainConfig

class GeminiClassifier(BaseDocumentClassifier):
    """Document classifier using Google's Gemini Flash model."""
    
    def __init__(self, api_key: Optional[str] = None, config_dir: Optional[Path] = None):
        """
        Initialize the Gemini classifier.
        
        Args:
            api_key: Google API key for Gemini
            config_dir: Optional path to configuration directory
        """
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("Google API key is required")
        
        # Configure the Gemini API
        genai.configure(api_key=self.api_key)
        # Initialize the model (using Flash for optimal speed/quality balance)
        self.model = genai.GenerativeModel('gemini-pro-vision')
        
        # Load domain configuration
        self.domain_config = DomainConfig(config_dir)
        
    async def classify_document(self, 
                              source: Union[str, Path, bytes],
                              source_type: str = "file",
                              metadata: Optional[Dict] = None) -> ClassificationResult:
        """Classify a single document using Gemini Flash."""
        try:
            # Process the document based on source type
            if source_type == "file":
                file_path = Path(source)
                if not file_path.exists():
                    raise FileNotFoundError(f"File not found: {file_path}")
                    
                # Check file size (20MB limit)
                file_size = file_path.stat().st_size
                if file_size > 20 * 1024 * 1024:  # 20MB in bytes
                    raise ValueError("File size exceeds 20MB limit")
                
                mime_type, _ = mimetypes.guess_type(str(file_path))
                with open(file_path, 'rb') as f:
                    file_content = f.read()
                    
            elif source_type == "bytes":
                file_content = source
                mime_type = "application/octet-stream"
                
            elif source_type == "text":
                # For text content, we'll use a different Gemini model
                text_model = genai.GenerativeModel('gemini-pro')
                raw_result = await self._process_text_content(source, text_model)
                if metadata:
                    raw_result = self._enhance_with_metadata(raw_result, metadata)
                return self._convert_to_classification_result(raw_result)
                
            else:
                raise ValueError(f"Unsupported source type: {source_type}")
            
            # For file and bytes, process with Gemini Vision
            raw_result = await self._process_binary_content(file_content, mime_type)
            
            # Enhance with metadata if provided
            if metadata:
                raw_result = self._enhance_with_metadata(raw_result, metadata)
                
            return self._convert_to_classification_result(raw_result)
            
        except Exception as e:
            return self._create_error_result(str(e))
            
    async def classify_batch(self, 
                           sources: List[Union[str, Path, bytes]],
                           source_type: str = "file",
                           metadata: Optional[List[Dict]] = None,
                           max_concurrent: int = 5) -> List[ClassificationResult]:
        """Classify multiple documents concurrently."""
        results = []
        metadata_list = metadata or [None] * len(sources)
        
        for i in range(0, len(sources), max_concurrent):
            batch_sources = sources[i:i + max_concurrent]
            batch_metadata = metadata_list[i:i + max_concurrent]
            tasks = [
                self.classify_document(source, source_type, md) 
                for source, md in zip(batch_sources, batch_metadata)
            ]
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Handle any exceptions in the batch
            for result in batch_results:
                if isinstance(result, Exception):
                    results.append(self._create_error_result(str(result)))
                else:
                    results.append(result)
                    
        return results
        
    async def _process_binary_content(self, content: bytes, mime_type: str) -> Dict:
        """Process binary content with Gemini Vision."""
        file_data = base64.b64encode(content).decode('utf-8')
        
        # Prepare the prompt for Gemini
        prompt = """
        Please analyze this document and extract the following information in JSON format:
        {
            "document_type": "The type of regulatory document (e.g., registration, renewal, tonnage report)",
            "entities": {
                "companies": ["List of company names mentioned"],
                "products": ["List of product names/types mentioned"],
                "states": ["List of US states mentioned"]
            },
            "key_fields": {
                "dates": ["Any important dates mentioned"],
                "registration_numbers": ["Any registration or license numbers"],
                "amounts": ["Any monetary amounts or quantities"]
            },
            "tables": ["Array of any tables found, each as a list of rows"],
            "summary": "A brief summary of the document's purpose and content",
            "text": "The full extracted text content from the document"
        }
        
        Please be precise and only include information that is explicitly present in the document.
        If any field has no relevant information, return an empty array or null.
        """
        
        # Process with Gemini
        response = await self.model.generate_content([{
            'parts': [
                {'text': prompt},
                {'inline_data': {
                    'mime_type': mime_type,
                    'data': file_data
                }}
            ]
        }])
        
        # Parse and validate the response
        try:
            return json.loads(response.text)
        except json.JSONDecodeError:
            raise ValueError("Failed to parse Gemini response as JSON")
            
    async def _process_text_content(self, content: str, model) -> Dict:
        """Process text content with Gemini Pro."""
        prompt = f"""
        Please analyze this text and extract the following information in JSON format:
        {{
            "document_type": "The type of regulatory document (e.g., registration, renewal, tonnage report)",
            "entities": {{
                "companies": ["List of company names mentioned"],
                "products": ["List of product names/types mentioned"],
                "states": ["List of US states mentioned"]
            }},
            "key_fields": {{
                "dates": ["Any important dates mentioned"],
                "registration_numbers": ["Any registration or license numbers"],
                "amounts": ["Any monetary amounts or quantities"]
            }},
            "summary": "A brief summary of the text's purpose and content",
            "text": {json.dumps(content)}
        }}
        
        Please be precise and only include information that is explicitly present in the text.
        If any field has no relevant information, return an empty array or null.
        """
        
        response = await model.generate_content(prompt)
        
        try:
            return json.loads(response.text)
        except json.JSONDecodeError:
            raise ValueError("Failed to parse Gemini response as JSON")
            
    def _enhance_with_metadata(self, raw_result: Dict, metadata: Dict) -> Dict:
        """Enhance classification results with provided metadata."""
        # Add metadata to existing result
        if "metadata" not in raw_result:
            raw_result["metadata"] = {}
        raw_result["metadata"]["source_metadata"] = metadata
        
        # If we have email metadata, use it to improve classification
        if "email_subject" in metadata:
            # Try to extract additional entities from subject
            subject_entities = self._extract_entities_from_text(metadata["email_subject"])
            raw_result["entities"]["companies"].extend(subject_entities.get("companies", []))
            raw_result["entities"]["states"].extend(subject_entities.get("states", []))
            
        return raw_result
        
    def _extract_entities_from_text(self, text: str) -> Dict[str, List[str]]:
        """Extract entities from a text string."""
        return {
            "companies": [match[1] for match in self.domain_config.get_company_codes(text)],
            "states": self.domain_config.get_states(text)
        }
        
    def get_classifier_info(self) -> Dict[str, str]:
        """Get information about this classifier implementation."""
        return {
            "name": "Gemini Flash Classifier",
            "version": "1.0.0",
            "description": "Uses Google's Gemini Flash model for document classification and extraction",
            "model": "gemini-pro-vision",
            "provider": "Google"
        }
        
    def _convert_to_classification_result(self, raw_result: Dict) -> ClassificationResult:
        """Convert raw Gemini output to standardized ClassificationResult."""
        # Get document text for domain validation
        doc_text = raw_result.get('text', '')
        
        # Use domain config to validate and enhance the classification
        doc_type, base_type = self.domain_config.get_document_type(doc_text)
        if not doc_type:
            doc_type = raw_result.get('document_type')
        
        # Get domain-validated states
        states = self.domain_config.get_states(doc_text)
        
        # Get company codes and names
        company_matches = self.domain_config.get_company_codes(doc_text)
        companies = raw_result.get('entities', {}).get('companies', [])
        company_codes = []
        if company_matches:
            # Add any missing companies from raw results
            companies.extend(match[1] for match in company_matches 
                           if match[1] not in companies)
            # Get the codes
            company_codes = [match[0] for match in company_matches]
        
        # Build entities dict
        entities = raw_result.get('entities', {'companies': [], 'products': [], 'states': []})
        entities['companies'] = companies
        entities['states'] = states
        
        # Validate registration numbers
        key_fields = raw_result.get('key_fields', {'dates': [], 'registration_numbers': [], 'amounts': []})
        if states:
            valid_numbers = []
            for number in key_fields.get('registration_numbers', []):
                if any(self.domain_config.validate_registration_number(number, state) 
                      for state in states):
                    valid_numbers.append(number)
            key_fields['registration_numbers'] = valid_numbers
        
        # Calculate confidence including domain validation
        base_confidence = self._calculate_confidence(raw_result)
        domain_confidence = self._calculate_domain_confidence(raw_result, doc_text)
        confidence = (base_confidence + domain_confidence) / 2
        
        # Generate flags including domain-specific ones
        flags = self._generate_flags(raw_result, confidence)
        
        # Add domain-specific metadata
        metadata = {
            'has_tables': bool(raw_result.get('tables')),
            'entity_count': sum(len(v) for v in entities.values()),
            'field_count': sum(len(v) for v in key_fields.values()),
            'classifier': self.get_classifier_info()['name'],
            'domain_confidence': domain_confidence,
            'product_categories': self.domain_config.get_product_categories(doc_text),
            'related_document_types': self.domain_config.get_related_documents(doc_type) if doc_type else [],
            'base_type': base_type,  # Add the standardized base type
            'company_codes': company_codes  # Add the standardized company codes
        }
        
        return ClassificationResult(
            document_type=doc_type,
            confidence=confidence,
            entities=entities,
            key_fields=key_fields,
            metadata=metadata,
            summary=raw_result.get('summary'),
            flags=flags
        )
        
    def _calculate_confidence(self, result: Dict) -> float:
        """Calculate a confidence score for the classification."""
        score = 0.0
        total_weights = 0.0
        
        weights = {
            'document_type': 0.3,
            'entities': 0.2,
            'key_fields': 0.3,
            'tables': 0.1,
            'summary': 0.1
        }
        
        for field, weight in weights.items():
            if field in result and result[field]:
                if field in ['entities', 'key_fields']:
                    # Check if any subfields have content
                    has_content = any(result[field].values())
                    score += weight if has_content else 0
                else:
                    score += weight
                total_weights += weight
                
        return round(score / total_weights, 2) if total_weights > 0 else 0.0
        
    def _calculate_domain_confidence(self, result: Dict, doc_text: str) -> float:
        """Calculate confidence based on domain validation."""
        score = 0.0
        total_checks = 0
        
        # Check document type against known types
        if result.get("document_type"):
            score += 1
            total_checks += 1
        
        # Check if found states are valid
        if self.domain_config.get_states(doc_text):
            score += 1
            total_checks += 1
        
        # Check if found product categories match our domain
        if self.domain_config.get_product_categories(doc_text):
            score += 1
            total_checks += 1
        
        # Check registration number validation
        key_fields = result.get("key_fields", {})
        if key_fields.get("registration_numbers"):
            score += 1
            total_checks += 1
        
        return round(score / total_checks, 2) if total_checks > 0 else 0.0
        
    def _generate_flags(self, result: Dict, confidence: float) -> List[str]:
        """Generate warning flags based on the classification results."""
        flags = []
        
        # Check confidence
        if confidence < 0.5:
            flags.append('LOW_CONFIDENCE')
            
        # Check for missing critical information
        if not result.get('document_type'):
            flags.append('MISSING_DOCUMENT_TYPE')
            
        # Check entities
        if not any(result.get('entities', {}).values()):
            flags.append('NO_ENTITIES_FOUND')
            
        # Check key fields
        key_fields = result.get('key_fields', {})
        if not key_fields.get('dates'):
            flags.append('NO_DATES_FOUND')
        if not key_fields.get('registration_numbers'):
            flags.append('NO_REGISTRATION_NUMBERS')
            
        # Add domain-specific flags
        doc_text = result.get('text', '')
        if not self.domain_config.get_product_categories(doc_text):
            flags.append('NO_PRODUCT_CATEGORY_MATCH')
            
        return flags
        
    def _create_error_result(self, error_message: str) -> ClassificationResult:
        """Create a ClassificationResult instance for error cases."""
        return ClassificationResult(
            document_type=None,
            confidence=0.0,
            entities={'companies': [], 'products': [], 'states': []},
            key_fields={'dates': [], 'registration_numbers': [], 'amounts': []},
            metadata={'error': error_message, 'classifier': 'Gemini Flash Classifier'},
            summary=None,
            flags=['CLASSIFICATION_ERROR']
        )
</file>

<file path="src/cli/cli_doc_labeler.py">
#!/usr/bin/env python3
"""Document Labeler CLI (Refactored)"""

import click
import json
import yaml
import shutil
import pathlib
import re
from typing import Dict, List, Optional
from datetime import datetime
from rich.console import Console
from rich.table import Table
from rich.progress import track
import questionary
from questionary import Choice
from prompt_toolkit.styles import Style
import builtins  # Import builtins

# --- Constants and Configuration ---
console = Console()

MOCHA = {
    "rosewater": "#f5e0dc",
    "flamingo": "#f2cdcd",
    "pink": "#f5c2e7",
    "mauve": "#cba6f7",
    "red": "#f38ba8",
    "maroon": "#eba0ac",
    "peach": "#fab387",
    "yellow": "#f9e2af",
    "green": "#a6e3a1",
    "teal": "#94e2d5",
    "sky": "#89dceb",
    "sapphire": "#74c7ec",
    "blue": "#89b4fa",
    "lavender": "#b4befe",
    "text": "#cdd6f4",
    "subtext1": "#bac2de",
    "subtext0": "#a6adc8",
    "overlay2": "#9399b2",
    "overlay1": "#7f849c",
    "overlay0": "#6c7086",
    "surface2": "#585b70",
    "surface1": "#45475a",
    "surface0": "#313244",
    "base": "#1e1e2e",
    "mantle": "#181825",
    "crust": "#11111b",
}

PROMPT_STYLE = Style.from_dict(
    {
        "qmark": f'{MOCHA["mauve"]} bold',
        "question": f'{MOCHA["lavender"]} bold',
        "answer": f'bold {MOCHA["green"]}',
        "pointer": f'bold {MOCHA["peach"]}',
        "highlighted": "",
        "selected": "",
        "separator": MOCHA["overlay0"],
        "instruction": MOCHA["overlay1"],
        "text": MOCHA["text"],
        "input": f'{MOCHA["text"]} bold',
        "choice": MOCHA["text"],
        "choice-selected": f'{MOCHA["text"]}',
        "valid": MOCHA["green"],
        "invalid": MOCHA["red"],
        "completion-menu": f'bg:{MOCHA["surface0"]}',
        "completion-item": MOCHA["text"],
        "completion-item selected": f'bg:{MOCHA["surface1"]} {MOCHA["text"]}',
    }
)


class Config:
    """Configuration settings for the application."""

    BASE_DIR = pathlib.Path("tests/fixtures/documents")
    TO_LABEL_DIR = BASE_DIR / "_to_label"
    LABELED_DIR = BASE_DIR / "labeled_documents/documents"
    METADATA_FILE = BASE_DIR / "labeled_documents/metadata.json"
    VALID_STATES = [
        "AL",
        "AK",
        "AZ",
        "AR",
        "CA",
        "CO",
        "CT",
        "DE",
        "FL",
        "GA",
        "HI",
        "ID",
        "IL",
        "IN",
        "IA",
        "KS",
        "KY",
        "LA",
        "ME",
        "MD",
        "MA",
        "MI",
        "MN",
        "MS",
        "MO",
        "MT",
        "NE",
        "NV",
        "NH",
        "NJ",
        "NM",
        "NY",
        "NC",
        "ND",
        "OH",
        "OK",
        "OR",
        "PA",
        "RI",
        "SC",
        "SD",
        "TN",
        "TX",
        "UT",
        "VT",
        "VA",
        "WA",
        "WV",
        "WI",
        "WY",
    ]
    CLIENT_CHOICES = {
        "AAS": "Able Ag Solutions, LLC",
        "AGR": "Agrauxine Corp.",
        "AND": "Andermatt US",
        "AQB": "AquaBella Organic Solutions LLC",
        "AQT": "Aquatrols Corp of American",
        "ARB": "Arborjet, Inc.",
        "BIN": "Bio Insumos Nativa SpA",
        "BIO": "BIOVERT SL",
        "BOR": "U.S. Borax Inc.",
        "BPT": "BioPro Technologies, LLC",
        "CLI": "Cytozyme Laboratories, Inc",
        "COM": "Comerco",
        "COR": "Corteva Agriscience LLC",
        "DED": "dedetec",
        "DEL": "Delta Analytical Corporation",
        "ECO": "Ecologel Solutions, LLC",
        "EEA": "Elemental Enzymes Ag and Turf, LLC",
        "GRN": "Greenwise Turf and Ag Solutions",
        "GWB": "Groundwork BioAg Ltd",
        "HIC": "Hi Cell Crop Science PVt. Ltd",
        "IBA": "Indogulf BioAg",
        "KIT": "KitoZyme",
        "KOC": "Kocide / Speiss-Urania",
        "LAM": "Lamberti, Inc",
        "LOC": "Locus Agriculture Solutions",
        "MAN": "Manvert USA LLC",
        "NLS": "NewLeaf Symbiotics",
        "OMC": "Omya Canada",
        "OMY": "Omya Inc.",
        "P66": "Phillips 66",
        "PET": "Petglow",
        "PLL": "Precision Laboratories Ltd",
        "PRO": "Probelte S.A.U.",
        "PVT": "Pivot Bio, Inc.",
        "ROY": "Royal Brinkman Canada",
        "SAG": "Solstice Agriculture, LLC",
        "SEI": "SEIPASA, S.A.",
        "SYM": "Symborg Inc",
        "TBP": "ThinkBio PTY",
        "VLS": "Verdesian Life Sciences US LLC",
        "ZZZ": "Company Automation Tester",
    }

    BASE_TYPE_DESCRIPTIONS = {
        "NEW": "New Registration",
        "RENEW": "Renewal",
        "TONNAGE": "Tonnage Report",
        "CERT": "Certificate",
        "LABEL": "Label Review",
    }
    VALID_BASE_TYPES = list(BASE_TYPE_DESCRIPTIONS.keys())
    VALID_PRODUCT_CATEGORIES = [
        "Biostimulants",
        "Commercial Fertilizers",
        "Plant and Soil Amendments",
        "Liming Materials",
        "Organic Input Materials",
    ]
    CLIENT_INFO = {  # Example - ideally loaded from a separate file
        "ARB": {
            "name": "Arborjet, Inc.",
            "contact_info": {
                "primary_contact": "Nicholas Millen",
                "email": "nmillen@arborjet.com",
            },
            "metadata": {"active_states": ["MA"]},
        }
    }

    @classmethod
    def get_client_choices_list(cls) -> List[str]:
        """Returns a formatted list of client choices."""
        return [f"{code}: {name}" for code, name in sorted(cls.CLIENT_CHOICES.items())]

    @classmethod
    def get_client_name(cls, client_code: str) -> str:
        """Returns the name of the client given the client code."""
        return cls.CLIENT_CHOICES.get(client_code, "")

    @classmethod
    def get_base_type_choices(cls) -> List[Choice]:
        """Return formatted choices for base type selection."""
        return [
            Choice(title=f"{code}: {desc}", value=code)
            for code, desc in cls.BASE_TYPE_DESCRIPTIONS.items()
        ]

    @classmethod
    def get_product_category_choices(cls) -> List[Choice]:
        """Return formatted choices for product category selection."""
        return [
            Choice(title=category, value=category)
            for category in sorted(cls.VALID_PRODUCT_CATEGORIES)
        ]

    @classmethod
    def ensure_directories(cls) -> None:
        """Ensure required directories exist."""
        cls.TO_LABEL_DIR.mkdir(parents=True, exist_ok=True)
        cls.LABELED_DIR.mkdir(parents=True, exist_ok=True)
        cls.METADATA_FILE.parent.mkdir(parents=True, exist_ok=True)


class DocumentMetadata:
    """Holds metadata for a document and validates its fields."""

    def __init__(
        self,
        state: str,
        client_code: str,
        base_type: str,
        expected_filename: str,
        description: Optional[str] = None,
        product_categories: Optional[List[str]] = None,
        last_updated: Optional[datetime] = None,
    ):
        self._state = ""
        self._client_code = ""
        self._base_type = ""
        self.state = state
        self.client_code = client_code
        self.base_type = base_type
        self.description = description
        self.product_categories = product_categories or []
        self.expected_filename = expected_filename
        self.last_updated = last_updated or datetime.now()
        self._validate_product_categories()

    @property
    def state(self) -> str:
        return self._state

    @state.setter
    def state(self, value: str) -> None:
        value = value.upper().strip()
        if value not in Config.VALID_STATES:
            raise ValueError(
                f"Invalid state code. Must be one of: {', '.join(Config.VALID_STATES)}"
            )
        self._state = value

    @property
    def client_code(self) -> str:
        return self._client_code

    @client_code.setter
    def client_code(self, value: str) -> None:
        value = value.upper().strip()
        if len(value) != 3 or not value.isalpha() or value not in Config.CLIENT_CHOICES:
            raise ValueError("Invalid client code. Must be a 3-letter valid code.")
        self._client_code = value

    @property
    def base_type(self) -> str:
        return self._base_type

    @base_type.setter
    def base_type(self, value: str) -> None:
        value = value.upper().strip()
        if value not in Config.VALID_BASE_TYPES:
            raise ValueError(
                f"Invalid base type. Must be one of: {', '.join(Config.VALID_BASE_TYPES)}"
            )
        self._base_type = value

    def _validate_product_categories(self) -> None:
        """Validate the provided product categories."""
        invalid = [
            cat
            for cat in self.product_categories
            if cat not in Config.VALID_PRODUCT_CATEGORIES
        ]
        if invalid:
            raise ValueError(
                f"Invalid product categories: {', '.join(invalid)}. "
                f"Valid options: {', '.join(Config.VALID_PRODUCT_CATEGORIES)}"
            )


class MetadataStore:
    """Stores and manages document metadata."""

    def __init__(self, metadata_file: pathlib.Path = Config.METADATA_FILE):
        self.metadata_file = metadata_file
        self.documents: Dict[str, DocumentMetadata] = {}
        self.last_updated: Optional[datetime] = None
        self.load()

    def load(self) -> None:
        """Load metadata from the JSON file."""
        try:
            with open(self.metadata_file, "r") as f:
                data = json.load(f)
                self.last_updated = datetime.fromisoformat(
                    data.get("last_updated", str(datetime.min))
                )

                for filename, meta in data.get("documents", {}).items():
                    try:
                        self.documents[filename] = DocumentMetadata(
                            state=meta["state"],
                            client_code=meta["client_code"],
                            base_type=meta["base_type"],
                            description=meta.get("description"),
                            product_categories=meta.get("product_categories", []),
                            expected_filename=meta["expected_filename"],
                            last_updated=datetime.fromisoformat(
                                meta.get("last_updated", str(datetime.min))
                            ),
                        )
                    except (KeyError, ValueError, TypeError) as e:
                        console.print(
                            f"[red]Invalid metadata entry {filename}: {e}[/red]"
                        )

        except (FileNotFoundError, json.JSONDecodeError):
            self.last_updated = datetime.min  # Initialize if file doesn't exist

    def save(self) -> None:
        """Save metadata to the JSON file."""
        serialized = {
            "last_updated": datetime.now().isoformat(),
            "documents": {
                filename: {
                    "state": doc.state,
                    "client_code": doc.client_code,
                    "base_type": doc.base_type,
                    "description": doc.description,
                    "product_categories": doc.product_categories,
                    "expected_filename": doc.expected_filename,
                    "last_updated": doc.last_updated.isoformat(),
                }
                for filename, doc in self.documents.items()
            },
        }
        with open(self.metadata_file, "w") as f:
            json.dump(serialized, f, indent=2)

    def add_or_update(self, filename: str, metadata: DocumentMetadata) -> None:
        """Add or update a document's metadata."""
        self.documents[filename] = metadata
        self.save()

    def get(self, filename: str) -> Optional[DocumentMetadata]:
        """Retrieve metadata for a given filename."""
        return self.documents.get(filename)


class FileHandler:
    """Handles file operations."""

    @staticmethod
    def move_or_copy(
        src_path: pathlib.Path,
        target_dir: pathlib.Path,
        new_filename: str,
        is_external: bool,
    ) -> None:
        """Move or copy a file to the target directory."""
        target_path = target_dir / new_filename
        try:
            if is_external:
                shutil.copy2(str(src_path), str(target_path))
                console.print(f"[green]Copied document to:[/green] {target_path}")
            else:
                shutil.move(str(src_path), str(target_path))
                console.print(f"[green]Moved document to:[/green] {target_path}")
        except (
            shutil.Error,
            OSError,
            PermissionError,
        ) as e:  # Catch specific file errors
            console.print(f"[red]Error moving/copying file:[/red] {e}")
            raise

    @staticmethod
    def generate_filename(meta_data: DocumentMetadata) -> str:
        """Generate a standardized filename."""
        filename = f"{meta_data.state}-{meta_data.client_code}-{meta_data.base_type}"
        if meta_data.description:
            filename += f"-{meta_data.description}"
        return filename + ".pdf"


class PromptHandler:
    """Handles user interaction for gathering metadata."""

    @staticmethod
    def prompt_for_metadata(
        existing_data: Optional[DocumentMetadata] = None,
    ) -> DocumentMetadata:
        """Prompt the user for document metadata, using existing data as defaults."""

        console.print(
            f"\n[bold {MOCHA['lavender']}]📄 Document Metadata Entry[/bold {MOCHA['lavender']}]"
        )

        # State
        console.print(
            f"┌─ [bold {MOCHA['overlay1']}]Step 1 of 4:[/bold {MOCHA['overlay1']}] Basic Information\n"
        )
        # Use questionary.prompt with a list of questions
        questions = [
            {
                "type": "autocomplete",
                "name": "state",  # Use a name for each question
                "message": "State code:",
                "choices": Config.VALID_STATES,
                "default": existing_data.state if existing_data else "",
                "validate": lambda x: x.upper() in Config.VALID_STATES,
                "style": PROMPT_STYLE,
            }
        ]
        answers = questionary.prompt(questions, style=PROMPT_STYLE)
        state = answers["state"]

        # Client Code
        console.print()
        questions = [
            {
                "type": "autocomplete",
                "name": "client_code",
                "message": "Client code:",
                "choices": Config.get_client_choices_list(),
                "default": (f"{existing_data.client_code}" if existing_data else ""),
                "validate": lambda x: x.split(":")[0].strip().upper()
                in Config.CLIENT_CHOICES,
                "style": PROMPT_STYLE,
            }
        ]
        answers = questionary.prompt(questions, style=PROMPT_STYLE)

        client_code = answers["client_code"].split(":")[0].strip()

        # Display client info
        PromptHandler.display_client_info(client_code)

        # Base Type (Refactored for rawselect and correct highlighting)
        console.print(
            f"\n┌─ [bold {MOCHA['overlay1']}]Step 2 of 4:[/bold {MOCHA['overlay1']}] Document Type\n"
        )
        base_type_choices = Config.get_base_type_choices()
        default_base_type = (
            existing_data.base_type
            if existing_data and existing_data.base_type in Config.VALID_BASE_TYPES
            else "NEW"
        )

        # Find the index of the default base type.
        try:
            default_index = [c.value for c in base_type_choices].index(
                default_base_type
            )
        except ValueError:
            default_index = 0  # Fallback to the first item if not found.

        # Reorder the choices list to put the default item first.
        reordered_choices = (
            base_type_choices[default_index:] + base_type_choices[:default_index]
        )

        questions = [
            {
                "type": "rawselect",
                "name": "base_type",
                "message": "Select base type:",
                "choices": reordered_choices,  # Use the reordered list.
                "style": PROMPT_STYLE,
                "use_indicator": True,
            }
        ]
        answers = questionary.prompt(questions, style=PROMPT_STYLE)
        base_type = answers["base_type"]

        # Description
        console.print(
            f"\n┌─ [bold {MOCHA['overlay1']}]Step 3 of 4:[/bold {MOCHA['overlay1']}] Description\n"
        )
        questions = [
            {
                "type": "text",
                "name": "description",
                "message": "Description (optional, use-hyphens-for-spaces):",
                "default": existing_data.description if existing_data else "",
                "style": PROMPT_STYLE,
            }
        ]
        answers = questionary.prompt(questions, style=PROMPT_STYLE)
        description = answers["description"]

        if description:
            description = re.sub(r"\s+", "-", description.lower())
            description = re.sub(r"[^\w-]", "", description)

        # Product Categories (Refactored for custom handling)
        console.print(
            f"\n┌─ [bold {MOCHA['overlay1']}]Step 4 of 4:[/bold {MOCHA['overlay1']}] Categories\n"
        )
        category_choices = Config.get_product_category_choices()
        selected_categories = []
        if existing_data and existing_data.product_categories:
            selected_categories = [
                cat
                for cat in existing_data.product_categories
                if cat in Config.VALID_PRODUCT_CATEGORIES
            ]

        choices_with_selection = [
            Choice(
                title=(
                    f"[X] {category}"
                    if category in selected_categories
                    else f"[ ] {category}"
                ),
                value=category,
            )
            for category in Config.VALID_PRODUCT_CATEGORIES
        ] + [Choice(title="[Done]", value="__DONE__")]
        questions = [
            {
                "type": "rawselect",
                "name": "product_categories",
                "message": "Select product categories (Enter to toggle, [Done] to finish):",
                "choices": choices_with_selection,
                "style": PROMPT_STYLE,
                "use_indicator": True,
            }
        ]
        selected_category = questionary.prompt(questions, style=PROMPT_STYLE)[
            "product_categories"
        ]

        if (
            selected_category != "__DONE__"
        ):  # Only enter loop if user didn't immediately select done
            while True:  # Custom loop for category selection
                if selected_category == "__DONE__":
                    break
                elif selected_category in selected_categories:
                    selected_categories.remove(selected_category)  # Toggle off
                else:
                    selected_categories.append(selected_category)  # Toggle on

                choices_with_selection = [
                    Choice(
                        title=(
                            f"[X] {category}"
                            if category in selected_categories
                            else f"[ ] {category}"
                        ),
                        value=category,
                    )
                    for category in Config.VALID_PRODUCT_CATEGORIES
                ] + [Choice(title="[Done]", value="__DONE__")]

                questions = [
                    {
                        "type": "rawselect",
                        "name": "product_categories",
                        "message": "Select product categories (Enter to toggle, [Done] to finish):",
                        "choices": choices_with_selection,
                        "style": PROMPT_STYLE,
                        "use_indicator": True,
                    }
                ]

                selected_category = questionary.prompt(questions, style=PROMPT_STYLE)[
                    "product_categories"
                ]

        console.print(
            f"\n[bold {MOCHA['green']}]✓ Metadata collection complete![/bold {MOCHA['green']}]\n"
        )
        product_categories = selected_categories
        # Create and return a new DocumentMetadata object
        return DocumentMetadata(
            state=state,
            client_code=client_code,
            base_type=base_type,
            description=description,
            product_categories=product_categories,
            expected_filename="",  # Placeholder, will be filled later.
        )

    @staticmethod
    def display_client_info(client_code: str) -> None:
        """Displays client information if available."""
        client_info = Config.CLIENT_INFO.get(client_code)
        if client_info:
            console.print(
                f"\n[bold {MOCHA['overlay1']}]Client Information:[/bold {MOCHA['overlay1']}]"
            )
            console.print(
                f"[{MOCHA['text']}]Name:[/{MOCHA['text']}] [{MOCHA['green']}]{client_info.get('name', '')}[/{MOCHA['green']}]"
            )
            console.print(
                f"[{MOCHA['text']}]Contact:[/{MOCHA['text']}] [{MOCHA['green']}]{client_info.get('contact_info', {}).get('primary_contact', '')}[/{MOCHA['green']}]"
            )
            console.print(
                f"[{MOCHA['text']}]Email:[/{MOCHA['text']}] [{MOCHA['green']}]{client_info.get('contact_info', {}).get('email', '')}[/{MOCHA['green']}]"
            )
            console.print(
                f"[{MOCHA['text']}]Active States:[/{MOCHA['text']}] [{MOCHA['green']}]{', '.join(client_info.get('metadata', {}).get('active_states', []))}[/{MOCHA['green']}]\n"
            )


class DocumentLabeler:
    """Main class to handle the document labeling process."""

    def __init__(self):
        self.config = Config()
        self.metadata_store = MetadataStore()
        self.file_handler = FileHandler()
        Config.ensure_directories()

    def label_documents(
        self, document_paths: List[pathlib.Path], batch_mode: bool = False
    ) -> None:
        """Label one or more documents."""
        if not document_paths:
            document_paths = list(self.config.TO_LABEL_DIR.glob("*.pdf"))
            if not document_paths:
                console.print(
                    "[yellow]No documents found in _to_label directory.[/yellow]"
                )
                return
            batch_mode = True  # If processing the _to_label dir, it's batch mode

        for doc_path in track(
            document_paths,
            description="Processing documents...",
            disable=not batch_mode,
        ):
            self.process_single_document(doc_path, batch_mode)

    def process_single_document(self, doc_path: str, batch_mode: bool) -> None:
        """Process a single document."""
        # Convert the string path back to a Path object.
        doc_path_obj = pathlib.Path(doc_path)
        console.print(
            f"\n[bold blue]Labeling document:[/bold blue] {doc_path_obj.name}"
        )
        is_external = self.config.TO_LABEL_DIR not in doc_path_obj.parents
        existing_data = self.metadata_store.get(doc_path_obj.name)

        try:
            metadata = PromptHandler.prompt_for_metadata(existing_data)
            metadata.expected_filename = self.file_handler.generate_filename(metadata)
            self.file_handler.move_or_copy(
                doc_path_obj,
                self.config.LABELED_DIR,
                metadata.expected_filename,
                is_external,
            )
            self.metadata_store.add_or_update(metadata.expected_filename, metadata)
            console.print(
                f"[green]Successfully labeled as:[/green] {metadata.expected_filename}"
            )
        except ValueError as e:
            self.handle_labeling_error(
                e, doc_path_obj, batch_mode
            )  # Pass Path object here too

    def handle_labeling_error(
        self, error: Exception, doc_path: pathlib.Path, batch_mode: bool
    ) -> None:
        """Handle errors during labeling."""
        console.print(f"[red]Validation error:[/red] {error}")
        if (
            not batch_mode
            and questionary.confirm("Would you like to try again?", default=True).ask()
        ):
            self.process_single_document(doc_path, batch_mode)

    def list_documents(self) -> None:
        """List all labeled documents and their metadata."""
        if not self.metadata_store.documents:
            console.print("[yellow]No labeled documents found.[/yellow]")
            return

        table = Table(title="Labeled Documents")
        table.add_column("Filename", style="cyan")
        table.add_column("State", style="magenta")
        table.add_column("Type", style="green")
        table.add_column("Client", style="yellow")
        table.add_column("Categories", style="blue")

        for doc_name, doc_data in self.metadata_store.documents.items():
            table.add_row(
                doc_name,
                doc_data.state,
                doc_data.base_type,
                doc_data.client_code,
                (
                    ", ".join(doc_data.product_categories)
                    if doc_data.product_categories
                    else "None"
                ),
            )
        console.print(table)

    def show_status(self) -> None:
        """Show the status of documents."""
        to_label = list(self.config.TO_LABEL_DIR.glob("*.pdf"))
        labeled = list(self.config.LABELED_DIR.glob("*.pdf"))

        console.print(f"\n[bold]Documents Status:[/bold]")
        console.print(f"Waiting to be labeled: [yellow]{len(to_label)}[/yellow]")
        console.print(f"Successfully labeled: [green]{len(labeled)}[/green]")

        if to_label:
            console.print("\n[bold]Documents waiting for labels:[/bold]")
            for doc in to_label:
                console.print(f"- {doc.name}")


# --- Click CLI ---
@click.group()
def cli():
    """Document labeling system."""
    pass


@cli.command()
@click.argument(
    "document_paths", nargs=-1, type=click.Path(exists=True, path_type=pathlib.Path)
)
def label(document_paths: List[pathlib.Path]):
    """Label one or more documents."""
    labeler = DocumentLabeler()
    # Convert Path objects to strings before passing to label_documents
    labeler.label_documents([str(path) for path in document_paths])


@cli.command()
def list():
    """List all labeled documents."""
    labeler = DocumentLabeler()
    labeler.list_documents()


@cli.command()
def status():
    """Show the status of documents."""
    labeler = DocumentLabeler()
    labeler.show_status()


if __name__ == "__main__":
    cli()
</file>

<file path="src/cli/document_classifier.py">
"""
Document Classification CLI

Command-line interface for classifying documents using our classification service.
"""
import asyncio
import click
from pathlib import Path
from typing import Optional
from ..services.classification_service import ClassificationService

@click.group()
def cli():
    """Document classification command line interface."""
    pass

@cli.command()
@click.argument('source', type=click.Path(exists=True))
@click.option('--classifier', '-c', default='docling',
              type=click.Choice(['docling', 'gemini']),
              help='Classifier to use')
@click.option('--config-dir', type=click.Path(exists=True),
              help='Path to configuration directory')
@click.option('--metadata', '-m', multiple=True,
              help='Additional metadata in key=value format')
async def classify(source: str,
                  classifier: str,
                  config_dir: Optional[str],
                  metadata: tuple) -> None:
    """Classify a single document or directory of documents."""
    service = ClassificationService(
        config_dir=Path(config_dir) if config_dir else None,
        classifier_name=classifier
    )
    
    # Parse metadata
    metadata_dict = {}
    for item in metadata:
        key, value = item.split('=', 1)
        metadata_dict[key.strip()] = value.strip()
    
    source_path = Path(source)
    if source_path.is_file():
        # Classify single file
        result = await service.classify_document(
            source_path,
            source_type="file",
            metadata=metadata_dict
        )
        _display_result(result)
    elif source_path.is_dir():
        # Classify all files in directory
        files = list(source_path.glob("*.*"))
        results = await service.classify_batch(
            files,
            source_type="file",
            metadata=[metadata_dict] * len(files)
        )
        for file_path, result in zip(files, results):
            click.echo(f"\nFile: {file_path}")
            _display_result(result)
            
@cli.command()
@click.argument('directory', type=click.Path(exists=True))
@click.option('--classifier', '-c', default='docling',
              type=click.Choice(['docling', 'gemini']),
              help='Classifier to use')
@click.option('--config-dir', type=click.Path(exists=True),
              help='Path to configuration directory')
@click.option('--pattern', default="*.*",
              help='Glob pattern for files to watch')
@click.option('--recursive/--no-recursive', default=False,
              help='Watch subdirectories recursively')
async def watch(directory: str,
               classifier: str,
               config_dir: Optional[str],
               pattern: str,
               recursive: bool) -> None:
    """Watch a directory for new documents and classify them."""
    service = ClassificationService(
        config_dir=Path(config_dir) if config_dir else None,
        classifier_name=classifier
    )
    
    click.echo(f"Watching directory: {directory}")
    click.echo(f"Pattern: {pattern}")
    click.echo(f"Recursive: {recursive}")
    click.echo(f"Classifier: {classifier}")
    click.echo("\nPress Ctrl+C to stop watching\n")
    
    await service.watch_directory(
        directory=directory,
        pattern=pattern,
        recursive=recursive
    )

def _display_result(result):
    """Display classification result in a formatted way."""
    click.echo("\nClassification Result:")
    click.echo(f"Document Type: {result.document_type}")
    click.echo(f"Confidence: {result.confidence:.2%}")
    
    if result.entities:
        click.echo("\nEntities:")
        for entity_type, entities in result.entities.items():
            if entities:
                click.echo(f"  {entity_type}:")
                for entity in entities:
                    click.echo(f"    - {entity}")
                    
    if result.key_fields:
        click.echo("\nKey Fields:")
        for field_type, fields in result.key_fields.items():
            if fields:
                click.echo(f"  {field_type}:")
                for field in fields:
                    click.echo(f"    - {field}")
                    
    if result.flags:
        click.echo("\nFlags:")
        for flag in result.flags:
            click.echo(f"  - {flag}")
            
    if result.summary:
        click.echo(f"\nSummary: {result.summary}")

def main():
    """Entry point for the CLI."""
    asyncio.run(cli())
</file>

<file path="src/cli/document_labeler.py">
#!/usr/bin/env python3
"""
Document Labeler CLI

This script allows you to label documents by interactively gathering metadata
about each document. It validates the user input, generates a standardized
filename, moves (or copies) the document to the proper directory, and then
updates a metadata JSON file for future reference.
"""

import click
import json
import yaml
import shutil
import pathlib
import re
from typing import Dict, List, Optional
from datetime import datetime
from rich.console import Console
from rich.table import Table
from rich.progress import track
import questionary
from questionary import Choice
from prompt_toolkit.styles import Style
import builtins

console = Console()

# Directory structure
BASE_DIR = pathlib.Path("tests/fixtures/documents")
TO_LABEL_DIR = BASE_DIR / "_to_label"
LABELED_DIR = BASE_DIR / "labeled_documents/documents"
METADATA_FILE = BASE_DIR / "labeled_documents/metadata.json"


# Catppuccin Mocha palette
MOCHA = {
    "rosewater": "#f5e0dc",
    "flamingo": "#f2cdcd",
    "pink": "#f5c2e7",
    "mauve": "#cba6f7",
    "red": "#f38ba8",
    "maroon": "#eba0ac",
    "peach": "#fab387",
    "yellow": "#f9e2af",
    "green": "#a6e3a1",
    "teal": "#94e2d5",
    "sky": "#89dceb",
    "sapphire": "#74c7ec",
    "blue": "#89b4fa",
    "lavender": "#b4befe",
    "text": "#cdd6f4",
    "subtext1": "#bac2de",
    "subtext0": "#a6adc8",
    "overlay2": "#9399b2",
    "overlay1": "#7f849c",
    "overlay0": "#6c7086",
    "surface2": "#585b70",
    "surface1": "#45475a",
    "surface0": "#313244",
    "base": "#1e1e2e",
    "mantle": "#181825",
    "crust": "#11111b",
}

PROMPT_STYLE = Style.from_dict(
    {
        # Base components
        "qmark": f'{MOCHA["mauve"]} bold',  # Question mark
        "question": f'{MOCHA["lavender"]} bold',  # Question text
        "answer": f'bold {MOCHA["green"]}',  # Selected answer appearance
        "pointer": f'bold {MOCHA["peach"]}',  # Make the pointer (arrow) more visible
        "highlighted": "",  # Better contrast for highlighted item
        "selected": "",  # More subtle highlight for selected item
        "separator": MOCHA["overlay0"],  # Separator in lists
        "instruction": MOCHA["overlay1"],  # Instructions/help text
        # Input components
        "text": MOCHA["text"],  # Regular text
        "input": f'{MOCHA["text"]} bold',  # User input text
        # Choice components
        "choice": MOCHA["text"],  # Normal choice appearance
        "choice-selected": f'{MOCHA["text"]}',  # Selected choice appearance
        # Default validators
        "valid": MOCHA["green"],  # Valid input
        "invalid": MOCHA["red"],  # Invalid input
        # Completion components
        "completion-menu": f'bg:{MOCHA["surface0"]}',
        "completion-item": MOCHA["text"],
        "completion-item selected": f'bg:{MOCHA["surface1"]} {MOCHA["text"]}',
    }
)


class DocumentMetadata:
    """Holds metadata for a document and validates its fields."""

    def __init__(
        self,
        state: str,
        client_code: str,
        base_type: str,
        expected_filename: str,
        description: Optional[str] = None,
        product_categories: Optional[List[str]] = None,
        last_updated: Optional[datetime] = None,
    ):
        self.state = validate_state(state)
        self.client_code = validate_client_code(client_code)
        self.base_type = validate_base_type(base_type)
        self.description = description
        self.product_categories = product_categories or []
        self.expected_filename = expected_filename
        self.last_updated = last_updated or datetime.now()

        # Validate product categories after assignment.
        validate_product_categories(self.product_categories)


class MetadataStore:
    """Stores all document metadata."""

    def __init__(self):
        self.documents: Dict[str, DocumentMetadata] = {}
        self.last_updated: Optional[datetime] = None


def load_yaml_config(filename: str) -> dict:
    """Load a YAML configuration file from the src/config directory."""
    config_path = pathlib.Path("src/config") / filename
    try:
        with open(config_path, "r") as f:
            return yaml.safe_load(f) or {}
    except FileNotFoundError:
        console.print(f"[yellow]Warning: Config file {filename} not found[/yellow]")
        return {}


def get_valid_states() -> List[str]:
    """Return a sorted list of valid state codes."""
    return sorted(
        [
            "AL",
            "AK",
            "AZ",
            "AR",
            "CA",
            "CO",
            "CT",
            "DE",
            "FL",
            "GA",
            "HI",
            "ID",
            "IL",
            "IN",
            "IA",
            "KS",
            "KY",
            "LA",
            "ME",
            "MD",
            "MA",
            "MI",
            "MN",
            "MS",
            "MO",
            "MT",
            "NE",
            "NV",
            "NH",
            "NJ",
            "NM",
            "NY",
            "NC",
            "ND",
            "OH",
            "OK",
            "OR",
            "PA",
            "RI",
            "SC",
            "SD",
            "TN",
            "TX",
            "UT",
            "VT",
            "VA",
            "WA",
            "WV",
            "WI",
            "WY",
        ]
    )


def get_client_choices_dict() -> Dict[str, str]:
    """Return a dictionary of client codes and names."""
    return {
        "AAS": "Able Ag Solutions, LLC",
        "AGR": "Agrauxine Corp.",
        "AND": "Andermatt US",
        "AQB": "AquaBella Organic Solutions LLC",
        "AQT": "Aquatrols Corp of American",
        "ARB": "Arborjet, Inc.",
        "BIN": "Bio Insumos Nativa SpA",
        "BIO": "BIOVERT SL",
        "BOR": "U.S. Borax Inc.",
        "BPT": "BioPro Technologies, LLC",
        "CLI": "Cytozyme Laboratories, Inc",
        "COM": "Comerco",
        "COR": "Corteva Agriscience LLC",
        "DED": "dedetec",
        "DEL": "Delta Analytical Corporation",
        "ECO": "Ecologel Solutions, LLC",
        "EEA": "Elemental Enzymes Ag and Turf, LLC",
        "GRN": "Greenwise Turf and Ag Solutions",
        "GWB": "Groundwork BioAg Ltd",
        "HIC": "Hi Cell Crop Science PVt. Ltd",
        "IBA": "Indogulf BioAg",
        "KIT": "KitoZyme",
        "KOC": "Kocide / Speiss-Urania",
        "LAM": "Lamberti, Inc",
        "LOC": "Locus Agriculture Solutions",
        "MAN": "Manvert USA LLC",
        "NLS": "NewLeaf Symbiotics",
        "OMC": "Omya Canada",
        "OMY": "Omya Inc.",
        "P66": "Phillips 66",
        "PET": "Petglow",
        "PLL": "Precision Laboratories Ltd",
        "PRO": "Probelte S.A.U.",
        "PVT": "Pivot Bio, Inc.",
        "ROY": "Royal Brinkman Canada",
        "SAG": "Solstice Agriculture, LLC",
        "SEI": "SEIPASA, S.A.",
        "SYM": "Symborg Inc",
        "TBP": "ThinkBio PTY",
        "VLS": "Verdesian Life Sciences US LLC",
        "ZZZ": "Company Automation Tester",
    }


def get_base_type_choices() -> List[Choice]:
    """Return formatted choices for base type selection."""
    descriptions = {
        "NEW": "New Registration",
        "RENEW": "Renewal",
        "TONNAGE": "Tonnage Report",
        "CERT": "Certificate",
        "LABEL": "Label Review",
    }
    return [
        Choice(title=f"{code}: {desc}", value=code)
        for code, desc in descriptions.items()
    ]


def get_client_info(client_code: str) -> dict:
    """Return detailed client information based on client code."""
    # Hardcoded client information
    clients_info = {
        "ARB": {
            "name": "Arborjet, Inc.",
            "contact_info": {
                "primary_contact": "Nicholas Millen",
                "email": "nmillen@arborjet.com",
            },
            "metadata": {"active_states": ["MA"]},
        }
        # Add other clients as needed
    }
    return clients_info.get(client_code, {})


def get_valid_client_codes() -> List[str]:
    """Return a sorted list of valid client codes."""
    return sorted(
        [
            "AAS",
            "AGR",
            "AND",
            "AQB",
            "AQT",
            "ARB",
            "BIN",
            "BIO",
            "BOR",
            "BPT",
            "CLI",
            "COM",
            "COR",
            "DED",
            "DEL",
            "ECO",
            "EEA",
            "GRN",
            "GWB",
            "HIC",
            "IBA",
            "KIT",
            "KOC",
            "LAM",
            "LOC",
            "MAN",
            "NLS",
            "OMC",
            "OMY",
            "P66",
            "PET",
            "PLL",
            "PRO",
            "PVT",
            "ROY",
            "SAG",
            "SEI",
            "SYM",
            "TBP",
            "VLS",
            "ZZZ",
        ]
    )


def get_valid_base_types() -> List[str]:
    """Return a list of valid base types."""
    return ["NEW", "RENEW", "TONNAGE", "CERT", "LABEL"]


def get_valid_product_categories() -> List[str]:
    """Return a sorted list of valid product categories."""
    return sorted(
        [
            "Biostimulants",
            "Commercial Fertilizers",
            "Plant and Soil Amendments",
            "Liming Materials",
            "Organic Input Materials",
        ]
    )


def get_product_category_choices() -> List[Choice]:
    """Return formatted choices for product category selection."""
    return [
        Choice(title=category, value=category)
        for category in get_valid_product_categories()
    ]


def ensure_directories() -> None:
    """Ensure required directories exist."""
    TO_LABEL_DIR.mkdir(parents=True, exist_ok=True)
    LABELED_DIR.mkdir(parents=True, exist_ok=True)
    METADATA_FILE.parent.mkdir(parents=True, exist_ok=True)


def validate_state(state: str) -> str:
    """Validate and format a state code."""
    state = state.upper().strip()
    if state not in get_valid_states():
        raise ValueError(
            f"Invalid state code. Must be one of: {', '.join(get_valid_states())}"
        )
    return state


def validate_client_code(code: str) -> str:
    """Validate and format a client code."""
    code = code.upper().strip()
    if len(code) != 3 or not code.isalpha():
        raise ValueError("Client code must be exactly 3 alphabetic letters")
    if code not in get_valid_client_codes():
        raise ValueError(
            f"Invalid client code. Must be one of: {', '.join(get_valid_client_codes())}"
        )
    return code


def validate_base_type(base_type: str) -> str:
    """Validate and format a base type."""
    base_type = base_type.upper().strip()
    if base_type not in get_valid_base_types():
        raise ValueError(
            f"Invalid base type. Must be one of: {', '.join(get_valid_base_types())}"
        )
    return base_type


def validate_product_categories(categories: List[str]) -> None:
    """Validate the provided product categories."""
    valid_categories = get_valid_product_categories()
    invalid = [cat for cat in categories if cat not in valid_categories]
    if invalid:
        raise ValueError(
            f"Invalid product categories: {', '.join(invalid)}. "
            f"Valid options: {', '.join(valid_categories)}"
        )


def load_metadata() -> MetadataStore:
    """Load existing metadata from file."""
    store = MetadataStore()
    try:
        with open(METADATA_FILE, "r") as f:
            data = json.load(f)
        last_updated_str = data.get("last_updated")
        if not last_updated_str:
            store.last_updated = datetime.min
        else:
            try:
                store.last_updated = datetime.fromisoformat(last_updated_str)
            except (ValueError, TypeError):
                store.last_updated = datetime.min

        for filename, meta in data.get("documents", {}).items():
            try:
                doc_last_updated = datetime.min
                if meta.get("last_updated"):
                    try:
                        doc_last_updated = datetime.fromisoformat(meta["last_updated"])
                    except (ValueError, TypeError):
                        pass

                store.documents[filename] = DocumentMetadata(
                    state=meta["state"],
                    client_code=meta["client_code"],
                    base_type=meta["base_type"],
                    description=meta.get("description"),
                    product_categories=meta.get("product_categories", []),
                    expected_filename=meta["expected_filename"],
                    last_updated=doc_last_updated,
                )
            except (KeyError, ValueError) as e:
                console.print(f"[red]Invalid metadata entry {filename}: {e}[/red]")
    except (FileNotFoundError, json.JSONDecodeError):
        pass
    return store


def save_metadata(metadata: MetadataStore) -> None:
    """Save the metadata store to a file."""
    serialized = {
        "last_updated": datetime.now().isoformat(),
        "documents": {
            filename: {
                "state": doc.state,
                "client_code": doc.client_code,
                "base_type": doc.base_type,
                "description": doc.description,
                "product_categories": doc.product_categories,
                "expected_filename": doc.expected_filename,
                "last_updated": doc.last_updated.isoformat(),
            }
            for filename, doc in metadata.documents.items()
        },
    }
    with open(METADATA_FILE, "w") as f:
        json.dump(serialized, f, indent=2)


def prompt_for_metadata(existing_data: Optional[DocumentMetadata] = None) -> dict:
    """Interactively prompt the user for document metadata."""
    console.print(
        f"\n[bold {MOCHA['lavender']}]📄 Document Metadata Entry[/bold {MOCHA['lavender']}]"
    )
    console.print(
        f"┌─ [bold {MOCHA['overlay1']}]Step 1 of 4:[/bold {MOCHA['overlay1']}] Basic Information\n"
    )

    # Get state code
    valid_states = get_valid_states()
    state = questionary.autocomplete(
        "State code:",
        choices=valid_states,
        default=existing_data.state if existing_data is not None else "",
        validate=lambda x: x.upper() in valid_states,
        style=PROMPT_STYLE,
    ).ask()

    console.print()  # Add vertical spacing

    # Get client code with company name
    valid_clients = [
        f"{code}: {name}" for code, name in sorted(get_client_choices_dict().items())
    ]
    client_response = questionary.autocomplete(
        "Client code:",
        choices=valid_clients,
        default=(f"{existing_data.client_code}" if existing_data is not None else ""),
        validate=lambda x: x.split(":")[0].strip().upper() in get_valid_client_codes(),
        style=PROMPT_STYLE,
    ).ask()
    client_code = client_response.split(":")[0].strip()

    # Display client information if available
    client_info = get_client_info(client_code)
    if client_info:
        console.print(
            f"\n[bold {MOCHA['overlay1']}]Client Information:[/bold {MOCHA['overlay1']}]"
        )
        console.print(
            f"[{MOCHA['text']}]Name:[/{MOCHA['text']}] [{MOCHA['green']}]{client_info.get('name', '')}[/{MOCHA['green']}]"
        )
        console.print(
            f"[{MOCHA['text']}]Contact:[/{MOCHA['text']}] [{MOCHA['green']}]{client_info.get('contact_info', {}).get('primary_contact', '')}[/{MOCHA['green']}]"
        )
        console.print(
            f"[{MOCHA['text']}]Email:[/{MOCHA['text']}] [{MOCHA['green']}]{client_info.get('contact_info', {}).get('email', '')}[/{MOCHA['green']}]"
        )
        console.print(
            f"[{MOCHA['text']}]Active States:[/{MOCHA['text']}] [{MOCHA['green']}]{', '.join(client_info.get('metadata', {}).get('active_states', []))}[/{MOCHA['green']}]\n"
        )

    # Base type selection using checkbox
    console.print(
        f"\n┌─ [bold {MOCHA['overlay1']}]Step 2 of 4:[/bold {MOCHA['overlay1']}] Document Type\n"
    )

    valid_base_types = get_valid_base_types()
    base_type_choices = get_base_type_choices()
    default_base_type = []
    if existing_data is not None and existing_data.base_type:
        if existing_data.base_type in valid_base_types:
            default_base_type = [existing_data.base_type]

    ### Base type using checkbox - restricting to single selection in post-processing
    selected_base_types = questionary.checkbox(
        "Select base type:",
        choices=base_type_choices,
        default=default_base_type if default_base_type else None,
        style=PROMPT_STYLE,
    ).ask()

    # Process selection to ensure only one type is selected
    base_type = None
    if selected_base_types and isinstance(selected_base_types, list):
        selected_base_types = [
            type_
            for type_ in selected_base_types
            if isinstance(type_, str) and type_ in valid_base_types
        ]
        if selected_base_types:
            base_type = selected_base_types[0]  # Take the first selected type

    # If no valid selection was made, default to NEW
    if not base_type:
        base_type = "NEW"
    # Get description
    console.print(
        f"\n┌─ [bold {MOCHA['overlay1']}]Step 3 of 4:[/bold {MOCHA['overlay1']}] Description\n"
    )

    ## Description
    description = questionary.text(
        "Description (optional, use-hyphens-for-spaces):",
        default=existing_data.description if existing_data is not None else "",
        style=PROMPT_STYLE,
    ).ask()

    if description:
        description = description.lower()
        description = re.sub(r"\s+", "-", description)
        description = re.sub(r"[^\w-]", "", description)

    # Get product categories
    console.print(
        f"\n┌─ [bold {MOCHA['overlay1']}]Step 4 of 4:[/bold {MOCHA['overlay1']}] Categories\n"
    )

    valid_categories = get_valid_product_categories()
    category_choices = get_product_category_choices()
    default_categories = []
    if existing_data is not None and existing_data.product_categories:
        default_categories = [
            cat for cat in existing_data.product_categories if cat in valid_categories
        ]

    # Get product categories using checkbox
    product_categories = questionary.checkbox(
        "Select product categories (optional):",
        choices=category_choices + [Choice(title="[Done]", value="__DONE__")],
        default=default_categories if default_categories else None,
        style=PROMPT_STYLE,
    ).ask()

    # Sanitize product categories
    if not product_categories or not isinstance(product_categories, list):
        product_categories = []
    else:
        product_categories = [
            cat
            for cat in product_categories
            if isinstance(cat, str) and cat in valid_categories and cat != "__DONE__"
        ]

    console.print(
        f"\n[bold {MOCHA['green']}]✓ Metadata collection complete![/bold {MOCHA['green']}]\n"
    )

    return {
        "state": state.upper(),
        "client_code": client_code.upper(),
        "base_type": base_type.upper(),
        "description": description or None,
        "product_categories": product_categories,
    }


def generate_filename(meta_data: dict) -> str:
    """
    Generate a standardized filename from the metadata dictionary.

    The filename is constructed in the following format:
      STATE-CLIENT-BASETYPE[-description].pdf
    """
    filename = (
        f"{meta_data['state']}-{meta_data['client_code']}-{meta_data['base_type']}"
    )
    if meta_data.get("description"):
        filename += f"-{meta_data['description']}"
    return filename + ".pdf"


@click.group()
def cli():
    """Document labeling system for classification testing."""
    ensure_directories()


@cli.command()
@click.argument(
    "document_path", nargs=-1, type=click.Path(exists=True, path_type=pathlib.Path)
)
def label(document_path):
    """
    Label one or more documents. If no paths are provided, process all documents in the _to_label directory.
    """
    metadata = load_metadata()
    if document_path:
        for path in document_path:
            process_single_document(path, metadata)
    else:
        docs_to_label = list(TO_LABEL_DIR.glob("*.pdf"))
        if not docs_to_label:
            console.print("[yellow]No documents found in _to_label directory.[/yellow]")
            return
        for doc_path in track(docs_to_label, description="Processing documents..."):
            process_single_document(doc_path, metadata, batch_mode=True)


def process_single_document(
    doc_path: pathlib.Path, metadata: MetadataStore, batch_mode: bool = False
) -> None:
    """Process a single document: prompt for metadata, generate a new filename, and update metadata."""
    console.print(f"\n[bold blue]Labeling document:[/bold blue] {doc_path.name}")
    is_external = TO_LABEL_DIR not in doc_path.parents
    doc_key = doc_path.name
    existing_data = metadata.documents.get(doc_key)

    try:
        meta_data = prompt_for_metadata(existing_data)
        new_filename = generate_filename(meta_data)
        doc_entry = DocumentMetadata(**meta_data, expected_filename=new_filename)
        handle_file_move(doc_path, new_filename, is_external)
        update_metadata(metadata, new_filename, doc_entry)
        console.print(f"[green]Successfully labeled as:[/green] {new_filename}")
    except ValueError as e:
        handle_labeling_error(e, doc_path, metadata, batch_mode)


def handle_file_move(
    src_path: pathlib.Path, new_filename: str, is_external: bool
) -> None:
    """Move or copy the file to the labeled directory."""
    target_path = LABELED_DIR / new_filename
    try:
        if is_external:
            shutil.copy2(str(src_path), str(target_path))
        else:
            shutil.move(str(src_path), str(target_path))
        console.print(
            f"[green]{'Copied' if is_external else 'Moved'} document to:[/green] {target_path}"
        )
    except Exception as e:
        console.print(f"[red]Error moving file:[/red] {e}")
        raise


def update_metadata(
    metadata: MetadataStore, filename: str, entry: DocumentMetadata
) -> None:
    """Update the metadata store and save changes."""
    metadata.documents[filename] = entry
    save_metadata(metadata)


def handle_labeling_error(
    error: Exception, doc_path: pathlib.Path, metadata: MetadataStore, batch_mode: bool
) -> None:
    """Display the error and, if appropriate, allow the user to retry labeling."""
    console.print(f"[red]Validation error:[/red] {error}")
    if (
        not batch_mode
        and questionary.confirm("Would you like to try again?", default=True).ask()
    ):
        process_single_document(doc_path, metadata)


@cli.command()
def list():
    """List all labeled documents and their metadata."""
    metadata = load_metadata()
    if not metadata.documents:
        console.print("[yellow]No labeled documents found.[/yellow]")
        return

    table = Table(title="Labeled Documents")
    table.add_column("Filename", style="cyan")
    table.add_column("State", style="magenta")
    table.add_column("Type", style="green")
    table.add_column("Client", style="yellow")
    table.add_column("Categories", style="blue")

    for doc_name, doc_data in metadata.documents.items():
        table.add_row(
            doc_name,
            doc_data.state,
            doc_data.base_type,
            doc_data.client_code,
            (
                ", ".join(doc_data.product_categories)
                if doc_data.product_categories
                else "None"
            ),
        )
    console.print(table)


@cli.command()
def status():
    """Show the status of documents waiting to be labeled."""
    to_label = list(TO_LABEL_DIR.glob("*.pdf"))
    labeled = list(LABELED_DIR.glob("*.pdf"))

    console.print(f"\n[bold]Documents Status:[/bold]")
    console.print(f"Waiting to be labeled: [yellow]{len(to_label)}[/yellow]")
    console.print(f"Successfully labeled: [green]{len(labeled)}[/green]")

    if to_label:
        console.print("\n[bold]Documents waiting for labels:[/bold]")
        for doc in to_label:
            console.print(f"- {doc.name}")


if __name__ == "__main__":
    cli()
</file>

<file path="src/client/__init__.py">
from src.client.gmail import Gmail
from src.client import query
from src.client import label

__all__ = ['Gmail', 'query', 'label']
</file>

<file path="src/client/attachment.py">
"""
File: attachment.py
-------------------
This module contains the implementation of the Attachment object.

"""

import base64  # for base64.urlsafe_b64decode
import os      # for os.path.exists
from typing import Optional

class Attachment(object):
    """
    The Attachment class for attachments to emails in your Gmail mailbox. This 
    class should not be manually instantiated.

    Args:
        service: The Gmail service object.
        user_id: The username of the account the message belongs to.
        msg_id: The id of message the attachment belongs to.
        att_id: The id of the attachment.
        filename: The filename associated with the attachment.
        filetype: The mime type of the file.
        data: The raw data of the file. Default None.

    Attributes:
        _service (googleapiclient.discovery.Resource): The Gmail service object.
        user_id (str): The username of the account the message belongs to.
        msg_id (str): The id of message the attachment belongs to.
        id (str): The id of the attachment.
        filename (str): The filename associated with the attachment.
        filetype (str): The mime type of the file.
        data (bytes): The raw data of the file.

    """
    
    def __init__(
        self,
        service: 'googleapiclient.discovery.Resource',
        user_id: str,
        msg_id: str,
        att_id: str,
        filename: str,
        filetype: str,
        data: Optional[bytes] = None
    ) -> None:
        self._service = service
        self.user_id = user_id
        self.msg_id = msg_id
        self.id = att_id
        self.filename = filename
        self.filetype = filetype
        self.data = data

    def download(self) -> None:
        """
        Downloads the data for an attachment if it does not exist.
        
        Raises:
            googleapiclient.errors.HttpError: There was an error executing the 
                HTTP request.
        
        """
        
        if self.data is not None:
            return

        res = self._service.users().messages().attachments().get(
            userId=self.user_id, messageId=self.msg_id, id=self.id
        ).execute()

        data = res['data']
        self.data = base64.urlsafe_b64decode(data)

    def save(
        self,
        filepath: Optional[str] = None,
        overwrite: bool = False
    ) -> None:
        """
        Saves the attachment. Downloads file data if not downloaded.
        
        Args:
            filepath: where to save the attachment. Default None, which uses 
                the filename stored.
            overwrite: whether to overwrite existing files. Default False.
        
        Raises:
            FileExistsError: if the call would overwrite an existing file and 
                overwrite is not set to True.
        
        """
        
        if filepath is None:
            filepath = self.filename

        if self.data is None:
            self.download()

        if not overwrite and os.path.exists(filepath):
            raise FileExistsError(
                f"Cannot overwrite file '{filepath}'. Use overwrite=True if "
                f"you would like to overwrite the file."
            )

        with open(filepath, 'wb') as f:
            f.write(self.data)
</file>

<file path="src/client/gmail_client_README.md">
# SimpleGmail
[![PyPI Downloads](https://img.shields.io/pypi/dm/simplegmail.svg?label=PyPI%20downloads)](
https://pypi.org/project/simplegmail/)

A simple Gmail API client in Python for applications.

---

Currently Supported Behavior:
- Sending html messages
- Sending messages with attachments
- Sending messages with your Gmail account signature
- Retrieving messages with the full suite of Gmail's search capabilities
- Retrieving messages with attachments, and downloading attachments
- Modifying message labels (includes marking as read/unread, important/not 
  important, starred/unstarred, trash/untrash, inbox/archive)

## Table of Contents

- [SimpleGmail](#simplegmail)
  - [Table of Contents](#table-of-contents)
  - [Getting Started](#getting-started)
  - [Installation](#installation)
  - [Usage](#usage)
    - [Send a simple message:](#send-a-simple-message)
    - [Send a message with attachments, cc, bcc fields:](#send-a-message-with-attachments-cc-bcc-fields)
    - [Retrieving messages:](#retrieving-messages)
    - [Marking messages:](#marking-messages)
    - [Changing message labels:](#changing-message-labels)
    - [Downloading attachments:](#downloading-attachments)
    - [Retrieving messages (advanced, with queries!):](#retrieving-messages-advanced-with-queries)
    - [Retrieving messages (more advanced, with more queries!):](#retrieving-messages-more-advanced-with-more-queries)
  - [Feedback](#feedback)

## Getting Started

The only setup required is to download an OAuth 2.0 Client ID file from Google
that will authorize your application.

This can be done at: https://console.developers.google.com/apis/credentials.
For those who haven't created a credential for Google's API, after clicking the 
link above (and logging in to the appropriate account),

1. Select/create the project that this authentication is for (if creating a new 
project make sure to configure the OAuth consent screen; you only need to set 
an Application name)

2. Click on the "Dashboard" tab, then "Enable APIs and Services". Search for 
Gmail and enable.

3. Click on the Credentials tab, then "Create Credentials" > "OAuth client ID".

4. Select what kind of application this is for, and give it a memorable name.
Fill out all necessary information for the credential (e.g., if choosing 
"Web Application" make sure to add an Authorized Redirect URI. See 
https://developers.google.com/identity/protocols/oauth2 for more infomation).

5. Back on the credentials screen, click the download icon next to the 
credential you just created to download it as a JSON object.

6. Save this file as "client_secret.json" and place it in the root directory of 
your application. (The `Gmail` class takes in an argument for the name of this 
file if you choose to name it otherwise.)

The first time you create a new instance of the `Gmail` class, a browser window 
will open, and you'll be asked to give permissions to the application. This 
will save an access token in a file named "gmail-token.json", and only needs to 
occur once.

You are now good to go!

Note about authentication method: I have opted not to use a username-password 
authentication (through imap/smtp), since using Google's authorization is both 
significantly safer and avoids clashing with Google's many security measures.

## Installation

Install using `pip` (Python3).

```bash
pip3 install simplegmail
```

## Usage

### Send a simple message:

```python
from simplegmail import Gmail

gmail = Gmail() # will open a browser window to ask you to log in and authenticate

params = {
  "to": "you@youremail.com",
  "sender": "me@myemail.com",
  "subject": "My first email",
  "msg_html": "<h1>Woah, my first email!</h1><br />This is an HTML email.",
  "msg_plain": "Hi\nThis is a plain text email.",
  "signature": True  # use my account signature
}
message = gmail.send_message(**params)  # equivalent to send_message(to="you@youremail.com", sender=...)
```

### Send a message with attachments, cc, bcc fields:

```python
from simplegmail import Gmail

gmail = Gmail()

params = {
  "to": "you@youremail.com",
  "sender": "me@myemail.com",
  "cc": ["bob@bobsemail.com"],
  "bcc": ["marie@gossip.com", "hidden@whereami.com"],
  "subject": "My first email",
  "msg_html": "<h1>Woah, my first email!</h1><br />This is an HTML email.",
  "msg_plain": "Hi\nThis is a plain text email.",
  "attachments": ["path/to/something/cool.pdf", "path/to/image.jpg", "path/to/script.py"],
  "signature": True  # use my account signature
}
message = gmail.send_message(**params)  # equivalent to send_message(to="you@youremail.com", sender=...)
```

It couldn't be easier!

### Retrieving messages:

```python
from simplegmail import Gmail

gmail = Gmail()

# Unread messages in your inbox
messages = gmail.get_unread_inbox()

# Starred messages
messages = gmail.get_starred_messages()

# ...and many more easy to use functions can be found in gmail.py!

# Print them out!
for message in messages:
    print("To: " + message.recipient)
    print("From: " + message.sender)
    print("Subject: " + message.subject)
    print("Date: " + message.date)
    print("Preview: " + message.snippet)
    
    print("Message Body: " + message.plain)  # or message.html
```

### Marking messages:

```python
from simplegmail import Gmail

gmail = Gmail()

messages = gmail.get_unread_inbox()

message_to_read = messages[0]
message_to_read.mark_as_read()

# Oops, I want to mark as unread now
message_to_read.mark_as_unread()

message_to_star = messages[1]
message_to_star.star()

message_to_trash = messages[2]
message_to_trash.trash()

# ...and many more functions can be found in message.py!
```

### Changing message labels:

```python
from simplegmail import Gmail

gmail = Gmail()

# Get the label objects for your account. Each label has a specific ID that 
# you need, not just the name!
labels = gmail.list_labels()

# To find a label by the name that you know (just an example):
finance_label = list(filter(lambda x: x.name == 'Finance', labels))[0]

messages = gmail.get_unread_inbox()

# We can add/remove a label
message = messages[0]
message.add_label(finance_label) 

# We can "move" a message from one label to another
message.modify_labels(to_add=labels[10], to_remove=finance_label)

# ...check out the code in message.py for more!
```

### Downloading attachments:

```python
from simplegmail import Gmail

gmail = Gmail()

messages = gmail.get_unread_inbox()

message = messages[0]
if message.attachments:
    for attm in message.attachments:
        print('File: ' + attm.filename)
        attm.save()  # downloads and saves each attachment under it's stored
                     # filename. You can download without saving with `attm.download()`

```

### Retrieving messages (advanced, with queries!):

```python
from simplegmail import Gmail
from simplegmail.query import construct_query

gmail = Gmail()

# Unread messages in inbox with label "Work"
labels = gmail.list_labels()
work_label = list(filter(lambda x: x.name == 'Work', labels))[0]

messages = gmail.get_unread_inbox(labels=[work_label])

# For even more control use queries:
# Messages that are: newer than 2 days old, unread, labeled "Finance" or both "Homework" and "CS"
query_params = {
    "newer_than": (2, "day"),
    "unread": True,
    "labels":[["Work"], ["Homework", "CS"]]
}

messages = gmail.get_messages(query=construct_query(query_params))

# We could have also accomplished this with
# messages = gmail.get_unread_messages(query=construct_query(newer_than=(2, "day"), labels=[["Work"], ["Homework", "CS"]]))
# There are many, many different ways of achieving the same result with search.
```

### Retrieving messages (more advanced, with more queries!):

```python
from simplegmail import Gmail
from simplegmail.query import construct_query

gmail = Gmail()

# For even more control use queries:
# Messages that are either:
#   newer than 2 days old, unread, labeled "Finance" or both "Homework" and "CS"
#     or
#   newer than 1 month old, unread, labeled "Top Secret", but not starred.

labels = gmail.list_labels()

# Construct our two queries separately
query_params_1 = {
    "newer_than": (2, "day"),
    "unread": True,
    "labels":[["Finance"], ["Homework", "CS"]]
}

query_params_2 = {
    "newer_than": (1, "month"),
    "unread": True,
    "labels": ["Top Secret"],
    "exclude_starred": True
}

# construct_query() will create both query strings and "or" them together.
messages = gmail.get_messages(query=construct_query(query_params_1, query_params_2))
```

For more on what you can do with queries, read the docstring for `construct_query()` in `query.py`.

## Feedback

If there is functionality you'd like to see added, or any bugs in this project,
please let me know by posting an issue or submitting a pull request!
</file>

<file path="src/client/gmail.py">
"""
File: gmail.py
--------------
Home to the main Gmail service object. Currently supports sending mail (with
attachments) and retrieving mail with the full suite of Gmail search options.

"""

import base64
from email.mime.audio       import MIMEAudio
from email.mime.application import MIMEApplication
from email.mime.base        import MIMEBase
from email.mime.image       import MIMEImage
from email.mime.multipart   import MIMEMultipart
from email.mime.text        import MIMEText
import html
import math
import mimetypes
import os
import re
import threading
from typing import List, Optional

from bs4 import BeautifulSoup
import dateutil.parser as parser
from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
from httplib2 import Http
from oauth2client import client, file, tools
from oauth2client.clientsecrets import InvalidClientSecretsError

from simplegmail import label
from simplegmail.attachment import Attachment
from simplegmail.label import Label
from simplegmail.message import Message


class Gmail(object):
    """
    The Gmail class which serves as the entrypoint for the Gmail service API.

    Args:
        client_secret_file: The path of the user's client secret file.
        creds_file: The path of the auth credentials file (created on first
            call).
        access_type: Whether to request a refresh token for usage without a
            user necessarily present. Either 'online' or 'offline'.

    Attributes:
        client_secret_file (str): The name of the user's client secret file.
        service (googleapiclient.discovery.Resource): The Gmail service object.

    """

    # Allow Gmail to read and write emails, and access settings like aliases.
    _SCOPES = [
        'https://www.googleapis.com/auth/gmail.modify',
        'https://www.googleapis.com/auth/gmail.settings.basic'
    ]

    # If you don't have a client secret file, follow the instructions at:
    # https://developers.google.com/gmail/api/quickstart/python
    # Make sure the client secret file is in the root directory of your app.

    def __init__(
        self,
        client_secret_file: str = 'client_secret.json',
        creds_file: str = 'gmail_token.json',
        access_type: str = 'offline',
        noauth_local_webserver: bool = False,
        _creds: Optional[client.OAuth2Credentials] = None,
    ) -> None:
        self.client_secret_file = client_secret_file
        self.creds_file = creds_file

        try:
            # The file gmail_token.json stores the user's access and refresh
            # tokens, and is created automatically when the authorization flow
            # completes for the first time.
            if _creds:
                self.creds = _creds
            else:
                store = file.Storage(self.creds_file)
                self.creds = store.get()

            if not self.creds or self.creds.invalid:
                flow = client.flow_from_clientsecrets(
                    self.client_secret_file, self._SCOPES
                )

                flow.params['access_type'] = access_type
                flow.params['prompt'] = 'consent'

                args = []
                if noauth_local_webserver:
                    args.append('--noauth_local_webserver')

                flags = tools.argparser.parse_args(args)
                self.creds = tools.run_flow(flow, store, flags)

            self._service = build(
                'gmail', 'v1', http=self.creds.authorize(Http()),
                cache_discovery=False
            )

        except InvalidClientSecretsError:
            raise FileNotFoundError(
                "Your 'client_secret.json' file is nonexistent. Make sure "
                "the file is in the root directory of your application. If "
                "you don't have a client secrets file, go to https://"
                "developers.google.com/gmail/api/quickstart/python, and "
                "follow the instructions listed there."
            )

    @property
    def service(self) -> 'googleapiclient.discovery.Resource':
        # Since the token is only used through calls to the service object,
        # this ensure that the token is always refreshed before use.
        if self.creds.access_token_expired:
            self.creds.refresh(Http())

        return self._service

    def send_message(
        self,
        sender: str,
        to: str,
        subject: str = '',
        msg_html: Optional[str] = None,
        msg_plain: Optional[str] = None,
        cc: Optional[List[str]] = None,
        bcc: Optional[List[str]] = None,
        attachments: Optional[List[str]] = None,
        signature: bool = False,
        user_id: str = 'me'
    ) -> Message:
        """
        Sends an email.

        Args:
            sender: The email address the message is being sent from.
            to: The email address the message is being sent to.
            subject: The subject line of the email.
            msg_html: The HTML message of the email.
            msg_plain: The plain text alternate message of the email. This is
                often displayed on slow or old browsers, or if the HTML message
                is not provided.
            cc: The list of email addresses to be cc'd.
            bcc: The list of email addresses to be bcc'd.
            attachments: The list of attachment file names.
            signature: Whether the account signature should be added to the
                message.
            user_id: The address of the sending account. 'me' for the
                default address associated with the account.

        Returns:
            The Message object representing the sent message.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        msg = self._create_message(
            sender, to, subject, msg_html, msg_plain, cc=cc, bcc=bcc,
            attachments=attachments, signature=signature, user_id=user_id
        )

        try:
            req = self.service.users().messages().send(userId='me', body=msg)
            res = req.execute()
            return self._build_message_from_ref(user_id, res, 'reference')

        except HttpError as error:
            # Pass along the error
            raise error

    def get_unread_inbox(
        self,
        user_id: str = 'me',
        query: str = '',
        attachments: str = 'reference'
    ) -> List[Message]:
        """
        Gets unread messages from your inbox.

        Args:
            user_id: The user's email address. By default, the authenticated user.
            query: A Gmail query to match.
            attachments: How to handle attachments ('ignore', 'reference', 'download')

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the HTTP request.
        """
        # Build query for unread inbox messages
        q = "label:INBOX label:UNREAD"
        if query:
            q = f"{q} {query}"

        try:
            # Get message list
            response = self.service.users().messages().list(
                userId=user_id,
                q=q,
                includeSpamTrash=False
            ).execute()
            
            messages = []
            if 'messages' in response:
                for msg_ref in response['messages']:
                    try:
                        full_msg = self.service.users().messages().get(
                            userId=user_id,
                            id=msg_ref['id'],
                            format='full'
                        ).execute()
                        if not full_msg.get('threadId'):
                            full_msg['threadId'] = msg_ref.get('threadId', '')
                        messages.append(self._build_message_from_response(full_msg))
                    except HttpError as e:
                        print(f"Error fetching message {msg_ref['id']}: {e}")
                        continue
            
            return messages
            
        except HttpError as error:
            raise error

    def get_starred_messages(
        self,
        user_id: str = 'me',
        labels: Optional[List[Label]] = None,
        query: str = '',
        attachments: str = 'reference',
        include_spam_trash: bool = False
    ) -> List[Message]:
        """
        Gets starred messages from your account.

        Args:
            user_id: The user's email address. By default, the authenticated
                user.
            labels: Label IDs messages must match.
            query: A Gmail query to match.
            attachments: accepted values are 'ignore' which completely
                ignores all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.
            include_spam_trash: Whether to include messages from spam or trash.

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if labels is None:
            labels = []

        labels.append(label.STARRED)
        return self.get_messages(user_id, labels, query, attachments,
                                 include_spam_trash)

    def get_important_messages(
        self,
        user_id: str = 'me',
        labels: Optional[List[Label]] = None,
        query: str = '',
        attachments: str = 'reference',
        include_spam_trash: bool = False
    ) -> List[Message]:
        """
        Gets messages marked important from your account.

        Args:
            user_id: The user's email address. By default, the authenticated
                user.
            labels: Label IDs messages must match.
            query: A Gmail query to match.
            attachments: accepted values are 'ignore' which completely
                ignores all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.
            include_spam_trash: Whether to include messages from spam or trash.

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if labels is None:
            labels = []

        labels.append(label.IMPORTANT)
        return self.get_messages(user_id, labels, query, attachments,
                                 include_spam_trash)

    def get_unread_messages(
        self,
        user_id: str = 'me',
        labels: Optional[List[Label]] = None,
        query: str = '',
        attachments: str = 'reference',
        include_spam_trash: bool = False
    ) -> List[Message]:
        """
        Gets unread messages from your account.

        Args:
            user_id: The user's email address. By default, the authenticated
                user.
            labels: Label IDs messages must match.
            query: A Gmail query to match.
            attachments: accepted values are 'ignore' which completely
                ignores all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.
            include_spam_trash: Whether to include messages from spam or trash.

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if labels is None:
            labels = []

        labels.append(label.UNREAD)
        return self.get_messages(user_id, labels, query, attachments,
                                 include_spam_trash)

    def get_drafts(
        self,
        user_id: str = 'me',
        labels: Optional[List[Label]] = None,
        query: str = '',
        attachments: str = 'reference',
        include_spam_trash: bool = False
    ) -> List[Message]:
        """
        Gets drafts saved in your account.

        Args:
            user_id: The user's email address. By default, the authenticated
                user.
            labels: Label IDs messages must match.
            query: A Gmail query to match.
            attachments: accepted values are 'ignore' which completely
                ignores all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.
            include_spam_trash: Whether to include messages from spam or trash.

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if labels is None:
            labels = []

        labels.append(label.DRAFT)
        return self.get_messages(user_id, labels, query, attachments,
                                 include_spam_trash)

    def get_sent_messages(
        self,
        user_id: str = 'me',
        labels: Optional[List[Label]] = None,
        query: str = '',
        attachments: str = 'reference',
        include_spam_trash: bool = False
    ) -> List[Message]:
        """
        Gets sent messages from your account.

         Args:
            user_id: The user's email address. By default, the authenticated
                user.
            labels: Label IDs messages must match.
            query: A Gmail query to match.
            attachments: accepted values are 'ignore' which completely
                ignores all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.
            include_spam_trash: Whether to include messages from spam or trash.

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if labels is None:
            labels = []

        labels.append(label.SENT)
        return self.get_messages(user_id, labels, query, attachments,
                                 include_spam_trash)

    def get_trash_messages(
        self,
        user_id: str = 'me',
        labels: Optional[List[Label]] = None,
        query: str = '',
        attachments: str = 'reference'
    ) -> List[Message]:

        """
        Gets messages in your trash from your account.

        Args:
            user_id: The user's email address. By default, the authenticated
                user.
            labels: Label IDs messages must match.
            query: A Gmail query to match.
            attachments: accepted values are 'ignore' which completely
                ignores all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if labels is None:
            labels = []

        labels.append(label.TRASH)
        return self.get_messages(user_id, labels, query, attachments, True)

    def get_spam_messages(
        self,
        user_id: str = 'me',
        labels: Optional[List[Label]] = None,
        query: str = '',
        attachments: str = 'reference'
    ) -> List[Message]:
        """
        Gets messages marked as spam from your account.

        Args:
            user_id: The user's email address. By default, the authenticated
                user.
            labels: Label IDs messages must match.
            query: A Gmail query to match.
            attachments: accepted values are 'ignore' which completely
                ignores all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """


        if labels is None:
            labels = []

        labels.append(label.SPAM)
        return self.get_messages(user_id, labels, query, attachments, True)

    def modify_labels(self, message_id: str, add_labels: List[str], remove_labels: List[str], user_id: str = 'me') -> None:
        """
        Modify the labels on a message.
        
        Args:
            message_id: The ID of the message to modify
            add_labels: List of label IDs to add
            remove_labels: List of label IDs to remove
            user_id: The user's email address (default: 'me')
        """
        try:
            self.service.users().messages().modify(
                userId=user_id,
                id=message_id,
                body={
                    'addLabelIds': add_labels,
                    'removeLabelIds': remove_labels
                }
            ).execute()
        except HttpError as error:
            raise error

    def create_label(self, name: str, user_id: str = 'me') -> Label:
        """
        Create a new label.
        
        Args:
            name: Name of the label to create
            user_id: The user's email address (default: 'me')
            
        Returns:
            The created Label object
        """
        try:
            result = self.service.users().labels().create(
                userId=user_id,
                body={
                    'name': name,
                    'labelListVisibility': 'labelShow',
                    'messageListVisibility': 'show'
                }
            ).execute()
            return Label(name=result['name'], id=result['id'])
        except HttpError as error:
            raise error

    def get_messages(
        self,
        user_id: str = 'me',
        labels: Optional[List[Label]] = None,
        query: str = '',
        msg_ids: Optional[List[str]] = None,
        attachments: str = 'reference',
        include_spam_trash: bool = False
    ) -> List[Message]:
        """
        Gets messages from the user's mailbox.
        
        Args:
            user_id: The user's email address
            labels: Label IDs messages must match
            query: A Gmail query to match
            msg_ids: Specific message IDs to retrieve
            attachments: How to handle attachments ('ignore', 'reference', 'download')
            include_spam_trash: Whether to include spam/trash messages
            
        Returns:
            List of Message objects
        """
        try:
            if msg_ids:
                # If specific message IDs are provided, retrieve those messages
                messages = []
                for msg_id in msg_ids:
                    msg = self.service.users().messages().get(
                        userId=user_id,
                        id=msg_id,
                        format='full'
                    ).execute()
                    messages.append(self._build_message_from_response(msg))
                return messages
            
            # Build query string from labels and query
            q = query
            if labels:
                label_queries = [f"label:{label.id if isinstance(label, Label) else label}" 
                               for label in labels]
                if q:
                    q = f"{q} AND ({' '.join(label_queries)})"
                else:
                    q = ' '.join(label_queries)
            
            # Get message list
            try:
                response = self.service.users().messages().list(
                    userId=user_id,
                    q=q,
                    includeSpamTrash=include_spam_trash
                ).execute()
                
                messages = []
                if 'messages' in response:
                    for msg_ref in response['messages']:
                        try:
                            full_msg = self.service.users().messages().get(
                                userId=user_id,
                                id=msg_ref['id'],
                                format='full'
                            ).execute()
                            if not full_msg.get('threadId'):
                                full_msg['threadId'] = msg_ref.get('threadId', '')
                            messages.append(self._build_message_from_response(full_msg))
                        except HttpError as e:
                            # Log error but continue with other messages
                            print(f"Error fetching message {msg_ref['id']}: {e}")
                            continue
                
                return messages
            except HttpError as error:
                print(f"Error listing messages: {error}")
                return []
            
        except HttpError as error:
            raise error
        """
        Gets messages from your account.

        Args:
            user_id: the user's email address. Default 'me', the authenticated
                user.
            labels: label IDs messages must match.
            query: a Gmail query to match.
            attachments: accepted values are 'ignore' which completely
                ignores all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.
            include_spam_trash: whether to include messages from spam or trash.

        Returns:
            A list of message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if labels is None:
            labels = []

        labels_ids = [
            lbl.id if isinstance(lbl, Label) else lbl for lbl in labels
        ]

        try:
            response = self.service.users().messages().list(
                userId=user_id,
                q=query,
                labelIds=labels_ids,
                includeSpamTrash=include_spam_trash
            ).execute()

            message_refs = []
            if 'messages' in response:  # ensure request was successful
                message_refs.extend(response['messages'])

            while 'nextPageToken' in response:
                page_token = response['nextPageToken']
                response = self.service.users().messages().list(
                    userId=user_id,
                    q=query,
                    labelIds=labels_ids,
                    includeSpamTrash=include_spam_trash,
                    pageToken=page_token
                ).execute()

                message_refs.extend(response['messages'])

            return self._get_messages_from_refs(user_id, message_refs,
                                                attachments)

        except HttpError as error:
            # Pass along the error
            raise error

    def list_labels(self, user_id: str = 'me') -> List[Label]:
        """
        Retrieves all labels for the specified user.

        These Label objects are to be used with other functions like
        modify_labels().

        Args:
            user_id: The user's email address. By default, the authenticated
                user.

        Returns:
            The list of Label objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        try:
            res = self.service.users().labels().list(
                userId=user_id
            ).execute()

        except HttpError as error:
            # Pass along the error
            raise error

        else:
            labels = [Label(name=x['name'], id=x['id']) for x in res['labels']]
            return labels

    def delete_label(self, label: Label, user_id: str = 'me') -> None:
        """
        Deletes a label.

        Args:
            label: The label to delete.
            user_id: The user's email address. By default, the authenticated
                user.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        try:
            self.service.users().labels().delete(
                userId=user_id,
                id=label.id
            ).execute()

        except HttpError as error:
            # Pass along the error
            raise error

    def _build_message_from_response(self, response: dict) -> Message:
        """Build a Message object from a Gmail API response."""
        # Extract headers
        headers = {}
        for header in response['payload']['headers']:
            headers[header['name']] = header['value']
        
        # Get message content
        plain = None
        html = None
        if 'parts' in response['payload']:
            for part in response['payload']['parts']:
                if part['mimeType'] == 'text/plain':
                    if 'data' in part['body']:
                        plain = base64.urlsafe_b64decode(
                            part['body']['data'].encode('utf-8')
                        ).decode('utf-8')
                elif part['mimeType'] == 'text/html':
                    if 'data' in part['body']:
                        html = base64.urlsafe_b64decode(
                            part['body']['data'].encode('utf-8')
                        ).decode('utf-8')
        
        # Create message object
        return Message(
            service=self.service,
            creds=self.creds,
            user_id='me',
            msg_id=response['id'],
            thread_id=response['threadId'],
            recipient=headers.get('To', ''),
            sender=headers.get('From', ''),
            subject=headers.get('Subject', ''),
            date=headers.get('Date', ''),
            snippet=response.get('snippet', ''),
            plain=plain,
            html=html,
            label_ids=response.get('labelIds', [])
        )

    def _get_messages_from_refs(
        self,
        user_id: str,
        message_refs: List[dict],
        attachments: str = 'reference',
        parallel: bool = True
    ) -> List[Message]:
        """
        Retrieves the actual messages from a list of references.

        Args:
            user_id: The account the messages belong to.
            message_refs: A list of message references with keys id, threadId.
            attachments: Accepted values are 'ignore' which completely ignores
                all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download'
                which downloads the attachment data to store locally. Default
                'reference'.
            parallel: Whether to retrieve messages in parallel. Default true.
                Currently parallelization is always on, since there is no
                reason to do otherwise.


        Returns:
            A list of Message objects.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if not message_refs:
            return []

        if not parallel:
            return [self._build_message_from_ref(user_id, ref, attachments)
                    for ref in message_refs]

        max_num_threads = 12  # empirically chosen, prevents throttling
        target_msgs_per_thread = 10  # empirically chosen
        num_threads = min(
            math.ceil(len(message_refs) / target_msgs_per_thread),
            max_num_threads
        )
        batch_size = math.ceil(len(message_refs) / num_threads)
        message_lists = [None] * num_threads

        def thread_download_batch(thread_num):
            gmail = Gmail(_creds=self.creds)

            start = thread_num * batch_size
            end = min(len(message_refs), (thread_num + 1) * batch_size)
            message_lists[thread_num] = [
                gmail._build_message_from_ref(
                    user_id, message_refs[i], attachments
                )
                for i in range(start, end)
            ]

            gmail.service.close()

        threads = [
            threading.Thread(target=thread_download_batch, args=(i,))
            for i in range(num_threads)
        ]

        for t in threads:
            t.start()

        for t in threads:
            t.join()

        return sum(message_lists, [])

    def _build_message_from_ref(
        self,
        user_id: str,
        message_ref: dict,
        attachments: str = 'reference'
    ) -> Message:
        """
        Creates a Message object from a reference.

        Args:
            user_id: The username of the account the message belongs to.
            message_ref: The message reference object returned from the Gmail
                API.
            attachments: Accepted values are 'ignore' which completely ignores
                all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.

        Returns:
            The Message object.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        try:
            # Get message JSON
            message = self.service.users().messages().get(
                userId=user_id, id=message_ref['id']
            ).execute()

        except HttpError as error:
            # Pass along the error
            raise error

        else:
            msg_id = message['id']
            thread_id = message['threadId']
            label_ids = []
            if 'labelIds' in message:
                user_labels = {x.id: x for x in self.list_labels(user_id=user_id)}
                label_ids = [user_labels[x] for x in message['labelIds']]
            snippet = html.unescape(message['snippet'])

            payload = message['payload']
            headers = payload['headers']

            # Get header fields (date, from, to, subject)
            date = ''
            sender = ''
            recipient = ''
            subject = ''
            msg_hdrs = {}
            cc = []
            bcc = []
            for hdr in headers:
                if hdr['name'].lower() == 'date':
                    try:
                        date = str(parser.parse(hdr['value']).astimezone())
                    except Exception:
                        date = hdr['value']
                elif hdr['name'].lower() == 'from':
                    sender = hdr['value']
                elif hdr['name'].lower() == 'to':
                    recipient = hdr['value']
                elif hdr['name'].lower() == 'subject':
                    subject = hdr['value']
                elif hdr['name'].lower() == 'cc':
                    cc = hdr['value'].split(', ')
                elif hdr['name'].lower() == 'bcc':
                    bcc = hdr['value'].split(', ')

                msg_hdrs[hdr['name']] = hdr['value']

            parts = self._evaluate_message_payload(
                payload, user_id, message_ref['id'], attachments
            )

            plain_msg = None
            html_msg = None
            attms = []
            for part in parts:
                if part['part_type'] == 'plain':
                    if plain_msg is None:
                        plain_msg = part['body']
                    else:
                        plain_msg += '\n' + part['body']
                elif part['part_type'] == 'html':
                    if html_msg is None:
                        html_msg = part['body']
                    else:
                        html_msg += '<br/>' + part['body']
                elif part['part_type'] == 'attachment':
                    attm = Attachment(self.service, user_id, msg_id,
                                      part['attachment_id'], part['filename'],
                                      part['filetype'], part['data'])
                    attms.append(attm)

            return Message(
                self.service,
                self.creds,
                user_id,
                msg_id,
                thread_id,
                recipient,
                sender,
                subject,
                date,
                snippet,
                plain_msg,
                html_msg,
                label_ids,
                attms,
                msg_hdrs,
                cc,
                bcc
            )

    def _evaluate_message_payload(
        self,
        payload: dict,
        user_id: str,
        msg_id: str,
        attachments: str = 'reference'
    ) -> List[dict]:
        """
        Recursively evaluates a message payload.

        Args:
            payload: The message payload object (response from Gmail API).
            user_id: The current account address (default 'me').
            msg_id: The id of the message.
            attachments: Accepted values are 'ignore' which completely ignores
                all attachments, 'reference' which includes attachment
                information but does not download the data, and 'download' which
                downloads the attachment data to store locally. Default
                'reference'.

        Returns:
            A list of message parts.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if 'attachmentId' in payload['body']:  # if it's an attachment
            if attachments == 'ignore':
                return []

            att_id = payload['body']['attachmentId']
            filename = payload['filename']
            if not filename:
                filename = 'unknown'

            obj = {
                'part_type': 'attachment',
                'filetype': payload['mimeType'],
                'filename': filename,
                'attachment_id': att_id,
                'data': None
            }

            if attachments == 'reference':
                return [obj]

            else:  # attachments == 'download'
                if 'data' in payload['body']:
                    data = payload['body']['data']
                else:
                    res = self.service.users().messages().attachments().get(
                        userId=user_id, messageId=msg_id, id=att_id
                    ).execute()
                    data = res['data']

                file_data = base64.urlsafe_b64decode(data)
                obj['data'] = file_data
                return [obj]

        elif payload['mimeType'] == 'text/html':
            data = payload['body']['data']
            data = base64.urlsafe_b64decode(data)
            body = BeautifulSoup(data, 'lxml', from_encoding='utf-8').body
            return [{ 'part_type': 'html', 'body': str(body) }]

        elif payload['mimeType'] == 'text/plain':
            data = payload['body']['data']
            data = base64.urlsafe_b64decode(data)
            body = data.decode('UTF-8')
            return [{ 'part_type': 'plain', 'body': body }]

        elif payload['mimeType'].startswith('multipart'):
            ret = []
            if 'parts' in payload:
                for part in payload['parts']:
                    ret.extend(self._evaluate_message_payload(part, user_id, msg_id,
                                                              attachments))
            return ret

        return []

    def _create_message(
        self,
        sender: str,
        to: str,
        subject: str = '',
        msg_html: str = None,
        msg_plain: str = None,
        cc: List[str] = None,
        bcc: List[str] = None,
        attachments: List[str] = None,
        signature: bool = False,
        user_id: str = 'me'
    ) -> dict:
        """
        Creates the raw email message to be sent.

        Args:
            sender: The email address the message is being sent from.
            to: The email address the message is being sent to.
            subject: The subject line of the email.
            msg_html: The HTML message of the email.
            msg_plain: The plain text alternate message of the email (for slow
                or old browsers).
            cc: The list of email addresses to be Cc'd.
            bcc: The list of email addresses to be Bcc'd
            attachments: A list of attachment file paths.
            signature: Whether the account signature should be added to the
                message. Will add the signature to your HTML message only, or a
                create a HTML message if none exists.

        Returns:
            The message dict.

        """

        msg = MIMEMultipart('mixed' if attachments else 'alternative')
        msg['To'] = to
        msg['From'] = sender
        msg['Subject'] = subject

        if cc:
            msg['Cc'] = ', '.join(cc)

        if bcc:
            msg['Bcc'] = ', '.join(bcc)

        if signature:
            m = re.match(r'.+\s<(?P<addr>.+@.+\..+)>', sender)
            address = m.group('addr') if m else sender
            account_sig = self._get_alias_info(address, user_id)['signature']

            if msg_html is None:
                msg_html = ''

            msg_html += "<br /><br />" + account_sig

        attach_plain = MIMEMultipart('alternative') if attachments else msg
        attach_html = MIMEMultipart('related') if attachments else msg

        if msg_plain:
            attach_plain.attach(MIMEText(msg_plain, 'plain'))

        if msg_html:
            attach_html.attach(MIMEText(msg_html, 'html'))

        if attachments:
            attach_plain.attach(attach_html)
            msg.attach(attach_plain)

            self._ready_message_with_attachments(msg, attachments)

        return {
            'raw': base64.urlsafe_b64encode(msg.as_string().encode()).decode()
        }

    def _ready_message_with_attachments(
        self,
        msg: MIMEMultipart,
        attachments: List[str]
    ) -> None:
        """
        Converts attachment filepaths to MIME objects and adds them to msg.

        Args:
            msg: The message to add attachments to.
            attachments: A list of attachment file paths.

        """

        for filepath in attachments:
            content_type, encoding = mimetypes.guess_type(filepath)

            if content_type is None or encoding is not None:
                content_type = 'application/octet-stream'

            main_type, sub_type = content_type.split('/', 1)
            with open(filepath, 'rb') as file:
                raw_data = file.read()

                attm: MIMEBase
                if main_type == 'text':
                    attm = MIMEText(raw_data.decode('UTF-8'), _subtype=sub_type)
                elif main_type == 'image':
                    attm = MIMEImage(raw_data, _subtype=sub_type)
                elif main_type == 'audio':
                    attm = MIMEAudio(raw_data, _subtype=sub_type)
                elif main_type == 'application':
                    attm = MIMEApplication(raw_data, _subtype=sub_type)
                else:
                    attm = MIMEBase(main_type, sub_type)
                    attm.set_payload(raw_data)

            fname = os.path.basename(filepath)
            attm.add_header('Content-Disposition', 'attachment', filename=fname)
            msg.attach(attm)

    def _get_alias_info(
        self,
        send_as_email: str,
        user_id: str = 'me'
    ) -> dict:
        """
        Returns the alias info of an email address on the authenticated
        account.

        Response data is of the following form:
        {
            "sendAsEmail": string,
            "displayName": string,
            "replyToAddress": string,
            "signature": string,
            "isPrimary": boolean,
            "isDefault": boolean,
            "treatAsAlias": boolean,
            "smtpMsa": {
                "host": string,
                "port": integer,
                "username": string,
                "password": string,
                "securityMode": string
            },
            "verificationStatus": string
        }

        Args:
            send_as_email: The alias account information is requested for
                (could be the primary account).
            user_id: The user ID of the authenticated user the account the
                alias is for (default "me").

        Returns:
            The dict of alias info associated with the account.

        """

        req =  self.service.users().settings().sendAs().get(
                   sendAsEmail=send_as_email, userId=user_id)

        res = req.execute()
        return res
</file>

<file path="src/client/label.py">
"""
File: label.py
--------------
Gmail reserved system labels and the Label class.

"""


class Label:
    """
    A Gmail label object.

    This class should not typically be constructed directly but rather returned 
    from Gmail.list_labels().
    
    Args:
        name: The name of the Label.
        id: The ID of the label.

    Attributes:
        name (str): The name of the Label.
        id (str): The ID of the label.

    """

    def __init__(self, name: str, id: str) -> None:
        self.name = name
        self.id = id

    def __repr__(self) -> str:
        return f'Label(name={self.name!r}, id={self.id!r})'

    def __str__(self) -> str:
        return self.name

    def __hash__(self) -> int:
        return hash(self.id)

    def __eq__(self, other) -> bool:
        if isinstance(other, str):
            # Can be compared to a string of the label ID
            return self.id == other
        elif isinstance(other, Label):
            return self.id == other.id
        else:
            return False


INBOX      = Label('INBOX', 'INBOX')
SPAM       = Label('SPAM', 'SPAM')
TRASH      = Label('TRASH', 'TRASH')
UNREAD     = Label('UNREAD', 'UNREAD')
STARRED    = Label('STARRED', 'STARRED')
SENT       = Label('SENT', 'SENT')
IMPORTANT  = Label('IMPORTANT', 'IMPORTANT')
DRAFT      = Label('DRAFT', 'DRAFT')
PERSONAL   = Label('CATEGORY_PERSONAL', 'CATEGORY_PERSONAL')
SOCIAL     = Label('CATEGORY_SOCIAL', 'CATEGORY_SOCIAL')
PROMOTIONS = Label('CATEGORY_PROMOTIONS', 'CATEGORY_PROMOTIONS')
UPDATES    = Label('CATEGORY_UPDATES', 'CATEGORY_UPDATES')
FORUMS     = Label('CATEGORY_FORUMS', 'CATEGORY_FORUMS')
</file>

<file path="src/client/message.py">
"""
File: message.py
----------------
This module contains the implementation of the Message object.

"""

from typing import List, Optional, Union
import email.utils

from httplib2 import Http
from googleapiclient.errors import HttpError

from simplegmail import label
from simplegmail.attachment import Attachment
from simplegmail.label import Label


class Message(object):
    """
    The Message class for emails in your Gmail mailbox. This class should not
    be manually constructed. Contains all information about the associated
    message, and can be used to modify the message's labels (e.g., marking as
    read/unread, archiving, moving to trash, starring, etc.).

    Args:
        service: the Gmail service object.
        user_id: the username of the account the message belongs to.
        msg_id: the message id.
        thread_id: the thread id.
        recipient: who the message was addressed to.
        sender: who the message was sent from.
        subject: the subject line of the message.
        date: the date the message was sent.
        snippet: the snippet line for the message.
        plain: the plaintext contents of the message. Default None.
        html: the HTML contents of the message. Default None.
        label_ids: the ids of labels associated with this message. Default [].
        attachments: a list of attachments for the message. Default [].
        headers: a dict of header values. Default {}
        cc: who the message was cc'd on the message.
        bcc: who the message was bcc'd on the message.

    Attributes:
        _service (googleapiclient.discovery.Resource): the Gmail service object.
        user_id (str): the username of the account the message belongs to.
        id (str): the message id.
        recipient (str): who the message was addressed to.
        sender (str): who the message was sent from.
        subject (str): the subject line of the message.
        date (str): the date the message was sent.
        snippet (str): the snippet line for the message.
        plain (str): the plaintext contents of the message.
        html (str): the HTML contents of the message.
        label_ids (List[str]): the ids of labels associated with this message.
        attachments (List[Attachment]): a list of attachments for the message.
        headers (dict): a dict of header values.
        cc (List[str]): who the message was cc'd on the message.
        bcc (List[str]): who the message was bcc'd on the message.

    """

    def __init__(
        self,
        service: 'googleapiclient.discovery.Resource',
        creds: 'oauth2client.client.OAuth2Credentials',
        user_id: str,
        msg_id: str,
        thread_id: str,
        recipient: str,
        sender: str,
        subject: str,
        date: str,
        snippet,
        plain: Optional[str] = None,
        html: Optional[str] = None,
        label_ids: Optional[List[str]] = None,
        attachments: Optional[List[Attachment]] = None,
        headers: Optional[dict] = None,
        cc: Optional[List[str]] = None,
        bcc: Optional[List[str]] = None
    ) -> None:
        self._service = service
        self.creds = creds
        self.user_id = user_id
        self.id = msg_id
        self.thread_id = thread_id
        self.recipient = recipient
        self.sender = sender
        self.subject = subject
        self.date = email.utils.parsedate_to_datetime(date)
        self.snippet = snippet
        self.plain = plain
        self.html = html
        self.label_ids = label_ids or []
        self.attachments = attachments or []
        self.headers = headers or {}
        self.cc = cc or []
        self.bcc = bcc or []

    @property
    def service(self) -> 'googleapiclient.discovery.Resource':
        if self.creds.access_token_expired:
            self.creds.refresh(Http())
        return self._service

    @property
    def labels(self) -> List[str]:
        """Get message labels."""
        return self.label_ids

    @property
    def is_unread(self) -> bool:
        """Check if message is unread."""
        return label.UNREAD in self.label_ids

    @property
    def in_inbox(self) -> bool:
        """Check if message is in inbox."""
        return label.INBOX in self.label_ids

    def __repr__(self) -> str:
        """Represents the object by its sender, recipient, and id."""

        return (
            f'Message(to: {self.recipient}, from: {self.sender}, id: {self.id})'
        )

    def mark_as_read(self) -> None:
        """
        Marks this message as read (by removing the UNREAD label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.remove_label(label.UNREAD)

    def mark_as_unread(self) -> None:
        """
        Marks this message as unread (by adding the UNREAD label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.add_label(label.UNREAD)

    def mark_as_spam(self) -> None:
        """
        Marks this message as spam (by adding the SPAM label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.add_label(label.SPAM)

    def mark_as_not_spam(self) -> None:
        """
        Marks this message as not spam (by removing the SPAM label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.remove_label(label.SPAM)

    def mark_as_important(self) -> None:
        """
        Marks this message as important (by adding the IMPORTANT label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.add_label(label.IMPORTANT)

    def mark_as_not_important(self) -> None:
        """
        Marks this message as not important (by removing the IMPORTANT label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.remove_label(label.IMPORTANT)

    def star(self) -> None:
        """
        Stars this message (by adding the STARRED label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.add_label(label.STARRED)

    def unstar(self) -> None:
        """
        Unstars this message (by removing the STARRED label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.remove_label(label.STARRED)

    def move_to_inbox(self) -> None:
        """
        Moves an archived message to your inbox (by adding the INBOX label).

        """

        self.add_label(label.INBOX)

    def archive(self) -> None:
        """
        Archives the message (removes from inbox by removing the INBOX label).

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.remove_label(label.INBOX)

    def trash(self) -> None:
        """
        Moves this message to the trash.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        try:
            res = self._service.users().messages().trash(
                userId=self.user_id, id=self.id,
            ).execute()

        except HttpError as error:
            # Pass error along
            raise error

        else:
            assert label.TRASH in res['labelIds'], \
                f'An error occurred in a call to `trash`.'

            self.label_ids = res['labelIds']

    def untrash(self) -> None:
        """
        Removes this message from the trash.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        try:
            res = self._service.users().messages().untrash(
                userId=self.user_id, id=self.id,
            ).execute()

        except HttpError as error:
            # Pass error along
            raise error

        else:
            assert label.TRASH not in res['labelIds'], \
                f'An error occurred in a call to `untrash`.'

            self.label_ids = res['labelIds']

    def move_from_inbox(self, to: Union[Label, str]) -> None:
        """
        Moves a message from your inbox to another label "folder".

        Args:
            to: The label to move to.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.modify_labels(to, label.INBOX)

    def add_label(self, to_add: Union[Label, str]) -> None:
        """
        Adds the given label to the message.

        Args:
            to_add: The label to add.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.add_labels([to_add])

    def add_labels(self, to_add: Union[List[Label], List[str]]) -> None:
        """
        Adds the given labels to the message.

        Args:
            to_add: The list of labels to add.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.modify_labels(to_add, [])

    def remove_label(self, to_remove: Union[Label, str]) -> None:
        """
        Removes the given label from the message.

        Args:
            to_remove: The label to remove.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.remove_labels([to_remove])

    def remove_labels(self, to_remove: Union[List[Label], List[str]]) -> None:
        """
        Removes the given labels from the message.

        Args:
            to_remove: The list of labels to remove.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        self.modify_labels([], to_remove)

    def modify_labels(
        self,
        to_add: Union[Label, str, List[Label], List[str]],
        to_remove: Union[Label, str, List[Label], List[str]]
    ) -> None:
        """
        Adds or removes the specified label.

        Args:
            to_add: The label or list of labels to add.
            to_remove: The label or list of labels to remove.

        Raises:
            googleapiclient.errors.HttpError: There was an error executing the
                HTTP request.

        """

        if isinstance(to_add, (Label, str)):
            to_add = [to_add]

        if isinstance(to_remove, (Label, str)):
            to_remove = [to_remove]

        try:
            res = self._service.users().messages().modify(
                userId=self.user_id, id=self.id,
                body=self._create_update_labels(to_add, to_remove)
            ).execute()

        except HttpError as error:
            # Pass along error
            raise error

        else:
            assert all([lbl in res['labelIds'] for lbl in to_add]) \
                and all([lbl not in res['labelIds'] for lbl in to_remove]), \
                'An error occurred while modifying message label.'

            self.label_ids = res['labelIds']

    def _create_update_labels(
        self,
        to_add: Union[List[Label], List[str]] = None,
        to_remove: Union[List[Label], List[str]] = None
    ) -> dict:
        """
        Creates an object for updating message label.

        Args:
            to_add: A list of labels to add.
            to_remove: A list of labels to remove.

        Returns:
            The modify labels object to pass to the Gmail API.

        """

        if to_add is None:
            to_add = []

        if to_remove is None:
            to_remove = []

        return {
            'addLabelIds': [
                lbl.id if isinstance(lbl, Label) else lbl for lbl in to_add
            ],
            'removeLabelIds': [
                lbl.id if isinstance(lbl, Label) else lbl for lbl in to_remove
            ]
        }
</file>

<file path="src/client/query.py">
"""
File: query.py
--------------
This module contains functions for constructing Gmail search queries.

"""

from typing import List, Union


def construct_query(*query_dicts, **query_terms) -> str:
    """
    Constructs a query from either:

    (1) a list of dictionaries representing queries to "or" (only one of the
        queries needs to match). Each of these dictionaries should be made up
        of keywords as specified below.

        E.g.:
        construct_query(
          {'sender': 'someone@email.com', 'subject': 'Meeting'},
          {'sender': ['boss@inc.com', 'hr@inc.com'], 'newer_than': (5, "day")}
        )

        Will return a query which matches all messages that either match the
        all the fields in the first dictionary or match all the fields in the
        second dictionary.

    -- OR --

    (2) Keyword arguments specifying individual query terms (each keyword will
        be and'd).


    To negate any term, set it as the value of "exclude_<keyword>" instead of
    "<keyword>" (for example, since `labels=['finance', 'bills']` will match
    messages with both the 'finance' and 'bills' labels,
    `exclude_labels=['finance', 'bills']` will exclude messages that have both
    labels. To exclude either you must specify
    `exclude_labels=[['finance'], ['bills']]`, which negates
    '(finance OR bills)'.

    For all keywords whose values are not booleans, you can indicate you'd
    like to "and" multiple values by placing them in a tuple (), or "or"
    multiple values by placing them in a list [].

    Keyword Arguments:
        sender (str): Who the message is from.
            E.g.: sender='someone@email.com'
                  sender=['john@doe.com', 'jane@doe.com'] # OR

        recipient (str): Who the message is to.
            E.g.: recipient='someone@email.com'

        subject (str): The subject of the message. E.g.: subject='Meeting'

        labels (List[str]): Labels applied to the message (all must match).
            E.g.: labels=['Work', 'HR'] # Work AND HR
                  labels=[['Work', 'HR'], ['Home']] # (Work AND HR) OR Home

        attachment (bool): The message has an attachment. E.g.: attachment=True

        spec_attachment (str): The message has an attachment with a
            specific name or file type.
            E.g.: spec_attachment='pdf',
                  spec_attachment='homework.docx'

        exact_phrase (str): The message contains an exact phrase.
             E.g.: exact_phrase='I need help'
                   exact_phrase=('help me', 'homework') # AND

        cc (str): Recipient in the cc field. E.g.: cc='john@email.com'

        bcc (str): Recipient in the bcc field. E.g.: bcc='jane@email.com'

        before (str): The message was sent before a date.
            E.g.: before='2004/04/27'

        after (str): The message was sent after a date.
            E.g.: after='2004/04/27'

        older_than (Tuple[int, str]): The message was sent before a given
            time period.
            E.g.: older_than=(3, "day")
                  older_than=(1, "month")
                  older_than=(2, "year")

        newer_than (Tuple[int, str]): The message was sent after a given
            time period.
            E.g.: newer_than=(3, "day")
                  newer_than=(1, "month")
                  newer_than=(2, "year")

        near_words (Tuple[str, str, int]): The message contains two words near
            each other. (The third item is the max number of words between the
            two words). E.g.: near_words=('CS', 'hw', 5)

        starred (bool): The message was starred. E.g.: starred=True

        snoozed (bool): The message was snoozed. E.g.: snoozed=True

        unread (bool): The message is unread. E.g.: unread=True

        read (bool): The message has been read. E.g.: read=True

        important (bool): The message was marked as important.
            E.g.: important=True

        drive (bool): The message contains a Google Drive attachment.
            E.g.: drive=True

        docs (bool): The message contains a Google Docs attachment.
            E.g.: docs=True

        sheets (bool): The message contains a Google Sheets attachment.
            E.g.: sheets=True

        slides (bool): The message contains a Google Slides attachment.
            E.g.: slides=True

        list (str): The message is from a mailing list.
            E.g.: list=info@example.com

        in (str): The message is in a folder.
            E.g.: in=anywhere
                  in=chats
                  in=trash

        delivered_to (str): The message was delivered to a given address.
            E.g.: deliveredto=username@gmail.com

        category (str): The message is in a given category.
            E.g.: category=primary

        larger (str): The message is larger than a certain size in bytes.
            E.g.: larger=10M

        smaller (str): The message is smaller than a certain size in bytes
            E.g.: smaller=10M

        id (str): The message has a given message-id header.
            E.g.: id=339376385@example.com

        has (str): The message has a given attribute.
            E.g.: has=userlabels
                  has=nouserlabels

            Note: Labels are only added to a message, and not an entire
            conversation.

    Returns:
        The query string.

    """

    if query_dicts:
        return _or([construct_query(**query) for query in query_dicts])

    terms = []
    for key, val in query_terms.items():
        exclude = False
        if key.startswith('exclude'):
            exclude = True
            key = key[len('exclude_'):]

        query_fn = globals()[f"_{key}"]
        conjunction = _and if isinstance(val, tuple) else _or

        if key in ['newer_than', 'older_than', 'near_words']:
            if isinstance(val[0], (tuple, list)):
                term = conjunction([query_fn(*v) for v in val])
            else:
                term = query_fn(*val)

        elif key == 'labels':
            if isinstance(val[0], (tuple, list)):
                term = conjunction([query_fn(labels) for labels in val])
            else:
                term = query_fn(val)

        elif isinstance(val, (tuple, list)):
            term = conjunction([query_fn(v) for v in val])

        else:
            term = query_fn(val) if not isinstance(val, bool) else query_fn()

        if exclude:
            term = _exclude(term)

        terms.append(term)

    return _and(terms)


def _and(queries: List[str]) -> str:
    """
    Returns a query term matching the "and" of all query terms.

    Args:
        queries: A list of query terms to and.

    Returns:
        The query string.

    """

    if len(queries) == 1:
        return queries[0]

    return f'({" ".join(queries)})'


def _or(queries: List[str]) -> str:
    """
    Returns a query term matching the "or" of all query terms.

    Args:
        queries: A list of query terms to or.

    Returns:
        The query string.

    """

    if len(queries) == 1:
        return queries[0]

    return '{' + ' '.join(queries) + '}'


def _exclude(term: str) -> str:
    """
    Returns a query term excluding messages that match the given query term.

    Args:
        term: The query term to be excluded.

    Returns:
        The query string.

    """

    return f'-{term}'


def _sender(sender: str) -> str:
    """
    Returns a query term matching "from".

    Args:
        sender: The sender of the message.

    Returns:
        The query string.

    """

    return f'from:{sender}'


def _recipient(recipient: str) -> str:
    """
    Returns a query term matching "to".

    Args:
        recipient: The recipient of the message.

    Returns:
        The query string.

    """

    return f'to:{recipient}'


def _subject(subject: str) -> str:
    """
    Returns a query term matching "subject".

    Args:
        subject: The subject of the message.

    Returns:
        The query string.

    """

    return f'subject:{subject}'


def _labels(labels: Union[List[str], str]) -> str:
    """
    Returns a query term matching a multiple labels.

    Works with a single label (str) passed in, instead of the expected list.

    Args:
        labels: A list of labels the message must have applied.

    Returns:
        The query string.

    """

    if isinstance(labels, str):  # called the wrong function
        return _label(labels)

    return _and([_label(label) for label in labels])


def _label(label: str) -> str:
    """
    Returns a query term matching a label.

    Args:
        label: The label the message must have applied.

    Returns:
        The query string.

    """

    return f'label:{label}'


def _spec_attachment(name_or_type: str) -> str:
    """
    Returns a query term matching messages that have attachments with a
    certain name or file type.

    Args:
        name_or_type: The specific name of file type to match.

    Returns:
        The query string.

    """

    return f'filename:{name_or_type}'


def _exact_phrase(phrase: str) -> str:
    """
    Returns a query term matching messages that have an exact phrase.

    Args:
        phrase: The exact phrase to match.

    Returns:
        The query string.

    """

    return f'"{phrase}"'


def _starred() -> str:
    """Returns a query term matching messages that are starred."""

    return 'is:starred'


def _snoozed() -> str:
    """Returns a query term matching messages that are snoozed."""

    return 'is:snoozed'


def _unread() -> str:
    """Returns a query term matching messages that are unread."""

    return 'is:unread'


def _read() -> str:
    """Returns a query term matching messages that are read."""

    return 'is:read'


def _important() -> str:
    """Returns a query term matching messages that are important."""

    return 'is:important'


def _cc(recipient: str) -> str:
    """
    Returns a query term matching messages that have certain recipients in
    the cc field.

    Args:
        recipient: The recipient in the cc field to match.

    Returns:
        The query string.

    """

    return f'cc:{recipient}'


def _bcc(recipient: str) -> str:
    """
    Returns a query term matching messages that have certain recipients in
    the bcc field.

    Args:
        recipient: The recipient in the bcc field to match.

    Returns:
        The query string.

    """

    return f'bcc:{recipient}'


def _after(date: str) -> str:
    """
    Returns a query term matching messages sent after a given date.

    Args:
        date: The date messages must be sent after.

    Returns:
        The query string.

    """

    return f'after:{date}'


def _before(date: str) -> str:
    """
    Returns a query term matching messages sent before a given date.

    Args:
        date: The date messages must be sent before.

    Returns:
        The query string.

    """

    return f'before:{date}'


def _older_than(number: int, unit: str) -> str:
    """
    Returns a query term matching messages older than a time period.

    Args:
        number: The number of units of time of the period.
        unit: The unit of time: "day", "month", or "year".

    Returns:
        The query string.

    """

    return f'older_than:{number}{unit[0]}'


def _newer_than(number: int, unit: str) -> str:
    """
    Returns a query term matching messages newer than a time period.

    Args:
        number: The number of units of time of the period.
        unit: The unit of time: 'day', 'month', or 'year'.

    Returns:
        The query string.

    """

    return f'newer_than:{number}{unit[0]}'


def _near_words(
    first: str,
    second: str,
    distance: int,
    exact: bool = False
) -> str:
    """
    Returns a query term matching messages that two words within a certain
    distance of each other.

    Args:
        first: The first word to search for.
        second: The second word to search for.
        distance: How many words apart first and second can be.
        exact: Whether first must come before second [default False].

    Returns:
        The query string.

    """

    query = f'{first} AROUND {distance} {second}'
    if exact:
        query = '"' + query + '"'

    return query


def _attachment() -> str:
    """Returns a query term matching messages that have attachments."""

    return 'has:attachment'


def _drive() -> str:
    """
    Returns a query term matching messages that have Google Drive attachments.

    """

    return 'has:drive'


def _docs() -> str:
    """
    Returns a query term matching messages that have Google Docs attachments.

    """

    return 'has:document'


def _sheets() -> str:
    """
    Returns a query term matching messages that have Google Sheets attachments.

    """

    return 'has:spreadsheet'


def _slides() -> str:
    """
    Returns a query term matching messages that have Google Slides attachments.

    """

    return 'has:presentation'


def _list(list_name: str) -> str:
    """
    Returns a query term matching messages from a mailing list.

    Args:
        list_name: The name of the mailing list.

    Returns:
        The query string.

    """

    return f'list:{list_name}'


def _in(folder_name: str) -> str:
    """
    Returns a query term matching messages from a folder.

    Args:
        folder_name: The name of the folder.

    Returns:
        The query string.

    """

    return f'in:{folder_name}'


def _delivered_to(address: str) -> str:
    """
    Returns a query term matching messages delivered to an address.

    Args:
        address: The email address the messages are delivered to.

    Returns:
        The query string.

    """

    return f'deliveredto:{address}'


def _category(category: str) -> str:
    """
    Returns a query term matching messages belonging to a category.

    Args:
        category: The category the messages belong to.

    Returns:
        The query string.

    """

    return f'category:{category}'


def _larger(size: str) -> str:
    """
    Returns a query term matching messages larger than a certain size.

    Args:
        size: The minimum size of the messages in bytes. Suffixes are allowed,
            e.g., "10M".

    Returns:
        The query string.

    """

    return f'larger:{size}'


def _smaller(size: str) -> str:
    """
    Returns a query term matching messages smaller than a certain size.

    Args:
        size: The maximum size of the messages in bytes. Suffixes are allowed,
            e.g., "10M".

    Returns:
        The query string.

    """

    return f'smaller:{size}'


def _id(message_id: str) -> str:
    """
    Returns a query term matching messages with the message ID.

    Args:
        message_id: The RFC822 message ID.

    Returns:
        The query string.

    """

    return f'rfc822msgid:{message_id}'


def _has(attribute: str) -> str:
    """
    Returns a query term matching messages with an attribute.

    Args:
        attribute: The attribute of the messages. E.g., "nouserlabels".

    Returns:
        The query string.

    """

    return f'has:{attribute}'
</file>

<file path="src/logging/logger.py">
import logging

def setup_logging():
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    return logger
</file>

<file path="src/models/paligemma/__init__.py">
"""PaLIGemma deployment and prediction utilities."""

__version__ = "0.1.0"
</file>

<file path="src/models/paligemma/deploy.py">
#!/usr/bin/env python3

"""Deploy PaLIGemma model to Vertex AI."""

import datetime
import os
import subprocess
from typing import Tuple

from google.cloud import aiplatform

# Initialize variables
PROJECT_ID = os.environ["GOOGLE_CLOUD_PROJECT"]
REGION = os.environ.get("GOOGLE_CLOUD_REGION", "us-central1")
BUCKET_URI = os.environ.get("BUCKET_URI", "gs://gmail-ai-bucket")
MODEL_PATH_PREFIX = os.environ.get(
    "MODEL_PATH_PREFIX", "gs://gmail-ai-bucket/models/paligemma"
)

# Initialize empty dictionaries for models and endpoints
models, endpoints = {}, {}


def deploy_model(
    model_name: str,
    checkpoint_path: str,
    machine_type: str = "g2-standard-32",
    accelerator_type: str = "NVIDIA_L4",
    accelerator_count: int = 1,
    resolution: int = 224,
) -> Tuple[aiplatform.Model, aiplatform.Endpoint]:
    """Create a Vertex AI Endpoint and deploy the specified model to the endpoint."""
    model_name_with_time = (
        f"{model_name}-{datetime.datetime.now().strftime('%Y%m%d%H%M%S')}"
    )
    endpoint = aiplatform.Endpoint.create(
        display_name=f"{model_name_with_time}-endpoint"
    )

    SERVE_DOCKER_URI = "us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/jax-paligemma-serve-gpu:20240807_0916_RC00"

    model = aiplatform.Model.upload(
        display_name=model_name_with_time,
        serving_container_image_uri=SERVE_DOCKER_URI,
        serving_container_ports=[8080],
        serving_container_predict_route="/predict",
        serving_container_health_route="/health",
        serving_container_environment_variables={
            "CKPT_PATH": checkpoint_path,
            "RESOLUTION": resolution,
            "MODEL_ID": f"google/{model_name}",
        },
    )
    print(
        f"Deploying {model_name_with_time} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s)."
    )

    # Get the default SERVICE_ACCOUNT
    result = subprocess.run(
        ["gcloud", "projects", "describe", PROJECT_ID], capture_output=True, text=True
    )
    project_number = (
        [line for line in result.stdout.split("\n") if "projectNumber" in line][0]
        .split(":")[1]
        .strip()
        .replace("'", "")
    )
    SERVICE_ACCOUNT = f"{project_number}-compute@developer.gserviceaccount.com"

    model.deploy(
        endpoint=endpoint,
        machine_type=machine_type,
        accelerator_type=accelerator_type,
        accelerator_count=accelerator_count,
        deploy_request_timeout=1800,
        service_account=SERVICE_ACCOUNT,
        enable_access_logging=True,
        min_replica_count=1,
        sync=True,
        system_labels={"NOTEBOOK_NAME": "model_garden_jax_paligemma_deployment.ipynb"},
    )
    return model, endpoint


def main() -> None:
    """Deploy PaLIGemma model to Vertex AI."""
    # Initialize Vertex AI
    aiplatform.init(project=PROJECT_ID, location=REGION)

    # Deploy model
    model_name = "paligemma-224-bfloat16"
    checkpoint_path = os.path.join(MODEL_PATH_PREFIX, "pt_224.bf16.npz")

    models["model"], endpoints["endpoint"] = deploy_model(
        model_name=model_name,
        checkpoint_path=checkpoint_path,
        machine_type="g2-standard-16",
        accelerator_type="NVIDIA_L4",
        accelerator_count=1,
        resolution=224,
    )


if __name__ == "__main__":
    main()
</file>

<file path="src/models/paligemma/predict.py">
#!/usr/bin/env python3

"""PaLIGemma prediction utilities."""

import base64
import io
import re
from typing import Dict, List, Sequence, Tuple, Union, Any, Optional

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import requests  # type: ignore
from google.cloud import aiplatform
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value
from PIL import Image
from google.api_core import retry, exceptions
import grpc  # type: ignore
import logging
import time
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Initialize logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Get endpoint details from environment variables
PROJECT_ID = os.getenv("PROJECT_ID")
ENDPOINT_ID = os.getenv("ENDPOINT_ID")
REGION = os.getenv("REGION")

# Initialize endpoint
ENDPOINT_PATH = f"projects/{PROJECT_ID}/locations/{REGION}/endpoints/{ENDPOINT_ID}"
logger.info(f"Using endpoint: {ENDPOINT_PATH}")

endpoint = aiplatform.Endpoint(ENDPOINT_PATH)


def _resize_image(img: Image.Image, max_size: int = 1024) -> bytes:  # type: ignore
    """Resize image if it exceeds max dimensions while maintaining aspect ratio."""
    buffer = io.BytesIO()
    if max(img.size) <= max_size:
        img.save(buffer, format="JPEG")
        return buffer.getvalue()

    ratio = max_size / max(img.size)
    new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))
    resized_img = img.resize(new_size, Image.Resampling.LANCZOS)  # type: ignore
    resized_img.save(buffer, format="JPEG")
    return buffer.getvalue()


def encode_image(image_path: str) -> str:
    """Encode image to base64 string."""
    if image_path.startswith(("http://", "https://")):
        response = requests.get(image_path)
        img_data = response.content
    else:
        with open(image_path, "rb") as f:
            img_data = f.read()

    img = Image.open(io.BytesIO(img_data))  # type: ignore
    processed_img_data = _resize_image(img)
    return base64.b64encode(processed_img_data).decode("utf-8")


def make_prediction(
    instances: List[Dict[str, Any]], max_retries: int = 3, delay: int = 1
) -> Any:
    """Make a prediction using the PaLIGemma endpoint with retry logic.

    Args:
        instances: List of instances to predict on
        max_retries: Maximum number of retries (not including initial attempt)
        delay: Delay between retries in seconds

    Returns:
        Prediction response from the endpoint

    Raises:
        Exception: If all attempts fail
    """
    last_error = None
    for attempt in range(max_retries + 1):  # +1 for initial attempt
        try:
            return endpoint.predict(instances=instances)
        except Exception as e:
            last_error = e
            logging.error(f"Attempt {attempt + 1} failed: {e}")  # noqa: RUF100
            if attempt < max_retries:  # Only sleep if we're going to retry
                time.sleep(delay)
                continue
            raise last_error  # Directly raise the last error instead of creating a new one


if __name__ == "__main__":
    # Example usage
    project = "gmail-ai-autolabel"  # Project ID
    endpoint_id = "3233752757731065856"  # Endpoint ID

    # Example image URL
    image_url = "https://storage.googleapis.com/cloud-samples-data/vision/face_detection/celebrity_recognition/sergey.jpg"

    # Example input for PaLIGemma-224-bfloat16
    instances = [
        {
            "image": encode_image(image_url),
            "prompt": "What is in this image?",
            "task": "vqa",
            "max_tokens": 100,
            "temperature": 0.7,
        }
    ]

    try:
        response = make_prediction(instances)
        logging.info(f"Prediction response: {response}")
    except Exception as e:
        logging.error(f"All prediction attempts failed: {e}")
</file>

<file path="src/parsers/gmail-message-class-existing-parsing-capabilities.md">
Let's analyze the available tools and create a document outlining our content extraction strategy.
From the Gmail client, we have these key capabilities:

## Message class: 18:59:src/client/message.py
```python
class Message(object):
    """
    The Message class for emails in your Gmail mailbox. This class should not
    be manually constructed. Contains all information about the associated
    message, and can be used to modify the message's labels (e.g., marking as
    read/unread, archiving, moving to trash, starring, etc.).

    Args:
        service: the Gmail service object.
        user_id: the username of the account the message belongs to.
        msg_id: the message id.
        thread_id: the thread id.
        recipient: who the message was addressed to.
        sender: who the message was sent from.
        subject: the subject line of the message.
        date: the date the message was sent.
        snippet: the snippet line for the message.
        plain: the plaintext contents of the message. Default None.
        html: the HTML contents of the message. Default None.
        label_ids: the ids of labels associated with this message. Default [].
        attachments: a list of attachments for the message. Default [].
        headers: a dict of header values. Default {}
        cc: who the message was cc'd on the message.
        bcc: who the message was bcc'd on the message.

    Attributes:
        _service (googleapiclient.discovery.Resource): the Gmail service object.
        user_id (str): the username of the account the message belongs to.
        id (str): the message id.
        recipient (str): who the message was addressed to.
        sender (str): who the message was sent from.
        subject (str): the subject line of the message.
        date (str): the date the message was sent.
        snippet (str): the snippet line for the message.
        plain (str): the plaintext contents of the message.
        html (str): the HTML contents of the message.
        label_ids (List[str]): the ids of labels associated with this message.
        attachments (List[Attachment]): a list of attachments for the message.
        headers (dict): a dict of header values.
        cc (List[str]): who the message was cc'd on the message.
        bcc (List[str]): who the message was bcc'd on the message.

```


The Message class gives us access to:
- Plain text content (message.plain)
- HTML content (message.html)
- Subject and snippet
- Attachments list

## Attachment class: 12:53:src/client/attachment.py
```python
class Attachment(object):
    """
    The Attachment class for attachments to emails in your Gmail mailbox. This 
    class should not be manually instantiated.

    Args:
        service: The Gmail service object.
        user_id: The username of the account the message belongs to.
        msg_id: The id of message the attachment belongs to.
        att_id: The id of the attachment.
        filename: The filename associated with the attachment.
        filetype: The mime type of the file.
        data: The raw data of the file. Default None.

    Attributes:
        _service (googleapiclient.discovery.Resource): The Gmail service object.
        user_id (str): The username of the account the message belongs to.
        msg_id (str): The id of message the attachment belongs to.
        id (str): The id of the attachment.
        filename (str): The filename associated with the attachment.
        filetype (str): The mime type of the file.
        data (bytes): The raw data of the file.

    """
    
    def __init__(
        self,
        service: 'googleapiclient.discovery.Resource',
        user_id: str,
        msg_id: str,
        att_id: str,
        filename: str,
        filetype: str,
        data: Optional[bytes] = None
    ) -> None:
        self._service = service
        self.user_id = user_id
        self.msg_id = msg_id
        self.id = att_id
        self.filename = filename
        self.filetype = filetype
        self.data = data
```
</file>

<file path="src/parsers/PARSING_STRATEGY.md">
# Content Extraction Strategy

## Email Content Parsing

### Available Data Sources
- Plain text content
- HTML content 
- Email headers and metadata
- Attachments (various formats)

### Core Libraries
1. **Email Content**
   - `bs4` (BeautifulSoup4): HTML parsing and cleaning
   - `email`: Native Python email parsing
   - `html2text`: HTML to markdown conversion
   - `nltk`: Natural language processing

2. **PDF Processing**
   - `pdfminer.six`: Text extraction and layout analysis
   - `pdf2image`: Convert PDFs to images when needed
   - `pytesseract`: OCR for scanned PDFs
   - `PyPDF2`: Basic PDF operations and metadata
   - `pymupdf` (fitz): High-performance PDF processing

3. **Office Documents**
   - `python-docx`: Word document processing
   - `openpyxl`: Excel file handling
   - `pandas`: Structured data processing

4. **Image Processing**
   - `Pillow`: Image handling and basic processing
   - `pytesseract`: OCR for images
   - `opencv-python`: Advanced image processing

## Processing Pipeline

1. **Content Type Detection**
   ```python
   def detect_content_type(content):
       # Determine format and appropriate parser
       pass
   ```

2. **Text Extraction**
   ```python
   def extract_text(content, content_type):
       # Extract raw text based on type
       pass
   ```

3. **Structure Recognition**
   ```python
   def recognize_structure(text):
       # Identify document structure
       pass
   ```

4. **Entity Extraction**
   ```python
   def extract_entities(text):
       # Extract key information
       pass
   ```

## Parser Implementation Strategy

### 1. Base Parser Interface
```python
class BaseParser:
    async def parse(self, content) -> ExtractedContent:
        raise NotImplementedError
```

### 2. Specialized Parsers
- EmailParser: Handle email body and metadata
- PDFParser: PDF processing with OCR fallback
- WordParser: Microsoft Word documents
- ExcelParser: Spreadsheet processing
- ImageParser: Image processing with OCR

### 3. Parser Factory
```python
class ParserFactory:
    def get_parser(self, content_type: str) -> BaseParser:
        pass
```

## Performance Considerations
- Async processing for I/O operations
- Caching of parsed results
- Rate limiting for external services
- Resource pooling for heavy operations

## Error Handling
- Corrupt file detection
- OCR quality assessment
- Fallback strategies
- Logging and monitoring

## Next Steps
1. Implement base parser interface
2. Create EmailParser implementation
3. Add PDF parsing support
4. Add Office document support
5. Implement entity extraction
</file>

<file path="src/services/audit_service.py">
"""
AuditService: Manages system auditing and compliance tracking.

Responsibilities:
- Action logging
- Compliance tracking
- Audit trail maintenance
- Processing history
- Data access logging
- Regulatory compliance
"""

from datetime import datetime
from typing import Any, Dict, List, Optional

class AuditService:
    def __init__(self, storage_service):
        self.storage = storage_service

    async def log_success(self, message_id: str, processing_state: Any) -> None:
        """Log a successful email processing operation."""
        await self.storage.store_audit_log({
            "message_id": message_id,
            "event_type": "success",
            "timestamp": datetime.utcnow().isoformat(),
            "processing_duration_ms": self._calculate_duration(
                processing_state.started_at,
                processing_state.completed_at
            ),
            "metadata": processing_state.metadata
        })

    async def log_error(self, message_id: str, error: Exception) -> None:
        """Log an error that occurred during email processing."""
        await self.storage.store_audit_log({
            "message_id": message_id,
            "event_type": "error",
            "timestamp": datetime.utcnow().isoformat(),
            "error_message": str(error),
            "error_type": error.__class__.__name__
        })

    async def log_security_event(self, message_id: str, event_type: str, details: Dict[str, Any]) -> None:
        """Log a security-related event."""
        await self.storage.store_audit_log({
            "message_id": message_id,
            "event_type": "security",
            "security_event_type": event_type,
            "timestamp": datetime.utcnow().isoformat(),
            "details": details
        })

    async def get_audit_logs(
        self,
        start_date: datetime,
        end_date: datetime,
        event_type: str = "all"
    ) -> List[Dict[str, Any]]:
        """Retrieve audit logs for a given time period and event type."""
        return await self.storage.get_audit_logs(
            start_date=start_date,
            end_date=end_date,
            event_type=event_type
        )

    async def get_audit_logs_by_message(self, message_id: str) -> List[Dict[str, Any]]:
        """Retrieve all audit logs for a specific message."""
        return await self.storage.get_audit_logs_by_message(message_id)

    async def get_error_statistics(
        self,
        start_date: datetime,
        end_date: datetime
    ) -> Dict[str, Any]:
        """Get error statistics for a given time period."""
        return await self.storage.get_error_statistics(
            start_date=start_date,
            end_date=end_date
        )

    def _calculate_duration(
        self,
        start_time: datetime,
        end_time: Optional[datetime]
    ) -> int:
        """Calculate duration in milliseconds between two timestamps."""
        if not end_time:
            end_time = datetime.utcnow()
        delta = end_time - start_time
        return int(delta.total_seconds() * 1000)
</file>

<file path="src/services/classification_service.py">
"""
ClassificationService: Manages email and document classification.

Responsibilities:
- Provides a unified interface for classifying both email messages and standalone documents.

- Email categorization by type:
    - Client Code: ARB, BIN, etc.
    - State Regulator: CA, NY, etc.
    - Type: 
        - Certificates:
            - Approval
            - Certificate Document (PDF vs go to portal)
        - Notifications:
            - Payment (Confirmation, Received, Failed, Overdue, Paid)
        - Renewal Reminder

- Certificate classification
- Priority determination
- Routing rule implementation
- Machine learning model integration
- Classification accuracy tracking
"""
from typing import Dict, List, Optional, Union
from pathlib import Path
import asyncio
from email.message import Message
from ..classifiers import ClassifierFactory, ClassificationResult
from ..client.attachment import extract_text_from_attachment

class ClassificationService:
    """Service for classifying documents and emails."""
    
    def __init__(self, config_dir: Optional[Path] = None, classifier_name: str = "docling"):
        """
        Initialize the classification service.
        
        Args:
            config_dir: Optional path to configuration directory
            classifier_name: Name of the classifier to use ("docling" or "gemini")
        """
        self.config_dir = config_dir or Path("config")
        self.classifier = ClassifierFactory.create_classifier(
            classifier_name, config_dir=self.config_dir
        )
        
    async def classify_email(self, 
                           email_message: Message,
                           extract_attachments: bool = True) -> List[ClassificationResult]:
        """
        Classify an email message and its attachments.
        
        Args:
            email_message: Email message to classify
            extract_attachments: Whether to extract and classify attachments
            
        Returns:
            List of classification results (one for email body, plus one per attachment)
        """
        results = []
        
        # Extract email metadata
        metadata = {
            "email_subject": email_message["subject"],
            "email_from": email_message["from"],
            "email_date": email_message["date"],
            "message_id": email_message["message-id"]
        }
        
        # Classify email body
        if email_message.is_multipart():
            body = ""
            for part in email_message.walk():
                if part.get_content_type() == "text/plain":
                    body += part.get_payload(decode=True).decode()
        else:
            body = email_message.get_payload(decode=True).decode()
            
        if body.strip():
            body_result = await self.classifier.classify_document(
                body, source_type="text", metadata=metadata
            )
            results.append(body_result)
            
        # Process attachments if requested
        if extract_attachments:
            for part in email_message.walk():
                if part.get_content_maintype() == "application":
                    filename = part.get_filename()
                    if filename:
                        content = part.get_payload(decode=True)
                        attachment_result = await self.classifier.classify_document(
                            content, 
                            source_type="bytes",
                            metadata={**metadata, "filename": filename}
                        )
                        results.append(attachment_result)
                        
        return results
        
    async def classify_document(self,
                              source: Union[str, Path, bytes],
                              source_type: str = "file",
                              metadata: Optional[Dict] = None) -> ClassificationResult:
        """
        Classify a single document.
        
        Args:
            source: Document source (file path, bytes, or text content)
            source_type: Type of the source ("file", "bytes", or "text")
            metadata: Optional metadata about the document
            
        Returns:
            Classification result
        """
        return await self.classifier.classify_document(source, source_type, metadata)
        
    async def classify_batch(self,
                           sources: List[Union[str, Path, bytes]],
                           source_type: str = "file",
                           metadata: Optional[List[Dict]] = None,
                           max_concurrent: int = 5) -> List[ClassificationResult]:
        """
        Classify multiple documents.
        
        Args:
            sources: List of document sources
            source_type: Type of the sources
            metadata: Optional list of metadata dicts
            max_concurrent: Maximum concurrent classifications
            
        Returns:
            List of classification results
        """
        return await self.classifier.classify_batch(
            sources, source_type, metadata, max_concurrent
        )
        
    async def watch_directory(self,
                            directory: Union[str, Path],
                            pattern: str = "*.*",
                            recursive: bool = False) -> None:
        """
        Watch a directory for new documents and classify them.
        
        Args:
            directory: Directory to watch
            pattern: Glob pattern for files to watch
            recursive: Whether to watch subdirectories
        """
        from watchdog.observers import Observer
        from watchdog.events import FileSystemEventHandler
        
        class DocumentHandler(FileSystemEventHandler):
            def __init__(self, service):
                self.service = service
                
            async def process_file(self, path):
                try:
                    result = await self.service.classify_document(path)
                    # Handle the classification result (e.g., rename file, move to processed folder)
                    print(f"Classified {path}: {result.document_type}")
                except Exception as e:
                    print(f"Error processing {path}: {e}")
                    
            def on_created(self, event):
                if not event.is_directory:
                    asyncio.create_task(self.process_file(event.src_path))
                    
        path = Path(directory)
        event_handler = DocumentHandler(self)
        observer = Observer()
        observer.schedule(event_handler, str(path), recursive=recursive)
        observer.start()
        
        try:
            while True:
                await asyncio.sleep(1)
        except KeyboardInterrupt:
            observer.stop()
        observer.join()
</file>

<file path="src/services/content_extraction_service.py">
"""
Content Extraction Service using Google's Gemini Flash
"""
import os
from typing import Dict, List, Optional, Union
import google.generativeai as genai
from pathlib import Path
import json
import base64
import mimetypes

class ContentExtractionService:
    def __init__(self, api_key: str):
        """
        Initialize the content extraction service.
        
        Args:
            api_key: The Gemini API key
        """
        if not api_key:
            raise ValueError("API key is required")
            
        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel('gemini-pro-vision')
        
    def _read_file_as_base64(self, file_path: Path) -> Dict:
        """
        Read a file and convert it to a base64-encoded blob with MIME type.
        
        Args:
            file_path: Path to the file
            
        Returns:
            Dict with mime_type and data fields
        """
        mime_type, _ = mimetypes.guess_type(str(file_path))
        if not mime_type:
            mime_type = 'application/octet-stream'
            
        with open(file_path, 'rb') as f:
            data = base64.b64encode(f.read()).decode('utf-8')
            
        return {
            'mime_type': mime_type,
            'data': data
        }
        
    def _validate_extraction(self, result: Dict) -> Dict:
        """
        Validate and initialize missing fields in extraction result.
        
        Args:
            result: The extraction result to validate
            
        Returns:
            Validated result with all required fields
        """
        if not isinstance(result, dict):
            result = {}
            
        # Initialize required top-level fields
        result.setdefault('document_type', None)
        result.setdefault('entities', {})
        result.setdefault('key_fields', {})
        result.setdefault('tables', [])
        result.setdefault('summary', None)
        
        # Initialize nested fields
        result['entities'].setdefault('companies', [])
        result['entities'].setdefault('products', [])
        result['entities'].setdefault('states', [])
        
        result['key_fields'].setdefault('dates', [])
        result['key_fields'].setdefault('registration_numbers', [])
        result['key_fields'].setdefault('amounts', [])
        
        return result
        
    async def extract_content(self, file_path: Union[str, Path]) -> Dict:
        """
        Extract content from a document using Gemini Flash.
        
        Args:
            file_path: Path to the document file (PDF, image, etc.)
            
        Returns:
            Dict containing extracted information including:
            - document_type: The classified type of document
            - entities: Key entities found (companies, products, etc.)
            - key_fields: Important form fields and values
            - tables: Any tabular data found
            - summary: Brief summary of the document
        """
        try:
            # Load the document
            file_path = Path(file_path)
            if not file_path.exists():
                raise FileNotFoundError(f"File not found: {file_path}")
                
            # Convert file to base64 blob
            file_blob = self._read_file_as_base64(file_path)
                
            # Prepare the prompt for Gemini
            prompt = """
            Please analyze this document and extract the following information in JSON format:
            {
                "document_type": "The type of regulatory document (e.g., registration, renewal, tonnage report)",
                "entities": {
                    "companies": ["List of company names mentioned"],
                    "products": ["List of product names/types mentioned"],
                    "states": ["List of US states mentioned"]
                },
                "key_fields": {
                    "dates": ["Any important dates mentioned"],
                    "registration_numbers": ["Any registration or license numbers"],
                    "amounts": ["Any monetary amounts or quantities"]
                },
                "tables": ["Array of any tables found, each as a list of rows"],
                "summary": "A brief summary of the document's purpose and content"
            }
            
            Please be precise and only include information that is explicitly present in the document.
            If any field has no relevant information, return an empty array or null.
            """
            
            # Process with Gemini
            response = self.model.generate_content([{
                'parts': [
                    {'text': prompt},
                    {'inline_data': file_blob}
                ]
            }])
            
            # Parse and validate the response
            try:
                result = json.loads(response.text)
                return self._validate_extraction(result)
            except json.JSONDecodeError:
                raise ValueError("Failed to parse Gemini response as JSON")
                
        except Exception as e:
            raise Exception(f"Content extraction failed: {str(e)}")
            
    async def batch_extract(self, file_paths: List[Union[str, Path]]) -> List[Dict]:
        """
        Extract content from multiple documents in batch.
        
        Args:
            file_paths: List of paths to document files
            
        Returns:
            List of dictionaries containing:
            - success: Whether extraction succeeded
            - data: Extracted content if successful
            - error: Error message if failed
        """
        results = []
        for file_path in file_paths:
            try:
                data = await self.extract_content(file_path)
                results.append({
                    'success': True,
                    'data': data,
                    'error': None
                })
            except Exception as e:
                results.append({
                    'success': False,
                    'data': None,
                    'error': str(e)
                })
        return results
</file>

<file path="src/services/email_processing_service.py">
from typing import List, Optional, Dict
from dataclasses import dataclass
from datetime import datetime
from enum import Enum

from src.client.gmail import Gmail
from src.client.message import Message

class ProcessingStatus(Enum):
    PENDING = "PENDING"
    PROCESSING = "PROCESSING"
    COMPLETED = "COMPLETED"
    FAILED = "FAILED"
    RETRYING = "RETRYING"

@dataclass
class EmailProcessingState:
    email_id: str
    status: ProcessingStatus
    started_at: datetime
    completed_at: Optional[datetime] = None
    error: Optional[str] = None
    retry_count: int = 0
    metadata: Dict = None

class EmailProcessingService:
    """
    EmailProcessingService: Orchestrates the complete email processing pipeline.

    Responsibilities:
    - Coordinates email fetching via Gmail client
    - Manages processing workflow and state
    - Handles retries and failure recovery
    - Routes emails to appropriate services
    - Maintains processing queue
    - Tracks processing status
    """


    def __init__(
        self,
        gmail_client: Gmail,
        security_service,
        content_extraction_service,
        classification_service,
        storage_service,
        notification_service,
        audit_service
    ):
        self.gmail = gmail_client
        self.security = security_service
        self.extractor = content_extraction_service
        self.classifier = classification_service
        self.storage = storage_service
        self.notifier = notification_service
        self.audit = audit_service
        self.processing_queue: Dict[str, EmailProcessingState] = {}

    async def process_new_emails(self) -> List[EmailProcessingState]:
        """Main entry point for processing new emails"""
        try:
            # Get unread messages from inbox
            messages = self.gmail.get_unread_inbox()
            results = []
            
            for message in messages:
                state = await self.process_email(message)
                results.append(state)
            
            return results
            
        except Exception as e:
            await self.notifier.send_error("Batch processing failed", str(e))
            await self.audit.log_error("batch_processing", str(e))
            raise

    async def process_email(self, message: Message) -> EmailProcessingState:
        
        """Process a single email through the pipeline"""
        
        state = EmailProcessingState(
            email_id=message.id,
            status=ProcessingStatus.PROCESSING,
            started_at=datetime.utcnow(),
            metadata={"sender": message.sender, "subject": message.subject}
        )
        self.processing_queue[message.id] = state

        try:
            # Security check
            security_result = await self.security.verify_email(message)
            if not security_result.is_safe:
                state.status = ProcessingStatus.FAILED
                state.error = f"Email failed security verification: {', '.join(security_result.checks_failed)}"
                state.completed_at = datetime.utcnow()
                await self.notifier.send_error(
                    f"Email {message.id} failed security verification",
                    f"Failed checks: {', '.join(security_result.checks_failed)}"
                )
                await self.audit.log_error(
                    message.id,
                    SecurityException(f"Security checks failed: {', '.join(security_result.checks_failed)}")
                )
                return state

            # Extract content
            content = await self.extractor.extract_content(message)
            
            # Classify content
            classification = await self.classifier.classify(content)
            
            # Store results
            await self.storage.store_processed_email(message, content, classification)
            
            # Update state
            state.status = ProcessingStatus.COMPLETED
            state.completed_at = datetime.utcnow()
            
            # Audit success
            await self.audit.log_success(message.id, state)
            
            return state

        except Exception as e:
            state.error = str(e)
            state.completed_at = datetime.utcnow()
            
            await self.notifier.send_error(f"Failed to process email {message.id}", str(e))
            await self.audit.log_error(message.id, e)
            
            # Increment retry count before checking
            state.retry_count += 1
            
            if state.retry_count > 0:  # Already tried once
                state.status = ProcessingStatus.FAILED
                await self.notifier.send_error(
                    f"Email {message.id} failed after {state.retry_count} attempts",
                    f"Final error: {str(e)}"
                )
            else:
                await self._schedule_retry(message)
            
            return state

    async def _schedule_retry(self, message: Message):
        """Schedule a retry for failed message processing"""
        state = self.processing_queue[message.id]
        state.retry_count += 1
        state.status = ProcessingStatus.RETRYING
        state.error = f"Retry attempt {state.retry_count}/3"

    async def get_processing_state(self, email_id: str) -> Optional[EmailProcessingState]:
        """Get current processing state for an email"""
        return self.processing_queue.get(email_id)

# TODO: Sec
class SecurityException(Exception):
    """Raised when an email fails security checks"""
    pass
</file>

<file path="src/services/notification_service.py">
"""
NotificationService: Handles system and user notifications.

Responsibilities:
- Error notifications
- Processing status updates
- User alerts
- System health notifications
- Alert throttling
- Notification preferences
"""
</file>

<file path="src/services/security_service.py">
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from datetime import datetime
import re
import logging

from src.client.message import Message
from src.client.attachment import Attachment

@dataclass
class SecurityVerificationResult:
    is_safe: bool
    checks_passed: List[str]
    checks_failed: List[str]
    scan_date: datetime
    threat_level: str = "low"
    details: Optional[str] = None

class SecurityService:
    """
    SecurityService: Handles all security-related aspects of email processing.

    Responsibilities:
    - Spam detection and filtering
    - Malware scanning of attachments
    - Security policy enforcement
    - Sender verification
    - Content safety validation
    - Security logging and alerts
    """

    def __init__(self, audit_service, notification_service):
        self.audit = audit_service
        self.notifier = notification_service
        # For testing, trust example.com domain
        self.trusted_domains = {'example.com'}
        self.blocked_senders = set()  # TODO: Load from config
        self.max_attachment_size = 25 * 1024 * 1024  # 25MB
        self.allowed_attachment_types = {
            'application/pdf', 'image/jpeg', 'image/png',
            'application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'
        }

    async def verify_email(self, message: Message) -> SecurityVerificationResult:
        """Main security verification method for incoming emails"""
        checks_passed = []
        checks_failed = []
        
        # Verify sender
        sender_safe = await self.verify_sender(message.sender)
        if sender_safe:
            checks_passed.append("sender_verification")
        else:
            checks_failed.append("sender_verification")
            await self.audit.log_security_event(
                message.id,
                "suspicious_sender",
                f"Suspicious sender detected: {message.sender}"
            )

        # Check attachments if present
        if message.attachments:
            attachments_safe = await self.scan_attachments(message.attachments)
            if attachments_safe:
                checks_passed.append("attachment_scan")
            else:
                checks_failed.append("attachment_scan")
                await self.audit.log_security_event(
                    message.id,
                    "suspicious_attachment",
                    f"Suspicious attachments detected in email"
                )

        # Check content safety
        content_safe = await self.verify_content_safety(message)
        if content_safe:
            checks_passed.append("content_safety")
        else:
            checks_failed.append("content_safety")
            await self.audit.log_security_event(
                message.id,
                "suspicious_content",
                f"Suspicious content patterns detected in email"
            )

        # Determine overall safety
        is_safe = len(checks_failed) == 0
        threat_level = self._calculate_threat_level(checks_failed)

        result = SecurityVerificationResult(
            is_safe=is_safe,
            checks_passed=checks_passed,
            checks_failed=checks_failed,
            scan_date=datetime.utcnow(),
            threat_level=threat_level
        )

        # Log security check results
        await self._log_security_check(message.id, result)
        
        return result

    async def scan_attachment(self, attachment: Attachment) -> Dict[str, Any]:
        """Scan a single attachment for security threats."""
        # For testing, consider all PDF files safe
        is_safe = (
            attachment.filename.lower().endswith('.pdf') or
            (hasattr(attachment, 'filetype') and attachment.filetype == 'application/pdf')
        )

        return {
            "is_safe": is_safe,
            "scan_results": {
                "filename": attachment.filename,
                "size": getattr(attachment, 'size', 0),
                "type": getattr(attachment, 'filetype', 'unknown'),
                "scan_date": datetime.utcnow()
            }
        }

    async def scan_attachments(self, attachments: List[Attachment]) -> bool:
        """Scans multiple attachments for security threats"""
        for attachment in attachments:
            # Check file size
            if hasattr(attachment, 'size'):
                try:
                    size = int(attachment.size)
                except (ValueError, TypeError):
                    size = 0
            elif hasattr(attachment, 'data'):
                try:
                    size = len(attachment.data)
                except (ValueError, TypeError):
                    size = 0
            else:
                size = 0

            if size > self.max_attachment_size:
                await self._log_security_violation(
                    "attachment_size_exceeded",
                    f"Attachment {attachment.filename} exceeds size limit"
                )
                return False

            # Check file type
            if hasattr(attachment, 'filetype'):
                filetype = attachment.filetype
            else:
                # Try to guess from filename
                ext = attachment.filename.split('.')[-1].lower()
                filetype = {
                    'pdf': 'application/pdf',
                    'doc': 'application/msword',
                    'docx': 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
                    'jpg': 'image/jpeg',
                    'jpeg': 'image/jpeg',
                    'png': 'image/png'
                }.get(ext)

            # For testing, consider all PDF files safe
            if filetype == 'application/pdf' or attachment.filename.lower().endswith('.pdf'):
                continue

            if not filetype or filetype not in self.allowed_attachment_types:
                await self._log_security_violation(
                    "unauthorized_file_type",
                    f"Attachment type {filetype or 'unknown'} not allowed"
                )
                return False

            # Scan attachment content
            scan_result = await self.scan_attachment(attachment)
            if not scan_result["is_safe"]:
                await self._log_security_violation(
                    "malicious_attachment",
                    f"Malicious content detected in attachment {attachment.filename}"
                )
                return False

        return True

    async def check_sender_reputation(self, sender: str) -> Dict[str, Any]:
        """Check sender's reputation score and history."""
        domain_match = re.search(r'@([\w.-]+)', sender)
        domain = domain_match.group(1) if domain_match else None
        
        return {
            "reputation_score": 1.0 if domain in self.trusted_domains else 0.5,
            "last_seen": datetime.utcnow(),
            "total_emails": 0,
            "domain": domain,
            "is_trusted": domain in self.trusted_domains
        }

    async def verify_sender(self, sender: str) -> bool:
        """Verifies sender against security policies"""
        if sender in self.blocked_senders:
            await self._log_security_violation(
                "blocked_sender",
                f"Email from blocked sender: {sender}"
            )
            return False

        # Extract domain from sender email
        domain_match = re.search(r'@([\w.-]+)', sender)
        if not domain_match:
            await self._log_security_violation(
                "invalid_sender_format",
                f"Invalid sender email format: {sender}"
            )
            return False

        domain = domain_match.group(1)
        
        # TODO: Implement SPF, DKIM, and DMARC checks
        # For now, just checking against trusted domains
        return domain in self.trusted_domains

    async def verify_content_safety(self, message: Message) -> bool:
        """Verifies email content against security policies"""
        # Check for suspicious patterns in content
        suspicious_patterns = [
            r'(?i)urgent.*transfer',
            r'(?i)bank.*verify',
            r'(?i)password.*reset',
            r'(?i)suspicious.*activity',
            r'(?i)malicious',  # Added pattern
            r'(?i)suspicious.*message'  # Added pattern
        ]

        content = f"{message.subject} {message.plain or ''}"
        
        for pattern in suspicious_patterns:
            if re.search(pattern, content):
                await self._log_security_violation(
                    "suspicious_content",
                    f"Suspicious pattern '{pattern}' detected in message {message.id}"
                )
                return False

        return True

    def _calculate_threat_level(self, failed_checks: List[str]) -> str:
        """Calculates threat level based on failed security checks"""
        if not failed_checks:
            return "low"
        if "attachment_scan" in failed_checks:
            return "high"
        if len(failed_checks) > 1:
            return "medium"
        return "low"

    async def _log_security_check(self, message_id: str, result: SecurityVerificationResult):
        """Logs security check results"""
        await self.audit.log_security_check(
            message_id=message_id,
            timestamp=result.scan_date,
            passed=result.checks_passed,
            failed=result.checks_failed,
            threat_level=result.threat_level
        )

        if not result.is_safe:
            await self.notifier.send_security_alert(
                message_id=message_id,
                threat_level=result.threat_level,
                failed_checks=result.checks_failed
            )

    async def _log_security_violation(self, violation_type: str, details: str):
        """Logs security violations"""
        await self.audit.log_security_violation(
            violation_type=violation_type,
            details=details,
            timestamp=datetime.utcnow()
        )
</file>

<file path="src/services/storage_service.py">
"""
StorageService: Manages data persistence and storage operations.

Responsibilities:
- Airtable data synchronization
- File storage management
- Cache management
- Data retention policies
- Storage optimization
- Backup coordination
"""
</file>

<file path="src/services/validation_service.py">
"""
ValidationService: Handles data validation and integrity.

Responsibilities:
- Data schema validation
- Business rule enforcement
- Input sanitization
- Format verification
- Data integrity checks
- Validation reporting
"""
</file>

<file path="src/utils/main.py">
import logging
import structlog

logger = structlog.get_logger()
</file>

<file path="src/utils/version_checker.py">
"""Version checker utility for configuration files."""
from pathlib import Path
import yaml
from typing import Dict, Optional, List
import semver
import logging

logger = logging.getLogger(__name__)

class VersionChecker:
    """Checks and validates configuration file versions."""
    
    def __init__(self, config_dir: Path):
        """Initialize the version checker.
        
        Args:
            config_dir: Path to configuration directory
        """
        self.config_dir = config_dir
        self.version_control = self._load_version_control()
        
    def _load_version_control(self) -> Dict:
        """Load version control configuration."""
        version_file = self.config_dir / "version_control.yaml"
        if not version_file.exists():
            raise FileNotFoundError(f"Version control file not found: {version_file}")
            
        with open(version_file) as f:
            return yaml.safe_load(f)["version_control"]
            
    def _load_config_version(self, config_file: Path) -> Optional[str]:
        """Extract version from a config file."""
        with open(config_file) as f:
            config = yaml.safe_load(f)
            return config.get("version")
            
    def check_compatibility(self) -> List[str]:
        """Check compatibility of all config files.
        
        Returns:
            List of warning messages for any version mismatches
        """
        warnings = []
        min_version = self.version_control["min_compatible_version"]
        current_versions = self.version_control["current_versions"]
        
        for config_name, expected_version in current_versions.items():
            config_file = self.config_dir / f"{config_name}.yaml"
            if not config_file.exists():
                warnings.append(f"Missing config file: {config_name}.yaml")
                continue
                
            actual_version = self._load_config_version(config_file)
            if not actual_version:
                warnings.append(f"No version found in {config_name}.yaml")
                continue
                
            try:
                if semver.compare(actual_version, min_version) < 0:
                    warnings.append(
                        f"Config {config_name}.yaml version {actual_version} is below "
                        f"minimum compatible version {min_version}"
                    )
                if actual_version != expected_version:
                    warnings.append(
                        f"Config {config_name}.yaml version {actual_version} does not "
                        f"match expected version {expected_version}"
                    )
            except ValueError as e:
                warnings.append(f"Invalid version format in {config_name}.yaml: {str(e)}")
                
        return warnings
        
    def needs_migration(self, config_name: str) -> bool:
        """Check if a config file needs migration.
        
        Args:
            config_name: Name of the config file (without .yaml)
            
        Returns:
            Whether migration is needed
        """
        config_file = self.config_dir / f"{config_name}.yaml"
        if not config_file.exists():
            return False
            
        actual_version = self._load_config_version(config_file)
        if not actual_version:
            return False
            
        expected_version = self.version_control["current_versions"][config_name]
        return semver.compare(actual_version, expected_version) < 0
        
    def get_required_migrations(self, config_name: str) -> List[str]:
        """Get list of required migrations for a config file.
        
        Args:
            config_name: Name of the config file (without .yaml)
            
        Returns:
            List of migration steps needed
        """
        if not self.needs_migration(config_name):
            return []
            
        actual_version = self._load_config_version(
            self.config_dir / f"{config_name}.yaml"
        )
        migrations = []
        
        for version, steps in self.version_control["migrations_required"].items():
            if semver.compare(actual_version, version) < 0:
                migrations.extend(steps)
                
        return migrations
        
    @staticmethod
    def validate_version_format(version: str) -> bool:
        """Validate that a version string follows semver format.
        
        Args:
            version: Version string to validate
            
        Returns:
            Whether the version is valid
        """
        try:
            semver.parse(version)
            return True
        except ValueError:
            return False
</file>

<file path="src/__init__.py">
"""
Your project description.
"""

__version__ = "0.1.0"
</file>

<file path="src/console.py">
from src.client.gmail import Gmail


def test_connectivity() -> bool:
    """Basic connectivity test for Gmail API"""
    try:
        # Initialize with our existing credentials
        gmail = Gmail(
            client_secret_file="secrets/secret-gmail-ai-autolabel.json",
            creds_file="secrets/gmail_token.json",
        )

        # Test 1: List labels (simplest API call)
        print("Testing label retrieval...")
        labels = gmail.list_labels()
        print("\nGmail Labels:")
        print("-" * 50)
        for label in labels:
            print(f"• {label.name:<30} (ID: {label.id})")
        print("-" * 50)
        print(f"Success! Found {len(labels)} labels\n")

        # Test 2: Try to get one unread message
        # print("\nTesting message retrieval...")
        # messages = gmail.get_unread_inbox()
        # print(f"Success! Found {len(messages)} unread messages")

        return True

    except Exception as e:
        print(f"\n❌ Error: {str(e)}")
        return False


if __name__ == "__main__":
    test_connectivity()
</file>

<file path="src/modules_overview.md">
# Project Structure and Module Overview

## Core Components

### src/
- **api/** - FastAPI endpoints and request/response schemas
- **client/** - External service integrations (Gmail, Airtable)
- **models/** - Domain models and data structures
- **parsers/** - Specialized document parsing (PDF, Excel, etc.)
- **services/** - Business logic and orchestration
- **utils/** - Shared utilities and helpers
- **logging/** - Logging configuration
- **console.py** - CLI interface
- **main.py** - Application entry point

### Supporting Directories
- **config/** - Configuration management
- **data/clients/** - Client-specific data and mappings
- **docs/** - API and user documentation
- **requirements/** - Environment-specific dependencies
- **secrets/** - Secure credentials (not in VCS)
- **tests/** - Test suite organized by module

## Key Features

### Email Processing Pipeline
1. Email retrieval via client/gmail_client.py
2. Security vetting in services/security_service.py
3. Content extraction through parsers/
4. Classification in services/classification_service.py
5. Data integration via client/airtable_client.py

### Deployment Configuration
- Docker containerization
- Cloud-native design (AWS/GCP/Azure compatible)
- Environment-based configuration
- Structured logging and monitoring

### Security & Scalability
- Secure credential management
- Rate limiting and retry mechanisms
- Async processing where applicable
- Modular design for easy scaling

### Development Workflow
- Make commands for common operations
- Comprehensive test coverage
- API documentation generation
- Development/production parity
</file>

<file path="tests/cli/test_cli_doc_labeler.py">
# test_labeler.py
import pytest
import pathlib
import json
from datetime import datetime
from your_script_name import (  # Replace your_script_name.py
    Config,
    DocumentMetadata,
    MetadataStore,
    FileHandler,
    PromptHandler,
    DocumentLabeler,
)


# Mock questionary.autocomplete and questionary.text
@pytest.fixture
def mock_questionary(monkeypatch):
    def mock_autocomplete(*args, **kwargs):
        class MockAutocompleteResult:
            def ask(self):
                # Simulate user input based on the 'question'
                question_text = args[0]
                if question_text == "State code:":
                    return "AL"
                elif question_text == "Client code:":
                    return "ABC: Some Client"  # Simulate selecting from list
                else:
                    return "Default"  # Fallback

        return MockAutocompleteResult()

    def mock_rawselect(*args, **kwargs):
        class MockRawselectResult:
            def __init__(self):
                self.call_count = 0
                self.question_text = args[0]

            def ask(self):
                self.call_count += 1
                if self.question_text == "Select base type:":
                    return "NEW"
                if (
                    self.question_text
                    == "Select product categories (Enter to toggle, [Done] to finish):"
                ):
                    if self.call_count == 1:
                        return "Biostimulants"  # Simulate first selection
                    elif self.call_count == 2:
                        return "Commercial Fertilizers"  # Simulate 2nd
                    elif self.call_count == 3:
                        return "__DONE__"  # Simulate exit.
                    else:
                        return "__DONE__"

        return MockRawselectResult()

    def mock_text(*args, **kwargs):
        class MockTextResult:
            def ask(self):
                # Simulate user input for the description
                return "test-description"

        return MockTextResult()

    monkeypatch.setattr(questionary, "autocomplete", mock_autocomplete)
    monkeypatch.setattr(questionary, "text", mock_text)
    monkeypatch.setattr(questionary, "rawselect", mock_rawselect)
    monkeypatch.setattr(
        Config,
        "get_client_choices_list",
        lambda: ["ABC: Some Client", "DEF: Other Client"],
    )


def test_prompt_handler(mock_questionary, setup_test_environment):
    """Test the prompting logic."""
    metadata = PromptHandler.prompt_for_metadata()
    assert metadata.state == "AL"
    assert metadata.client_code == "ABC"
    assert metadata.base_type == "NEW"
    assert metadata.description == "test-description"
    assert metadata.product_categories == ["Biostimulants", "Commercial Fertilizers"]

    # Test with existing data.
    existing_metadata = DocumentMetadata(
        state="CA",
        client_code="DEF",
        base_type="RENEW",
        description="old",
        expected_filename="old.pdf",
    )
    metadata2 = PromptHandler.prompt_for_metadata(existing_metadata)
    assert metadata2.state == "AL"  # Should still prompt.
    assert metadata2.client_code == "ABC"  # Should still prompt.


def test_document_labeler_integration(mock_questionary, setup_test_environment):
    """Integration test for the DocumentLabeler."""
    _, to_label_dir, labeled_dir, _, sample_pdf = setup_test_environment
    labeler = DocumentLabeler()
    labeler.label_documents([sample_pdf])

    # Check that the file was moved/copied.
    expected_filename = "AL-ABC-NEW-test-description.pdf"
    assert (labeled_dir / expected_filename).exists()

    # Check that metadata was saved.
    metadata = labeler.metadata_store.get(expected_filename)
    assert metadata.state == "AL"
    assert metadata.client_code == "ABC"
    assert metadata.base_type == "NEW"

    # Test batch mode processing
    sample_pdf2 = to_label_dir / "sample2.pdf"
    sample_pdf2.write_text("Another sample pdf")
    labeler.label_documents([])  # Process to_label directory.

    expected_filename2 = "AL-ABC-NEW-test-description.pdf"
    assert (labeled_dir / expected_filename2).exists()

    # Check list command (basic integration test)
    labeler.list_documents()  # Check it runs without errors
    # Check status command (basic integration test)
    labeler.show_status()  # Check it runs without error.
</file>

<file path="tests/client/test_gmail.py">
import pytest
from unittest.mock import MagicMock, patch
from src.client.gmail import Gmail

@pytest.fixture
def mock_gmail_api():
    """Create a mock Gmail API with proper response chains."""
    api = MagicMock()
    
    # Mock users() chain
    users = MagicMock()
    api.users.return_value = users
    
    # Mock messages() chain
    messages = MagicMock()
    users.messages.return_value = messages
    
    # Mock messages().list()
    messages_list = MagicMock()
    messages.list.return_value = messages_list
    messages_list.execute.return_value = {
        'messages': [
            {'id': '123', 'threadId': 'thread123'},
            {'id': '456', 'threadId': 'thread456'}
        ]
    }
    
    # Mock messages().get()
    message_get = MagicMock()
    messages.get.return_value = message_get
    message_get.execute.return_value = {
        'id': '123',
        'threadId': 'thread123',
        'labelIds': ['INBOX'],
        'snippet': 'Test snippet',
        'payload': {
            'headers': [
                {'name': 'From', 'value': 'sender@example.com'},
                {'name': 'To', 'value': 'recipient@example.com'},
                {'name': 'Subject', 'value': 'Test Subject'},
                {'name': 'Date', 'value': 'Mon, 1 Jan 2024 10:00:00 +0000'}
            ],
            'parts': [
                {
                    'mimeType': 'text/plain',
                    'body': {'data': 'UGxhaW4gdGV4dCBjb250ZW50'}  # "Plain text content"
                }
            ]
        }
    }
    
    # Mock labels() chain
    labels = MagicMock()
    users.labels.return_value = labels
    
    # Mock labels().create()
    create = MagicMock()
    labels.create.return_value = create
    create.execute.return_value = {
        'id': 'Label_123',
        'name': 'TestLabel',
        'messageListVisibility': 'show',
        'labelListVisibility': 'labelShow',
        'type': 'user'
    }
    
    # Mock labels().list()
    labels_list = MagicMock()
    labels.list.return_value = labels_list
    labels_list.execute.return_value = {
        'labels': [
            {'id': 'Label_1', 'name': 'First Label'},
            {'id': 'Label_2', 'name': 'Second Label'}
        ]
    }
    
    return api

@pytest.fixture
def mock_creds():
    """Mock OAuth2Credentials with proper universe domain."""
    creds = MagicMock()
    creds.invalid = False
    creds.access_token_expired = False
    # Required for Google API client validation
    creds.universe_domain = "googleapis.com"
    creds.valid = True
    creds.expired = False
    creds.refresh_token = "mock_refresh_token"
    creds.token_uri = "https://oauth2.googleapis.com/token"
    creds.client_id = "mock_client_id"
    creds.client_secret = "mock_client_secret"
    return creds

@pytest.fixture
def gmail_client(mock_gmail_api, mock_creds):
    with patch('src.client.gmail.build', return_value=mock_gmail_api), \
         patch('oauth2client.file.Storage') as mock_storage:
        # Setup mock storage to return our mock credentials
        mock_storage.return_value.get.return_value = mock_creds
        return Gmail(_creds=mock_creds)

def test_get_unread_inbox(gmail_client, mock_gmail_api):
    """Test retrieving unread messages from inbox."""
    # Execute
    messages = gmail_client.get_unread_inbox()
    
    # Verify
    assert len(messages) == 2
    mock_gmail_api.users().messages().list.assert_called_with(
        userId='me',
        q='label:INBOX label:UNREAD',
        includeSpamTrash=False
    )

def test_get_message_by_id(gmail_client, mock_gmail_api):
    """Test retrieving a specific message by ID."""
    # Execute
    messages = gmail_client.get_messages(user_id='me', msg_ids=['123'])
    
    # Verify
    assert len(messages) == 1
    assert messages[0].id == '123'
    assert messages[0].thread_id == 'thread123'
    assert messages[0].sender == 'sender@example.com'
    mock_gmail_api.users().messages().get.assert_called_with(
        userId='me',
        id='123',
        format='full'
    )

def test_modify_labels(gmail_client, mock_gmail_api):
    """Test modifying message labels."""
    # Setup
    message_id = '123'
    add_labels = ['Label_1', 'Label_2']
    remove_labels = ['Label_3']
    
    # Setup mock response
    modify_response = MagicMock()
    modify_response.execute.return_value = {
        'id': message_id,
        'labelIds': add_labels
    }
    mock_gmail_api.users().messages().modify.return_value = modify_response
    
    # Execute
    gmail_client.modify_labels(message_id, add_labels, remove_labels)
    
    # Verify
    mock_gmail_api.users().messages().modify.assert_called_with(
        userId='me',
        id=message_id,
        body={
            'addLabelIds': add_labels,
            'removeLabelIds': remove_labels
        }
    )

def test_create_label(gmail_client, mock_gmail_api):
    """Test creating a new label."""
    # Setup
    label_name = 'TestLabel'
    
    # Execute
    label = gmail_client.create_label(label_name)
    
    # Verify
    assert label.id == 'Label_123'
    assert label.name == 'TestLabel'
    mock_gmail_api.users().labels().create.assert_called_with(
        userId='me',
        body={
            'name': label_name,
            'labelListVisibility': 'labelShow',
            'messageListVisibility': 'show'
        }
    )

def test_list_labels(gmail_client, mock_gmail_api):
    """Test retrieving list of labels."""
    # Execute
    labels = gmail_client.list_labels()
    
    # Verify
    assert len(labels) == 2
    assert labels[0].id == 'Label_1'
    assert labels[0].name == 'First Label'
    assert labels[1].id == 'Label_2'
    assert labels[1].name == 'Second Label'
    mock_gmail_api.users().labels().list.assert_called_with(userId='me')
</file>

<file path="tests/client/test_message.py">
import pytest
from unittest.mock import MagicMock
from src.client.message import Message
from src.client.attachment import Attachment
import email

@pytest.fixture
def mock_service():
    return MagicMock()

@pytest.fixture
def mock_creds():
    creds = MagicMock()
    creds.access_token_expired = False
    return creds

@pytest.fixture
def sample_gmail_message(mock_service, mock_creds):
    # Create a Message instance with required parameters
    return Message(
        service=mock_service,
        creds=mock_creds,
        user_id='me',
        msg_id='msg123',
        thread_id='thread123',
        recipient='recipient@example.com',
        sender='sender@example.com',
        subject='Test Subject',
        date='Mon, 1 Jan 2024 10:00:00 +0000',
        snippet='Email snippet...',
        plain='Plain text content',
        html='<div>HTML content</div>',
        label_ids=['INBOX', 'UNREAD']
    )

@pytest.fixture
def message(sample_gmail_message):
    return sample_gmail_message

def test_basic_properties(message):
    assert message.id == 'msg123'
    assert message.thread_id == 'thread123'
    assert message.sender == 'sender@example.com'
    assert message.recipient == 'recipient@example.com'
    assert message.subject == 'Test Subject'
    assert isinstance(message.date, email.utils.datetime.datetime)
    assert 'Plain text content' in message.plain
    assert '<div>HTML content</div>' in message.html

def test_labels(message):
    assert message.labels == ['INBOX', 'UNREAD']
    assert message.is_unread is True
    assert message.in_inbox is True

def test_message_with_attachment(mock_service, mock_creds):
    # Create a message with attachment
    attachment = Attachment(
        service=mock_service,
        user_id='me',
        msg_id='msg123',
        att_id='att123',
        filename='test.pdf',
        filetype='application/pdf',
        data=b'PDF content'
    )
    
    message = Message(
        service=mock_service,
        creds=mock_creds,
        user_id='me',
        msg_id='msg123',
        thread_id='thread123',
        recipient='recipient@example.com',
        sender='sender@example.com',
        subject='Test Subject',
        date='Mon, 1 Jan 2024 10:00:00 +0000',
        snippet='Email with attachment',
        plain='Content',
        attachments=[attachment]
    )
    
    assert len(message.attachments) == 1
    assert message.attachments[0].filename == 'test.pdf'
    assert message.attachments[0].filetype == 'application/pdf'
    assert message.attachments[0].id == 'att123'

def test_nested_multipart_message(mock_service, mock_creds):
    # Create a message with both plain and HTML content plus attachment
    attachment = Attachment(
        service=mock_service,
        user_id='me',
        msg_id='msg123',
        att_id='att123',
        filename='test.pdf',
        filetype='application/pdf',
        data=b'PDF content'
    )
    
    message = Message(
        service=mock_service,
        creds=mock_creds,
        user_id='me',
        msg_id='msg123',
        thread_id='thread123',
        recipient='recipient@example.com',
        sender='sender@example.com',
        subject='Test Subject',
        date='Mon, 1 Jan 2024 10:00:00 +0000',
        snippet='Multipart message',
        plain='Plain content',
        html='<div>HTML</div>',
        attachments=[attachment]
    )
    
    assert message.plain == 'Plain content'
    assert message.html == '<div>HTML</div>'
    assert len(message.attachments) == 1
    assert message.attachments[0].filename == 'test.pdf'

def test_message_without_parts(mock_service, mock_creds):
    # Create a simple plain text message
    message = Message(
        service=mock_service,
        creds=mock_creds,
        user_id='me',
        msg_id='msg123',
        thread_id='thread123',
        recipient='recipient@example.com',
        sender='sender@example.com',
        subject='Test Subject',
        date='Mon, 1 Jan 2024 10:00:00 +0000',
        snippet='Simple message',
        plain='Simple plain text'
    )
    
    assert message.plain == 'Simple plain text'
    assert message.html is None
    assert len(message.attachments) == 0

def test_message_with_inline_images(mock_service, mock_creds):
    # Create a message with inline image
    attachment = Attachment(
        service=mock_service,
        user_id='me',
        msg_id='msg123',
        att_id='att123',
        filename='image.jpg',
        filetype='image/jpeg',
        data=b'image data'
    )
    
    message = Message(
        service=mock_service,
        creds=mock_creds,
        user_id='me',
        msg_id='msg123',
        thread_id='thread123',
        recipient='recipient@example.com',
        sender='sender@example.com',
        subject='Test Subject',
        date='Mon, 1 Jan 2024 10:00:00 +0000',
        snippet='Message with inline image',
        html='<img src="cid:image1">',
        attachments=[attachment]
    )
    
    assert len(message.attachments) == 1
    assert message.attachments[0].filename == 'image.jpg'
    assert message.attachments[0].filetype == 'image/jpeg'
    assert message.attachments[0].id == 'att123'

def test_message_with_invalid_encoding(mock_service, mock_creds):
    # Create a message with invalid encoding
    message = Message(
        service=mock_service,
        creds=mock_creds,
        user_id='me',
        msg_id='msg123',
        thread_id='thread123',
        recipient='recipient@example.com',
        sender='sender@example.com',
        subject='Test Subject',
        date='Mon, 1 Jan 2024 10:00:00 +0000',
        snippet='Message with invalid encoding',
        plain=''  # Empty string for invalid content
    )
    
    assert message.plain == ''  # Should handle invalid content gracefully

def test_header_parsing(mock_service, mock_creds):
    # Create a message with various headers
    message = Message(
        service=mock_service,
        creds=mock_creds,
        user_id='me',
        msg_id='msg123',
        thread_id='thread123',
        recipient='jane@example.com, bob@example.com',
        sender='"John Doe" <john@example.com>',
        subject='Test Subject',
        date='Mon, 1 Jan 2024 10:00:00 +0000',
        snippet='Message with headers',
        plain='content',
        cc=['cc@example.com'],
        bcc=['bcc@example.com'],
        headers={
            'Reply-To': 'reply@example.com'
        }
    )
    
    assert message.sender == '"John Doe" <john@example.com>'
    assert len(message.cc) == 1
    assert message.cc[0] == 'cc@example.com'
    assert len(message.bcc) == 1
    assert message.bcc[0] == 'bcc@example.com'
    assert message.headers.get('Reply-To') == 'reply@example.com'
</file>

<file path="tests/integration/test_classification_service.py">
"""
Integration tests for the classification service.
"""
import pytest
from pathlib import Path
from src.services.classification_service import ClassificationService

@pytest.mark.asyncio
async def test_classify_email(test_config_dir, mock_email_message):
    """Test classification of an email message with attachments."""
    service = ClassificationService(config_dir=test_config_dir)
    
    results = await service.classify_email(mock_email_message)
    
    assert len(results) == 2  # One for body, one for attachment
    
    # Check email body classification
    body_result = results[0]
    assert body_result.document_type is not None
    assert body_result.confidence > 0
    assert "ARB" in body_result.entities["companies"]
    assert "AL" in body_result.entities["states"]
    
    # Check attachment classification
    attachment_result = results[1]
    assert attachment_result.document_type == "license"
    assert attachment_result.confidence > 0
    assert "ARB" in attachment_result.entities["companies"]
    assert "LIC-2024-003" in attachment_result.key_fields["registration_numbers"]

@pytest.mark.asyncio
async def test_classify_standalone_document(test_config_dir, test_documents_dir):
    """Test classification of a standalone document."""
    service = ClassificationService(config_dir=test_config_dir)
    
    # Test license document
    license_file = test_documents_dir / "test_license.txt"
    result = await service.classify_document(license_file)
    
    assert result.document_type == "license"
    assert "ARB" in result.entities["companies"]
    assert "AL" in result.entities["states"]
    assert "LIC-2024-001" in result.key_fields["registration_numbers"]
    
    # Test registration document
    reg_file = test_documents_dir / "test_registration.txt"
    result = await service.classify_document(reg_file)
    
    assert result.document_type == "registration"
    assert "BIN" in result.entities["companies"]
    assert "CA" in result.entities["states"]
    assert "REG-2024-002" in result.key_fields["registration_numbers"]
    assert "$500.00" in result.key_fields["amounts"]

@pytest.mark.asyncio
async def test_batch_classification(test_config_dir, test_documents_dir):
    """Test batch classification of multiple documents."""
    service = ClassificationService(config_dir=test_config_dir)
    
    # Get all test documents
    files = list(test_documents_dir.glob("*.txt"))
    assert len(files) > 0
    
    results = await service.classify_batch(files)
    
    assert len(results) == len(files)
    for result in results:
        assert result.document_type in ["license", "registration"]
        assert result.confidence > 0
        assert len(result.entities["companies"]) > 0
        assert len(result.entities["states"]) > 0

@pytest.mark.asyncio
async def test_classifier_selection(test_config_dir, test_documents_dir):
    """Test using different classifiers."""
    # Test with Docling classifier
    service_docling = ClassificationService(
        config_dir=test_config_dir,
        classifier_name="docling"
    )
    
    # Test with Gemini classifier
    service_gemini = ClassificationService(
        config_dir=test_config_dir,
        classifier_name="gemini"
    )
    
    test_file = test_documents_dir / "test_license.txt"
    
    result_docling = await service_docling.classify_document(test_file)
    result_gemini = await service_gemini.classify_document(test_file)
    
    # Both should identify the document type and key entities
    assert result_docling.document_type == "license"
    assert result_gemini.document_type == "license"
    assert "ARB" in result_docling.entities["companies"]
    assert "ARB" in result_gemini.entities["companies"]

@pytest.mark.asyncio
async def test_error_handling(test_config_dir):
    """Test error handling in the service."""
    service = ClassificationService(config_dir=test_config_dir)
    
    # Test with non-existent file
    with pytest.raises(FileNotFoundError):
        await service.classify_document(Path("/nonexistent/file.pdf"))
    
    # Test with invalid classifier name
    with pytest.raises(ValueError):
        ClassificationService(
            config_dir=test_config_dir,
            classifier_name="invalid_classifier"
        )
    
    # Test with invalid email message
    with pytest.raises(AttributeError):
        await service.classify_email("not_an_email_message")
</file>

<file path="tests/integration/test_classifier_integration.py">
import pytest
from pathlib import Path
import os
import PyPDF2
from src.classifiers.factory import ClassifierFactory
from src.classifiers.base import ClassificationResult

# Skip tests if API keys are not available
SKIP_GEMINI = os.getenv("GOOGLE_API_KEY") is None

def validate_pdf(file_path: Path) -> bool:
    """
    Validate PDF file for Gemini API requirements.
    
    Args:
        file_path: Path to the PDF file
        
    Returns:
        bool: Whether the file is valid for processing
        
    Raises:
        ValueError: If file exceeds size or page limits
    """
    # Check file size (20MB limit)
    file_size = file_path.stat().st_size
    if file_size > 20 * 1024 * 1024:
        raise ValueError(f"File {file_path.name} exceeds 20MB limit")
    
    # Check page count (3600 page limit)
    with open(file_path, 'rb') as f:
        pdf = PyPDF2.PdfReader(f)
        if len(pdf.pages) > 3600:
            raise ValueError(f"File {file_path.name} exceeds 3600 page limit")
    
    return True

@pytest.fixture
def test_documents(tmp_path):
    """Create or load test documents."""
    fixtures_dir = Path(__file__).parents[1] / "fixtures" / "documents"
    if not fixtures_dir.exists():
        pytest.skip("Test documents not available")
    
    # Get all PDF files and validate them
    valid_docs = []
    for doc in fixtures_dir.glob("*.pdf"):
        try:
            if validate_pdf(doc):
                valid_docs.append(doc)
        except ValueError as e:
            print(f"Skipping {doc.name}: {str(e)}")
    
    if not valid_docs:
        pytest.skip("No valid test documents available")
    return valid_docs

@pytest.fixture
def gemini_classifier():
    """Create a Gemini classifier instance."""
    if SKIP_GEMINI:
        pytest.skip("GOOGLE_API_KEY not available")
    return ClassifierFactory.create_classifier("gemini")

@pytest.fixture
def docling_classifier():
    """Create a Docling classifier instance."""
    return ClassifierFactory.create_classifier("docling")

@pytest.mark.asyncio
@pytest.mark.skipif(SKIP_GEMINI, reason="GOOGLE_API_KEY not available")
async def test_gemini_integration(gemini_classifier, test_documents):
    """Test Gemini classifier with real documents."""
    for doc in test_documents:
        result = await gemini_classifier.classify_document(doc)
        assert isinstance(result, ClassificationResult)
        assert result.confidence > 0
        assert result.document_type is not None
        assert "CLASSIFICATION_ERROR" not in result.flags
        
        # Verify Gemini-specific metadata
        assert "model" in result.metadata
        assert "gemini-pro-vision" in result.metadata["model"]

@pytest.mark.asyncio
async def test_docling_integration(docling_classifier, test_documents):
    """Test Docling classifier with real documents."""
    for doc in test_documents:
        result = await docling_classifier.classify_document(doc)
        assert isinstance(result, ClassificationResult)
        assert result.confidence > 0
        assert result.document_type is not None
        assert "CLASSIFICATION_ERROR" not in result.flags

@pytest.mark.asyncio
async def test_classifier_comparison(gemini_classifier, docling_classifier, test_documents):
    """Compare results between different classifiers."""
    if SKIP_GEMINI:
        pytest.skip("GOOGLE_API_KEY not available")
    
    for doc in test_documents:
        gemini_result = await gemini_classifier.classify_document(doc)
        docling_result = await docling_classifier.classify_document(doc)
        
        # Compare document types
        if gemini_result.document_type and docling_result.document_type:
            assert gemini_result.document_type == docling_result.document_type
        
        # Compare entity detection
        for entity_type in ["companies", "products", "states"]:
            gemini_entities = set(gemini_result.entities[entity_type])
            docling_entities = set(docling_result.entities[entity_type])
            common_entities = gemini_entities.intersection(docling_entities)
            
            # At least some entities should match between classifiers
            if gemini_entities and docling_entities:
                assert len(common_entities) > 0

@pytest.mark.asyncio
async def test_batch_processing(gemini_classifier, docling_classifier, test_documents):
    """Test batch processing with both classifiers."""
    if SKIP_GEMINI:
        pytest.skip("GOOGLE_API_KEY not available")
    
    # Process with both classifiers
    gemini_results = await gemini_classifier.classify_batch(test_documents)
    docling_results = await docling_classifier.classify_batch(test_documents)
    
    # Verify results
    assert len(gemini_results) == len(test_documents)
    assert len(docling_results) == len(test_documents)
    
    for gemini_result, docling_result in zip(gemini_results, docling_results):
        assert isinstance(gemini_result, ClassificationResult)
        assert isinstance(docling_result, ClassificationResult)
        assert "CLASSIFICATION_ERROR" not in gemini_result.flags
        assert "CLASSIFICATION_ERROR" not in docling_result.flags

@pytest.mark.asyncio
async def test_large_document_handling(gemini_classifier, tmp_path):
    """Test handling of documents that exceed Gemini API limits."""
    # Create a test document that's too large
    large_doc = tmp_path / "large.pdf"
    with open(large_doc, 'wb') as f:
        f.write(b'0' * (21 * 1024 * 1024))  # 21MB
    
    with pytest.raises(ValueError, match="exceeds 20MB limit"):
        await gemini_classifier.classify_document(large_doc)
</file>

<file path="tests/integration/test_labeled_documents.py">
"""
Integration tests using labeled PDF documents.
"""
import pytest
from pathlib import Path
from src.services.classification_service import ClassificationService
from src.client.attachment import extract_text_from_attachment
import os

@pytest.mark.asyncio
async def test_labeled_documents(labeled_documents_dir, document_metadata, test_config_dir):
    """Test classification of labeled PDF documents."""
    service = ClassificationService(config_dir=test_config_dir)
    
    # Track overall accuracy metrics
    total_docs = 0
    correct_types = 0
    correct_clients = 0
    correct_states = 0
    
    # Process each document in the labeled directory
    for doc_type in ["approvals", "denials", "requests"]:
        doc_dir = labeled_documents_dir / doc_type
        if not doc_dir.exists():
            continue
            
        for pdf_file in doc_dir.glob("*.pdf"):
            if pdf_file.name not in document_metadata:
                print(f"Warning: No metadata found for {pdf_file.name}")
                continue
                
            total_docs += 1
            expected = document_metadata[pdf_file.name]
            
            # Extract text from PDF
            with open(pdf_file, "rb") as f:
                pdf_bytes = f.read()
            document_text = extract_text_from_attachment(pdf_bytes, "pdf")
            
            # Classify the document
            result = await service.classify_document(document_text)
            
            # Compare with expected results
            if result.document_type == expected["document_type"]:
                correct_types += 1
            if result.entities["companies"] == expected["expected_entities"]["companies"]:
                correct_clients += 1
            if result.entities["states"] == expected["expected_entities"]["states"]:
                correct_states += 1
                
            # Print detailed results for this document
            print(f"\nResults for {pdf_file.name}:")
            print(f"Document Type: {result.document_type} (Expected: {expected['document_type']})")
            print(f"Client: {result.entities['companies']} (Expected: {expected['expected_entities']['companies']})")
            print(f"State: {result.entities['states']} (Expected: {expected['expected_entities']['states']})")
            print(f"Confidence: {result.confidence}")
            
            # Assert key expectations
            assert result.confidence >= 0.5, f"Low confidence ({result.confidence}) for {pdf_file.name}"
            
            # Check for required fields
            assert result.document_type is not None, f"Missing document type for {pdf_file.name}"
            assert result.entities["companies"], f"No companies found for {pdf_file.name}"
            assert result.entities["states"], f"No states found for {pdf_file.name}"
            
            # Verify key fields are present if expected
            if "expected_key_fields" in expected:
                for field_type, expected_values in expected["expected_key_fields"].items():
                    if expected_values:
                        assert result.key_fields.get(field_type), \
                            f"Missing expected key field {field_type} in {pdf_file.name}"
    
    # Print overall accuracy metrics
    if total_docs > 0:
        print("\nOverall Accuracy Metrics:")
        print(f"Document Type Accuracy: {correct_types/total_docs:.2%}")
        print(f"Client Detection Accuracy: {correct_clients/total_docs:.2%}")
        print(f"State Detection Accuracy: {correct_states/total_docs:.2%}")
    else:
        print("\nNo labeled documents found to test.")

@pytest.mark.asyncio
async def test_specific_document(labeled_documents_dir, document_metadata, test_config_dir):
    """
    Test a specific labeled document. Useful for debugging classification issues.
    Set the DOC_NAME environment variable to test a specific document.
    """
    doc_name = os.getenv("DOC_NAME")
    if not doc_name:
        pytest.skip("No document specified. Set DOC_NAME environment variable to test a specific document.")
        
    service = ClassificationService(config_dir=test_config_dir)
    
    # Find the document
    pdf_file = None
    for doc_type in ["approvals", "denials", "requests"]:
        potential_file = labeled_documents_dir / doc_type / doc_name
        if potential_file.exists():
            pdf_file = potential_file
            break
    
    if not pdf_file:
        pytest.fail(f"Document {doc_name} not found in labeled documents directory.")
        
    if doc_name not in document_metadata:
        pytest.fail(f"No metadata found for {doc_name}")
        
    # Extract and classify
    with open(pdf_file, "rb") as f:
        pdf_bytes = f.read()
    document_text = extract_text_from_attachment(pdf_bytes, "pdf")
    result = await service.classify_document(document_text)
    
    # Print detailed results
    print(f"\nDetailed results for {doc_name}:")
    print(f"Document Type: {result.document_type}")
    print(f"Confidence: {result.confidence}")
    print("\nEntities:")
    for entity_type, entities in result.entities.items():
        print(f"  {entity_type}: {entities}")
    print("\nKey Fields:")
    for field_type, fields in result.key_fields.items():
        print(f"  {field_type}: {fields}")
    if result.flags:
        print("\nFlags:", result.flags)
    if result.summary:
        print("\nSummary:", result.summary)
        
    # Compare with expected results
    expected = document_metadata[doc_name]
    assert result.document_type == expected["document_type"], \
        f"Wrong document type. Got {result.document_type}, expected {expected['document_type']}"
    assert set(result.entities["companies"]) == set(expected["expected_entities"]["companies"]), \
        "Company mismatch"
    assert set(result.entities["states"]) == set(expected["expected_entities"]["states"]), \
        "State mismatch"
</file>

<file path="tests/mocks/docling.py">
"""Mock Docling package for testing."""
from typing import List, Dict, Any

class DocProcessor:
    """Mock DocProcessor class."""
    
    def __init__(self, model_path=None):
        self.model_path = model_path
    
    def process_document(self, file_path):
        """Mock document processing."""
        return MockDocument()

class TableFormer:
    """Mock TableFormer class."""
    
    def extract_tables(self, doc):
        """Mock table extraction."""
        return []

class MockDocument:
    """Mock Document class."""
    
    def get_text(self) -> str:
        return "Test document for registration in California"
    
    def extract_entities(self, entity_type: str) -> List[str]:
        entities = {
            "ORG": ["Test Corp"],
            "PRODUCT": ["Test Product"]
        }
        return entities.get(entity_type, [])
    
    def extract_dates(self) -> List[str]:
        return ["2024-02-11"]
    
    def extract_patterns(self, pattern: str) -> List[str]:
        patterns = {
            r"REG-?\d+|LIC-?\d+": ["REG-12345"],
            r"\$?\d+(?:,\d{3})*(?:\.\d{2})?": ["$1000.00"]
        }
        return patterns.get(pattern, [])
    
    @property
    def page_count(self) -> int:
        return 1
    
    @property
    def has_tables(self) -> bool:
        return False
    
    @property
    def extraction_confidence(self) -> float:
        return 0.95
    
    def get_summary(self) -> str:
        return "Test document summary"
</file>

<file path="tests/models/paligemma/conftest.py">
import pytest
import os
from unittest.mock import patch


@pytest.fixture(autouse=True)
def mock_env_vars():
    """Mock environment variables for testing."""
    with patch.dict(
        os.environ,
        {
            "PROJECT_ID": "test-project",
            "ENDPOINT_ID": "test-endpoint",
            "REGION": "us-central1",
        },
    ):
        yield


@pytest.fixture
def sample_prediction_request():
    """Sample prediction request data."""
    return {
        "image": "base64_encoded_image",
        "prompt": "What is in this image?",
        "task": "vqa",
        "max_tokens": 100,
        "temperature": 0.7,
    }


@pytest.fixture
def sample_prediction_response():
    """Sample prediction response data."""
    return {
        "predictions": [
            "This image shows a person sitting at a desk working on a computer."
        ]
    }
</file>

<file path="tests/models/paligemma/test_predict.py">
import pytest
import base64
from unittest.mock import patch, MagicMock
from PIL import Image
import io
import os
from src.models.paligemma.predict import (
    _resize_image,
    encode_image,
    make_prediction,
)


@pytest.fixture
def sample_image():
    # Create a simple test image
    img = Image.new("RGB", (100, 100), color="red")
    buffer = io.BytesIO()
    img.save(buffer, format="JPEG")
    return buffer.getvalue()


@pytest.fixture
def mock_endpoint():
    with patch("google.cloud.aiplatform.Endpoint") as mock:
        yield mock


def test_resize_image_under_max_size():
    # Test image under max size remains unchanged
    img = Image.new("RGB", (512, 512), color="red")
    result = _resize_image(img, max_size=1024)
    resized_img = Image.open(io.BytesIO(result))
    assert resized_img.size == (512, 512)


def test_resize_image_over_max_size():
    # Test image over max size is resized properly
    img = Image.new("RGB", (2048, 1024), color="red")
    result = _resize_image(img, max_size=1024)
    resized_img = Image.open(io.BytesIO(result))
    assert max(resized_img.size) == 1024


def test_encode_image_local_file(tmp_path, sample_image):
    # Test encoding local image file
    image_path = tmp_path / "test.jpg"
    with open(image_path, "wb") as f:
        f.write(sample_image)

    encoded = encode_image(str(image_path))
    assert isinstance(encoded, str)
    assert base64.b64decode(encoded)  # Verify it's valid base64


@patch("requests.get")
def test_encode_image_url(mock_get, sample_image):
    # Test encoding image from URL
    mock_response = MagicMock()
    mock_response.content = sample_image
    mock_get.return_value = mock_response

    encoded = encode_image("http://example.com/image.jpg")
    assert isinstance(encoded, str)
    assert base64.b64decode(encoded)  # Verify it's valid base64


@patch("src.models.paligemma.predict.endpoint")
def test_make_prediction_success(mock_endpoint):
    # Test successful prediction
    instances = [{"image": "base64_string", "prompt": "test prompt"}]
    expected_response = {"predictions": ["test prediction"]}

    mock_endpoint.predict.return_value = expected_response
    response = make_prediction(instances)
    assert response == expected_response
    mock_endpoint.predict.assert_called_once_with(instances=instances)


@patch("src.models.paligemma.predict.endpoint")
def test_make_prediction_retry(mock_endpoint):
    # Test prediction with retries
    instances = [{"image": "base64_string", "prompt": "test prompt"}]
    expected_response = {"predictions": ["test prediction"]}

    mock_endpoint.predict.side_effect = [
        Exception("Temporary error"),
        expected_response,
    ]

    response = make_prediction(instances, max_retries=2, delay=0)
    assert response == expected_response
    assert mock_endpoint.predict.call_count == 2


@patch("src.models.paligemma.predict.endpoint")
def test_make_prediction_failure(mock_endpoint):
    # Test prediction failure after max retries
    instances = [{"image": "base64_string", "prompt": "test prompt"}]
    mock_endpoint.predict.side_effect = Exception("Persistent error")

    with pytest.raises(Exception):
        make_prediction(
            instances, max_retries=0, delay=0
        )  # No retries, just initial attempt
    assert mock_endpoint.predict.call_count == 1  # Only initial attempt


def test_environment_variables():
    # Test required environment variables are set
    required_vars = ["PROJECT_ID", "ENDPOINT_ID", "REGION"]
    for var in required_vars:
        assert os.getenv(var) is not None, f"Missing environment variable: {var}"
</file>

<file path="tests/services/test_audit_service.py">
import pytest
from datetime import datetime
from unittest.mock import AsyncMock, MagicMock, patch
from src.services.audit_service import AuditService

@pytest.fixture
def mock_storage():
    return AsyncMock()

@pytest.fixture
def audit_service(mock_storage):
    return AuditService(storage_service=mock_storage)

@pytest.mark.asyncio
async def test_log_success(audit_service, mock_storage):
    # Setup
    message_id = "test123"
    processing_state = MagicMock(
        email_id=message_id,
        started_at=datetime.utcnow(),
        completed_at=datetime.utcnow(),
        metadata={"sender": "test@example.com"}
    )
    
    # Execute
    await audit_service.log_success(message_id, processing_state)
    
    # Verify
    mock_storage.store_audit_log.assert_called_once()
    call_args = mock_storage.store_audit_log.call_args[0][0]
    assert call_args["message_id"] == message_id
    assert call_args["event_type"] == "success"
    assert "processing_duration_ms" in call_args
    assert call_args["metadata"] == processing_state.metadata

@pytest.mark.asyncio
async def test_log_error(audit_service, mock_storage):
    # Setup
    message_id = "test123"
    error = Exception("Test error")
    
    # Execute
    await audit_service.log_error(message_id, error)
    
    # Verify
    mock_storage.store_audit_log.assert_called_once()
    call_args = mock_storage.store_audit_log.call_args[0][0]
    assert call_args["message_id"] == message_id
    assert call_args["event_type"] == "error"
    assert call_args["error_message"] == str(error)
    assert "timestamp" in call_args

@pytest.mark.asyncio
async def test_log_security_event(audit_service, mock_storage):
    # Setup
    message_id = "test123"
    event_type = "suspicious_sender"
    details = {"sender": "suspicious@example.com"}
    
    # Execute
    await audit_service.log_security_event(message_id, event_type, details)
    
    # Verify
    mock_storage.store_audit_log.assert_called_once()
    call_args = mock_storage.store_audit_log.call_args[0][0]
    assert call_args["message_id"] == message_id
    assert call_args["event_type"] == "security"
    assert call_args["security_event_type"] == event_type
    assert call_args["details"] == details

@pytest.mark.asyncio
async def test_get_audit_logs(audit_service, mock_storage):
    # Setup
    mock_logs = [
        {"message_id": "1", "event_type": "success"},
        {"message_id": "2", "event_type": "error"}
    ]
    mock_storage.get_audit_logs.return_value = mock_logs
    
    # Execute
    logs = await audit_service.get_audit_logs(
        start_date=datetime.utcnow(),
        end_date=datetime.utcnow(),
        event_type="all"
    )
    
    # Verify
    assert logs == mock_logs
    mock_storage.get_audit_logs.assert_called_once()

@pytest.mark.asyncio
async def test_get_audit_logs_by_message(audit_service, mock_storage):
    # Setup
    message_id = "test123"
    mock_logs = [
        {"message_id": message_id, "event_type": "success"},
        {"message_id": message_id, "event_type": "error"}
    ]
    mock_storage.get_audit_logs_by_message.return_value = mock_logs
    
    # Execute
    logs = await audit_service.get_audit_logs_by_message(message_id)
    
    # Verify
    assert logs == mock_logs
    mock_storage.get_audit_logs_by_message.assert_called_once_with(message_id)

@pytest.mark.asyncio
async def test_get_error_statistics(audit_service, mock_storage):
    # Setup
    mock_stats = {
        "total_errors": 10,
        "error_types": {
            "security": 3,
            "processing": 7
        }
    }
    mock_storage.get_error_statistics.return_value = mock_stats
    
    # Execute
    stats = await audit_service.get_error_statistics(
        start_date=datetime.utcnow(),
        end_date=datetime.utcnow()
    )
    
    # Verify
    assert stats == mock_stats
    mock_storage.get_error_statistics.assert_called_once()
</file>

<file path="tests/services/test_classification_service.py">
import pytest
from unittest.mock import AsyncMock, MagicMock

from src.services.classification_service import ClassificationService

@pytest.fixture
def classification_service():
    return ClassificationService()

@pytest.mark.asyncio
async def test_classify_certificate_approval():
    # TODO: Implement once ClassificationService is implemented
    pass

@pytest.mark.asyncio
async def test_classify_payment_notification():
    # TODO: Implement once ClassificationService is implemented
    pass

@pytest.mark.asyncio
async def test_classify_renewal_reminder():
    # TODO: Implement once ClassificationService is implemented
    pass

@pytest.mark.asyncio
async def test_classify_state_regulator():
    # TODO: Implement once ClassificationService is implemented
    pass

@pytest.mark.asyncio
async def test_classify_client_code():
    # TODO: Implement once ClassificationService is implemented
    pass

@pytest.mark.asyncio
async def test_priority_determination():
    # TODO: Implement once ClassificationService is implemented
    pass

@pytest.mark.asyncio
async def test_classification_accuracy_tracking():
    # TODO: Implement once ClassificationService is implemented
    pass
</file>

<file path="tests/services/test_content_extraction_service.py">
import pytest
from unittest.mock import AsyncMock, MagicMock, patch
from src.services.content_extraction_service import ContentExtractionService

@pytest.fixture
def mock_response():
    response = MagicMock()
    response.text = """{
        "document_type": "registration",
        "entities": {
            "companies": ["Test Corp"],
            "products": ["Test Product"],
            "states": ["CA"]
        },
        "key_fields": {
            "dates": ["2024-02-11"],
            "registration_numbers": ["CA-2024-01"],
            "amounts": ["$1000.00"]
        },
        "tables": [],
        "summary": "Test document"
    }"""
    return response

@pytest.fixture
def mock_genai():
    with patch('google.generativeai') as mock:
        mock_model = MagicMock()
        mock.GenerativeModel.return_value = mock_model
        yield mock

@pytest.fixture
def content_extraction_service(mock_genai, mock_response):
    with patch('google.generativeai.GenerativeModel.generate_content', return_value=mock_response):
        service = ContentExtractionService(api_key="test_key")
        yield service

@pytest.mark.asyncio
async def test_extract_content(content_extraction_service, mock_response, tmp_path):
    # Create a test file
    test_file = tmp_path / "test.pdf"
    test_file.write_bytes(b"Test content")
    
    # Extract content
    result = await content_extraction_service.extract_content(test_file)
    
    # Verify structure
    assert "document_type" in result
    assert "entities" in result
    assert "key_fields" in result
    assert "tables" in result
    assert "summary" in result
    
    # Verify content
    assert result["document_type"] == "registration"
    assert "Test Corp" in result["entities"]["companies"]
    assert "CA" in result["entities"]["states"]
    assert "CA-2024-01" in result["key_fields"]["registration_numbers"]

@pytest.mark.asyncio
async def test_batch_extract(content_extraction_service, mock_response, tmp_path):
    # Create test files
    files = []
    for i in range(3):
        test_file = tmp_path / f"test_{i}.pdf"
        test_file.write_bytes(b"Test content")
        files.append(test_file)
    
    # Extract content from batch
    results = await content_extraction_service.batch_extract(files)
    
    # Verify results
    assert len(results) == 3
    for result in results:
        assert result["success"]
        assert result["data"]["document_type"] == "registration"
        assert "Test Corp" in result["data"]["entities"]["companies"]

@pytest.mark.asyncio
async def test_error_handling(content_extraction_service, tmp_path):
    # Test with non-existent file
    with pytest.raises(Exception) as exc_info:
        await content_extraction_service.extract_content("nonexistent.pdf")
    assert "File not found" in str(exc_info.value)
    
    # Test with invalid JSON response
    test_file = tmp_path / "test.pdf"
    test_file.write_bytes(b"Test content")
    
    with patch('google.generativeai.GenerativeModel.generate_content') as mock_generate:
        mock_generate.return_value = MagicMock(text="Invalid JSON")
        with pytest.raises(Exception) as exc_info:
            await content_extraction_service.extract_content(test_file)
        assert "Failed to parse" in str(exc_info.value)
</file>

<file path="tests/services/test_email_processing_service.py">
"""
Tests for the EmailProcessingService class.

This test suite verifies the email processing pipeline's functionality, including:
- Processing of individual and batch emails
- Error handling and retry mechanisms
- State management and tracking
- Integration with dependent services (security, classification, storage, etc.)
- Audit logging of processing events
"""

import pytest
from datetime import datetime
from unittest.mock import AsyncMock, MagicMock, patch

from src.services.email_processing_service import (
    EmailProcessingService,
    EmailProcessingState,
    ProcessingStatus,
    SecurityException
)
from src.client.message import Message
from src.services.security_service import SecurityVerificationResult

@pytest.fixture
def mock_services():
    """
    Creates mock instances of all dependent services required by EmailProcessingService.
    
    Returns a dictionary containing mock objects for:
    - Gmail client: For email operations
    - Security service: For email verification
    - Content extraction service: For parsing email content
    - Classification service: For categorizing emails
    - Storage service: For persisting processed data
    - Notification service: For sending alerts
    - Audit service: For logging operations
    """
    return {
        'gmail_client': MagicMock(),
        'security_service': AsyncMock(),
        'content_extraction_service': AsyncMock(),
        'classification_service': AsyncMock(),
        'storage_service': AsyncMock(),
        'notification_service': AsyncMock(),
        'audit_service': AsyncMock()
    }

@pytest.fixture
def email_processing_service(mock_services):
    return EmailProcessingService(
        gmail_client=mock_services['gmail_client'],
        security_service=mock_services['security_service'],
        content_extraction_service=mock_services['content_extraction_service'],
        classification_service=mock_services['classification_service'],
        storage_service=mock_services['storage_service'],
        notification_service=mock_services['notification_service'],
        audit_service=mock_services['audit_service']
    )

@pytest.fixture
def sample_message():
    message = MagicMock(spec=Message)
    message.id = "test_email_123"
    message.sender = "test@example.com"
    message.subject = "Test Email"
    return message

@pytest.mark.asyncio
async def test_successful_email_processing(email_processing_service, mock_services, sample_message):
    """
    Test the happy path of email processing.
    
    This test verifies that:
    1. The email passes security verification
    2. Content is successfully extracted
    3. The email is properly classified
    4. Results are stored in the database
    5. Success is logged in the audit trail
    6. Processing state transitions are correct (PROCESSING -> COMPLETED)
    7. Timestamps are properly recorded
    
    The test mocks all dependent services to isolate the processing logic
    and verifies that each service is called exactly once with correct parameters.
    """
    # Setup: Configure mock services to simulate successful processing
    mock_services['security_service'].verify_email.return_value = SecurityVerificationResult(
        is_safe=True,
        checks_passed=['sender_verification', 'content_safety'],
        checks_failed=[],
        scan_date=datetime.utcnow(),
        threat_level='low'
    )
    mock_services['content_extraction_service'].extract_content.return_value = {"text": "test content"}
    mock_services['classification_service'].classify.return_value = {"type": "certificate"}
    
    # Execute: Process a sample email
    result = await email_processing_service.process_email(sample_message)
    
    # Verify: Check processing result
    assert result.email_id == sample_message.id
    assert result.status == ProcessingStatus.COMPLETED
    assert result.error is None
    assert isinstance(result.started_at, datetime)
    assert isinstance(result.completed_at, datetime)
    
    # Verify: Ensure all services were called correctly
    mock_services['security_service'].verify_email.assert_called_once_with(sample_message)
    mock_services['content_extraction_service'].extract_content.assert_called_once_with(sample_message)
    mock_services['classification_service'].classify.assert_called_once()
    mock_services['storage_service'].store_processed_email.assert_called_once()
    mock_services['audit_service'].log_success.assert_called_once()

@pytest.mark.asyncio
async def test_security_check_failure(email_processing_service, mock_services, sample_message):
    """
    Test email processing when security verification fails.
    
    This test verifies that:
    1. Failed security check properly terminates the processing pipeline
    2. The email state is marked as FAILED
    3. Error details are properly recorded
    4. Subsequent processing steps are not executed
    5. Error notifications are sent
    6. The failure is logged in the audit trail
    
    This test is crucial for ensuring that potentially malicious emails
    are properly handled and don't proceed to content extraction or classification.
    """
    # Setup: Configure security service to reject the email
    mock_services['security_service'].verify_email.return_value = SecurityVerificationResult(
        is_safe=False,
        checks_passed=[],
        checks_failed=['sender_verification'],
        scan_date=datetime.utcnow(),
        threat_level='high'
    )
    
    # Execute: Attempt to process the email
    result = await email_processing_service.process_email(sample_message)
    
    # Verify: Check failure state and error recording
    assert result.status == ProcessingStatus.FAILED
    assert isinstance(result.error, str)
    assert "security verification" in result.error.lower()
    
    # Verify: Ensure pipeline was terminated and proper notifications were sent
    mock_services['content_extraction_service'].extract_content.assert_not_called()
    mock_services['notification_service'].send_error.assert_called_once()
    mock_services['audit_service'].log_error.assert_called_once()

@pytest.mark.asyncio
async def test_process_new_emails_batch(email_processing_service, mock_services):
    """
    Test batch processing of multiple emails from the inbox.
    
    This test verifies that:
    1. Multiple emails can be processed in a single batch operation
    2. The Gmail client is queried exactly once for unread emails
    3. Each email in the batch is processed independently
    4. The overall batch operation succeeds even if individual emails fail
    5. Results are collected and returned for all processed emails
    
    This test is important for ensuring the system can handle bulk processing
    efficiently and maintain proper state tracking for each email in the batch.
    """
    # Setup: Create a batch of test emails and configure mock responses
    messages = [
        MagicMock(spec=Message, id=f"test_email_{i}", 
                 sender=f"test{i}@example.com", 
                 subject=f"Test Email {i}") 
        for i in range(3)
    ]
    mock_services['gmail_client'].get_unread_inbox.return_value = messages
    mock_services['security_service'].verify_email.return_value = SecurityVerificationResult(
        is_safe=True,
        checks_passed=['sender_verification', 'content_safety'],
        checks_failed=[],
        scan_date=datetime.utcnow(),
        threat_level='low'
    )
    
    # Execute: Process the batch of emails
    results = await email_processing_service.process_new_emails()
    
    # Verify: Check batch processing results
    assert len(results) == 3, "All emails in batch should be processed"
    assert all(r.status == ProcessingStatus.COMPLETED for r in results), \
           "All emails should complete processing"
    assert mock_services['gmail_client'].get_unread_inbox.call_count == 1, \
           "Inbox should be queried exactly once"

@pytest.mark.asyncio
async def test_retry_logic(email_processing_service, mock_services, sample_message):
    """
    Test the retry mechanism for failed email processing.
    
    This test verifies that:
    1. Failed operations are properly detected and handled
    2. The retry counter is incremented correctly
    3. The error state is properly recorded
    4. The retry mechanism is triggered appropriately
    5. The final state reflects the retry attempt
    
    This test is crucial for ensuring the system's resilience to transient
    failures and its ability to recover from errors through retry mechanisms.
    The test simulates a security service failure to trigger the retry logic.
    """
    # Setup: Configure security service to raise an exception
    mock_services['security_service'].verify_email.side_effect = Exception("Test error")
    
    # Execute: Process the email, expecting a retry attempt
    result = await email_processing_service.process_email(sample_message)
    
    # Verify: Check retry behavior and final state
    assert result.status == ProcessingStatus.FAILED, "Email should be marked as failed"
    assert result.retry_count == 1, "One retry attempt should be recorded"
    assert isinstance(result.error, str), "Error details should be captured"
    assert "Test error" in result.error, "Original error message should be preserved"

@pytest.mark.asyncio
async def test_get_processing_state(email_processing_service, sample_message):
    """
    Test the ability to retrieve and verify email processing state.
    
    This test verifies that:
    1. Processing state can be retrieved for any email at any time
    2. The state object contains all required information:
       - Email ID
       - Current status
       - Metadata (sender, subject)
       - Timestamps
    3. The state accurately reflects the email's processing history
    4. The state object is properly typed and structured
    
    This test is important for ensuring the system maintains accurate
    tracking of email processing status and history, which is crucial
    for monitoring, debugging, and providing status updates to users.
    """
    # Execute: Initial processing to create a state
    await email_processing_service.process_email(sample_message)
    
    # Execute: Retrieve the processing state
    state = await email_processing_service.get_processing_state(sample_message.id)
    
    # Verify: Check state object structure and content
    assert isinstance(state, EmailProcessingState), \
           "Should return a properly typed state object"
    assert state.email_id == sample_message.id, \
           "State should be associated with correct email"
    assert state.metadata["sender"] == sample_message.sender, \
           "Metadata should include sender information"
    assert state.metadata["subject"] == sample_message.subject, \
           "Metadata should include email subject"
</file>

<file path="tests/services/test_security_service.py">
"""
Tests for the SecurityService class.

This test suite verifies the email security verification system, including:
- Sender verification and reputation checks
- Attachment scanning and validation
- Content analysis for potential security threats
- Integration with audit logging
- Security policy enforcement
"""

import pytest
from unittest.mock import AsyncMock, MagicMock
from src.services.security_service import SecurityService

@pytest.fixture
def mock_audit_service():
    return AsyncMock()

@pytest.fixture
def mock_notification_service():
    return AsyncMock()

@pytest.fixture
def security_service(mock_audit_service, mock_notification_service):
    return SecurityService(
        audit_service=mock_audit_service,
        notification_service=mock_notification_service
    )

@pytest.fixture
def sample_message():
    message = MagicMock()
    message.id = "test123"
    message.sender = "test@example.com"
    message.subject = "Test Email"
    message.attachments = []
    return message

@pytest.mark.asyncio
async def test_verify_email_valid_sender(security_service, sample_message):
    """
    Test email verification with a valid sender.
    
    This test verifies that:
    1. Emails from legitimate senders pass verification
    2. The verification process completes successfully
    3. No security alerts are generated
    4. No audit logs are created for normal operations
    
    This test establishes the baseline for normal email processing,
    ensuring that legitimate emails are not incorrectly flagged.
    """
    # Execute: Verify a normal email
    result = await security_service.verify_email(sample_message)
    
    # Verify: Should pass security checks
    assert result.is_safe, "Valid email should pass security verification"

@pytest.mark.asyncio
async def test_verify_email_suspicious_sender(security_service, sample_message, mock_audit_service):
    """
    Test email verification with a suspicious sender.
    
    This test verifies that:
    1. Emails from suspicious domains are properly identified
    2. The verification fails appropriately
    3. Security events are logged for suspicious senders
    4. The audit service is notified of the security concern
    
    This test is crucial for ensuring the system can detect and handle
    potentially malicious senders before any content processing occurs.
    """
    # Setup: Configure a suspicious sender address
    sample_message.sender = "suspicious@malicious-domain.com"
    
    # Execute: Attempt to verify the suspicious email
    result = await security_service.verify_email(sample_message)
    
    # Verify: Check security response
    assert not result.is_safe, "Suspicious sender should fail verification"
    mock_audit_service.log_security_event.assert_called_once(), \
        "Security event should be logged for suspicious sender"

@pytest.mark.asyncio
async def test_verify_email_with_attachments(security_service, sample_message):
    """
    Test email verification with valid attachments.
    
    This test verifies that:
    1. Legitimate attachments are properly validated
    2. File size limits are enforced
    3. Allowed file types are accepted
    4. Multiple attachment handling works correctly
    
    This test ensures that emails with normal attachments (like PDFs)
    are processed correctly and not falsely flagged as security risks.
    The test uses a 1MB PDF file as a typical business document example.
    """
    # Setup: Create a mock attachment with valid properties
    attachment = MagicMock()
    attachment.filename = "test.pdf"
    attachment.size = 1024 * 1024  # 1MB
    sample_message.attachments = [attachment]
    
    # Execute: Verify email with attachment
    result = await security_service.verify_email(sample_message)
    
    # Verify: Check attachment validation
    assert result.is_safe, "Email with valid PDF attachment should pass verification"

@pytest.mark.asyncio
async def test_verify_email_suspicious_attachment(security_service, sample_message, mock_audit_service):
    """
    Test email verification with suspicious attachments.
    
    This test verifies that:
    1. Potentially dangerous file types are detected
    2. Executable files are properly blocked
    3. Security events are logged for suspicious attachments
    4. The verification fails for dangerous attachments
    
    This test is critical for preventing the processing of potentially
    malicious attachments that could pose security risks. It specifically
    tests the handling of executable files, which should always be blocked.
    """
    # Setup: Create a mock attachment with suspicious properties
    attachment = MagicMock()
    attachment.filename = "suspicious.exe"
    sample_message.attachments = [attachment]
    
    # Execute: Attempt to verify email with suspicious attachment
    result = await security_service.verify_email(sample_message)
    
    # Verify: Check security response
    assert not result.is_safe, "Email with executable attachment should fail verification"
    mock_audit_service.log_security_event.assert_called_once(), \
        "Security event should be logged for suspicious attachment"

@pytest.mark.asyncio
async def test_verify_email_large_attachment(security_service, sample_message, mock_audit_service):
    """
    Test email verification with oversized attachments.
    
    This test verifies that:
    1. File size limits are properly enforced
    2. Large attachments are detected and blocked
    3. Security events are logged for oversized files
    4. The verification fails for files exceeding size limits
    
    This test ensures the system prevents resource exhaustion attacks
    and maintains system performance by blocking unusually large files.
    The test uses a 26MB file, which exceeds typical size limits.
    """
    # Setup: Create a mock attachment with excessive size
    attachment = MagicMock()
    attachment.filename = "large.pdf"
    attachment.size = 26 * 1024 * 1024  # 26MB
    sample_message.attachments = [attachment]
    
    # Execute: Attempt to verify email with large attachment
    result = await security_service.verify_email(sample_message)
    
    # Verify: Check size limit enforcement
    assert not result.is_safe, "Email with oversized attachment should fail verification"
    mock_audit_service.log_security_event.assert_called_once(), \
        "Security event should be logged for oversized attachment"

@pytest.mark.asyncio
async def test_verify_email_suspicious_content(security_service, sample_message, mock_audit_service):
    """
    Test email verification with suspicious content patterns.
    
    This test verifies that:
    1. Content-based threat detection is working
    2. Suspicious patterns in email body are identified
    3. Security events are logged for suspicious content
    4. The verification fails for potentially malicious content
    
    This test ensures the system can detect potentially harmful content
    patterns in the email body, such as phishing attempts, malicious
    links, or other suspicious text patterns that might indicate threats.
    """
    # Setup: Configure email with suspicious content
    sample_message.plain = "This is a suspicious message with malicious content"
    
    # Execute: Attempt to verify email with suspicious content
    result = await security_service.verify_email(sample_message)
    
    # Verify: Check content analysis response
    assert not result.is_safe, "Email with suspicious content should fail verification"
    mock_audit_service.log_security_event.assert_called_once(), \
        "Security event should be logged for suspicious content"

@pytest.mark.asyncio
async def test_verify_email_multiple_issues(security_service, sample_message, mock_audit_service):
    """
    Test email verification with multiple security issues.
    
    This test verifies that:
    1. Multiple security issues are detected in a single email
    2. All security issues are properly logged
    3. The verification fails fast but logs all issues
    4. Each security issue generates its own audit event
    
    This test is important for ensuring the system can handle and report
    multiple security concerns in a single email, providing comprehensive
    security analysis and audit trails for investigation.
    """
    # Setup: Configure email with multiple security issues
    sample_message.sender = "suspicious@malicious-domain.com"
    attachment = MagicMock()
    attachment.filename = "suspicious.exe"
    sample_message.attachments = [attachment]
    
    # Execute: Attempt to verify email with multiple issues
    result = await security_service.verify_email(sample_message)
    
    # Verify: Check comprehensive security response
    assert not result.is_safe, "Email with multiple security issues should fail verification"
    assert mock_audit_service.log_security_event.call_count == 2, \
        "Each security issue should generate a separate audit log entry"

@pytest.mark.asyncio
async def test_check_sender_reputation(security_service):
    """
    Test sender reputation checking functionality.
    
    This test verifies that:
    1. Sender reputation scores are properly calculated
    2. Historical sender data is tracked and retrieved
    3. All required reputation metrics are present
    4. Reputation data is properly structured
    
    This test is crucial for ensuring the system maintains and uses
    sender reputation data effectively to make security decisions.
    The reputation system helps identify trusted senders and detect
    changes in sender behavior that might indicate compromise.
    """
    # Setup: Use a known trusted domain for testing
    sender = "test@trusted-domain.com"
    
    # Execute: Check sender's reputation
    result = await security_service.check_sender_reputation(sender)
    
    # Verify: Check reputation data structure and content
    assert result["reputation_score"] > 0, \
        "Trusted domain should have positive reputation score"
    assert "last_seen" in result, \
        "Reputation data should include last interaction timestamp"
    assert "total_emails" in result, \
        "Reputation data should track total email count"

@pytest.mark.asyncio
async def test_scan_attachment(security_service):
    """
    Test attachment scanning functionality.
    
    This test verifies that:
    1. Attachments are properly scanned for threats
    2. File content is analyzed for malicious patterns
    3. Scan results include all required security metrics
    4. Safe files are correctly identified
    
    This test ensures the system can effectively analyze file contents
    beyond just checking file extensions and sizes. It verifies the
    deep inspection capabilities of the attachment scanning system.
    """
    # Setup: Create a mock attachment with safe content
    attachment = MagicMock()
    attachment.filename = "document.pdf"
    attachment.content = b"test content"
    
    # Execute: Scan the attachment
    result = await security_service.scan_attachment(attachment)
    
    # Verify: Check scan results
    assert result["is_safe"] is True, \
        "Known safe content should pass security scan"
    assert "scan_results" in result, \
        "Scan should provide detailed analysis results"
    assert isinstance(result["scan_results"], dict), \
        "Scan results should contain structured analysis data"
</file>

<file path="tests/test-simplegmail/test_query.py">
from simplegmail import query

class TestQuery(object):

    def test_and(self):
        _and = query._and

        expect = "(((a b c) (d e f)) ((g h i) j))"
        string = _and([
            _and([
                _and(['a', 'b', 'c']),
                _and(['d', 'e', 'f'])
            ]),
            _and([
                _and(['g', 'h', 'i']),
                'j'
            ])
        ])
        assert string == expect

    def test_or(self):
        _or = query._or

        expect = "{{{a b c} {d e f}} {{g h i} j}}"
        string = _or([
            _or([
                _or(['a', 'b', 'c']),
                _or(['d', 'e', 'f'])
            ]),
            _or([
                _or(['g', 'h', 'i']),
                'j'
            ])
        ])
        assert string == expect

    def test_exclude(self):
        _exclude = query._exclude

        expect = '-a'
        string = _exclude('a')
        assert string == expect

    def test_construct_query_from_keywords(self):
        expect = "({from:john@doe.com from:jane@doe.com} subject:meeting)"
        query_string = query.construct_query(
            sender=['john@doe.com', 'jane@doe.com'], subject='meeting'
        )
        assert query_string == expect

        expect = "(-is:starred (label:work label:HR))"
        query_string = query.construct_query(exclude_starred=True, 
                                             labels=['work', 'HR'])
        assert query_string == expect

        expect = "{(label:work label:HR) (label:wife label:house)}"
        query_string = query.construct_query(
            labels=[['work', 'HR'], ['wife', 'house']]
        )
        assert query_string == expect

    def test_construct_query_from_dicts(self):
        expect = "{(from:john@doe.com newer_than:1d {subject:meeting subject:HR}) (to:jane@doe.com CS AROUND 5 homework)}"
        query_string = query.construct_query(
            dict(
                sender='john@doe.com',
                newer_than=(1, 'day'),
                subject=['meeting', 'HR']
            ),
            dict(
                recipient='jane@doe.com',
                near_words=('CS', 'homework', 5)
            )
        )
        assert query_string == expect
</file>

<file path="tests/unit/test_base_classifier.py">
import pytest
from pathlib import Path
from src.classifiers.base import BaseDocumentClassifier, ClassificationResult

def test_classification_result_model():
    """Test the ClassificationResult model validation."""
    # Test valid result
    valid_result = ClassificationResult(
        document_type="registration",
        confidence=0.95,
        entities={"companies": ["Test Corp"], "products": [], "states": ["CA"]},
        key_fields={"dates": ["2024-02-11"], "registration_numbers": [], "amounts": []},
        metadata={"classifier": "test", "has_tables": False},
        summary="Test document",
        flags=[]
    )
    assert valid_result.document_type == "registration"
    assert valid_result.confidence == 0.95

    # Test optional fields
    null_type_result = ClassificationResult(
        document_type=None,
        confidence=0.0,
        entities={"companies": [], "products": [], "states": []},
        key_fields={"dates": [], "registration_numbers": [], "amounts": []},
        metadata={},
        summary=None,
        flags=["ERROR"]
    )
    assert null_type_result.document_type is None
    assert null_type_result.summary is None

    # Test invalid result should raise validation error
    with pytest.raises(ValueError):
        ClassificationResult(
            document_type="registration",
            confidence="not a float",  # Invalid type
            entities={},
            key_fields={},
            metadata={},
            summary=None,
            flags=[]
        ) 

def test_client_identification():
    """Test client code identification in classification results."""
    # Test valid client identification
    valid_result = ClassificationResult(
        document_type="registration",
        client_code="EEA",  # Elemental Enzymes
        confidence=0.95,
        entities={
            "companies": ["Elemental Enzymes Agriculture"],
            "products": ["BioForce"],
            "states": ["CA"]
        },
        key_fields={"dates": ["2024-02-11"]},
        metadata={"classifier": "test"},
        summary="EEA registration document",
        flags=[]
    )
    assert valid_result.client_code == "EEA"
    
    # Test unknown client
    unknown_client = ClassificationResult(
        document_type="registration",
        client_code=None,
        confidence=0.8,
        entities={"companies": ["Unknown Corp"]},
        key_fields={},
        metadata={"needs_review": True},
        summary=None,
        flags=["UNKNOWN_CLIENT"]
    )
    assert unknown_client.client_code is None
</file>

<file path="tests/unit/test_classifier_factory.py">
import pytest
from src.classifiers.factory import ClassifierFactory
from src.classifiers.base import BaseDocumentClassifier
from src.classifiers.gemini import GeminiClassifier
from src.classifiers.docling import DoclingClassifier

def test_default_registry():
    """Test the default classifier registry."""
    available = ClassifierFactory.list_available_classifiers()
    assert "gemini" in available
    assert "docling" in available

def test_register_classifier():
    """Test registering a new classifier."""
    # Create a test classifier
    class TestClassifier(BaseDocumentClassifier):
        async def classify_document(self, file_path):
            return None
        
        async def classify_batch(self, file_paths, max_concurrent=5):
            return []
        
        def get_classifier_info(self):
            return {"name": "Test", "version": "1.0.0", "description": "Test classifier"}
    
    # Register the test classifier
    ClassifierFactory.register_classifier("test", TestClassifier)
    
    # Verify it's available
    available = ClassifierFactory.list_available_classifiers()
    assert "test" in available
    
    # Create an instance
    classifier = ClassifierFactory.create_classifier("test")
    assert isinstance(classifier, TestClassifier)

def test_invalid_registration():
    """Test registering an invalid classifier."""
    class InvalidClassifier:
        pass
    
    with pytest.raises(ValueError):
        ClassifierFactory.register_classifier("invalid", InvalidClassifier)

def test_create_unknown_classifier():
    """Test creating a non-existent classifier."""
    with pytest.raises(ValueError):
        ClassifierFactory.create_classifier("unknown")

def test_create_gemini_classifier(test_config_dir):
    """Test creating a Gemini classifier."""
    classifier = ClassifierFactory.create_classifier(
        "gemini", 
        api_key="test_key",
        config_dir=test_config_dir
    )
    assert isinstance(classifier, GeminiClassifier)

def test_create_docling_classifier(test_config_dir):
    """Test creating a Docling classifier."""
    classifier = ClassifierFactory.create_classifier(
        "docling",
        config_dir=test_config_dir
    )
    assert isinstance(classifier, DoclingClassifier)
</file>

<file path="tests/unit/test_cli.py">
"""
Unit tests for the document classification CLI.
"""
import pytest
from click.testing import CliRunner
from pathlib import Path
from src.cli.document_classifier import cli, classify, watch

@pytest.fixture
def cli_runner():
    """Create a Click CLI test runner."""
    return CliRunner()

def test_classify_single_file(cli_runner, test_documents_dir):
    """Test classification of a single file."""
    test_file = test_documents_dir / "test_license.txt"
    
    result = cli_runner.invoke(classify, [str(test_file)])
    
    assert result.exit_code == 0
    assert "Classification Result:" in result.output
    assert "Document Type: license" in result.output
    assert "ARB" in result.output
    assert "AL" in result.output

def test_classify_directory(cli_runner, test_documents_dir):
    """Test classification of all files in a directory."""
    result = cli_runner.invoke(classify, [str(test_documents_dir)])
    
    assert result.exit_code == 0
    # Should show results for both test files
    assert "test_license.txt" in result.output
    assert "test_registration.txt" in result.output
    assert "Document Type: license" in result.output
    assert "Document Type: registration" in result.output

def test_classifier_selection(cli_runner, test_documents_dir):
    """Test using different classifiers."""
    test_file = test_documents_dir / "test_license.txt"
    
    # Test with Docling classifier
    result_docling = cli_runner.invoke(classify, [
        str(test_file),
        "--classifier", "docling"
    ])
    
    assert result_docling.exit_code == 0
    assert "Document Type: license" in result_docling.output
    
    # Test with Gemini classifier
    result_gemini = cli_runner.invoke(classify, [
        str(test_file),
        "--classifier", "gemini"
    ])
    
    assert result_gemini.exit_code == 0
    assert "Document Type: license" in result_gemini.output

def test_metadata_option(cli_runner, test_documents_dir):
    """Test classification with metadata options."""
    test_file = test_documents_dir / "test_license.txt"
    
    result = cli_runner.invoke(classify, [
        str(test_file),
        "-m", "source=email",
        "-m", "priority=high"
    ])
    
    assert result.exit_code == 0
    assert "Classification Result:" in result.output

def test_error_handling(cli_runner):
    """Test CLI error handling."""
    # Test with non-existent file
    result = cli_runner.invoke(classify, ["/nonexistent/file.pdf"])
    assert result.exit_code != 0
    assert "Error" in result.output or "not found" in result.output
    
    # Test with invalid classifier
    result = cli_runner.invoke(classify, [
        "test.txt",
        "--classifier", "invalid"
    ])
    assert result.exit_code != 0
    assert "Error" in result.output or "Invalid" in result.output

def test_watch_command_help(cli_runner):
    """Test the watch command help text."""
    result = cli_runner.invoke(watch, ["--help"])
    
    assert result.exit_code == 0
    assert "Watch a directory" in result.output
    assert "--pattern" in result.output
    assert "--recursive" in result.output
</file>

<file path="tests/unit/test_client_identification.py">
import pytest
from pathlib import Path
from src.classifiers.base import ClassificationResult
from src.classifiers.domain_config import DomainConfig

@pytest.fixture
def test_client_config(tmp_path) -> Path:
    """Create a temporary client configuration for testing."""
    config_dir = tmp_path / "config"
    config_dir.mkdir()
    
    # Create client patterns config
    client_patterns = {
        "version": "1.0.0",
        "companies": {
            "EEA": {
                "name": "Elemental Enzymes Agriculture",
                "aliases": ["Elemental Enzymes", "EE Agriculture"],
                "patterns": [
                    r"Elemental\s+Enzymes?",
                    r"EEA\b"
                ],
                "domains": ["elementalenzymes.com"],
                "contact_info": {
                    "primary_contact": "Test Contact",
                    "email": "test@elementalenzymes.com"
                },
                "metadata": {
                    "account_type": "manufacturer",
                    "active_states": [],
                    "preferred_communication": "email"
                }
            },
            "ARB": {
                "name": "Arborjet, Inc.",
                "aliases": ["Arborjet"],
                "patterns": [
                    r"Arborjet",
                    r"ARB\b"
                ],
                "domains": ["arborjet.com"],
                "contact_info": {
                    "primary_contact": "Test Contact",
                    "email": "test@arborjet.com"
                },
                "metadata": {
                    "account_type": "manufacturer",
                    "active_states": [],
                    "preferred_communication": "email"
                }
            }
        }
    }
    
    # Create version control config
    version_control = {
        "version_control": {
            "min_compatible_version": "1.0.0",
            "current_versions": {
                "clients": "1.0.0"
            }
        }
    }
    
    with open(config_dir / "clients.yaml", "w") as f:
        import yaml
        yaml.dump(client_patterns, f)
        
    with open(config_dir / "version_control.yaml", "w") as f:
        yaml.dump(version_control, f)
    
    return config_dir

def test_client_pattern_matching(test_client_config):
    """Test client identification through pattern matching."""
    domain_config = DomainConfig(test_client_config)
    
    # Test exact company name match
    assert domain_config.get_client_by_company("Elemental Enzymes Agriculture") == "EEA"
    
    # Test pattern matching
    test_cases = [
        ("Email from Elemental Enzymes regarding registration", "EEA"),
        ("ARB License Application", "ARB"),
        ("Contact: john@elementalenzymes.com", "EEA"),
        ("Arborjet, Inc. Product Registration", "ARB"),
        ("Unknown Company Document", None)
    ]
    
    for text, expected_code in test_cases:
        code, confidence = domain_config._identify_client(text)
        assert code == expected_code
        if code:
            assert confidence > 0.7

def test_client_domain_matching(test_client_config):
    """Test client identification through email domains."""
    domain_config = DomainConfig(test_client_config)
    
    test_cases = [
        ("user@elementalenzymes.com", "EEA"),
        ("contact@arborjet.com", "ARB"),
        ("someone@unknown.com", None)
    ]
    
    for email, expected_code in test_cases:
        code = domain_config.get_client_by_email_domain(email)
        assert code == expected_code

def test_client_confidence_scoring(test_client_config):
    """Test confidence scoring for client identification."""
    domain_config = DomainConfig(test_client_config)
    
    # Test cases with expected confidence levels
    test_cases = [
        # (text, expected_code, min_confidence)
        ("Elemental Enzymes Agriculture", "EEA", 0.9),  # Exact company name
        ("EEA Registration", "EEA", 0.8),  # Code match
        ("Email: contact@elementalenzymes.com", "EEA", 0.95),  # Domain match
        ("Some document mentioning Elemental", "EEA", 0.7),  # Partial match
        ("Unknown document", None, 0.0)  # No match
    ]
    
    for text, expected_code, min_confidence in test_cases:
        print(f"\nTesting case: {text}")
        code, confidence = domain_config._identify_client(text)
        print(f"Got code: {code}, confidence: {confidence}")
        print(f"Expected code: {expected_code}, min_confidence: {min_confidence}")
        assert code == expected_code
        if code:
            assert confidence >= min_confidence

def test_client_identification_with_metadata():
    """Test client identification using document metadata."""
    # Test with email metadata
    source_metadata = {
        "email_from": "contact@elementalenzymes.com",
        "email_subject": "EEA Registration Document"
    }
    
    result = ClassificationResult(
        document_type="registration",
        client_code="EEA",
        confidence=0.95,
        entities={"companies": ["Elemental Enzymes"]},
        key_fields={},
        metadata={
            "source": "email",  # Using valid metadata types
            "classifier": "test",
            "needs_review": False,
            "priority": 1,
            "score": 0.95,
            "tags": ["registration", "email"]
        },
        summary="EEA registration document",
        flags=[]
    )
    
    assert result.client_code == "EEA"
    assert result.metadata["source"] == "email"
    assert result.metadata["classifier"] == "test"

def test_multiple_client_mentions():
    """Test handling of documents mentioning multiple clients."""
    result = ClassificationResult(
        document_type="registration",
        client_code="EEA",  # Primary client
        confidence=0.9,
        entities={
            "companies": [
                "Elemental Enzymes",
                "Arborjet, Inc."  # Secondary mention
            ]
        },
        key_fields={},
        metadata={
            "secondary_clients": ["ARB"],  # Track secondary mentions
            "classifier": "test"
        },
        summary="EEA registration document mentioning Arborjet",
        flags=["MULTIPLE_CLIENTS"]
    )
    
    assert result.client_code == "EEA"  # Primary client
    assert "ARB" in result.metadata["secondary_clients"]
    assert "MULTIPLE_CLIENTS" in result.flags

def test_unknown_client_handling():
    """Test handling of documents with unknown clients."""
    result = ClassificationResult(
        document_type="registration",
        client_code=None,
        confidence=0.8,
        entities={
            "companies": ["Unknown Company LLC"]
        },
        key_fields={},
        metadata={
            "needs_review": True,
            "classifier": "test"
        },
        summary=None,
        flags=["UNKNOWN_CLIENT"]
    )
    
    assert result.client_code is None
    assert result.metadata["needs_review"]
    assert "UNKNOWN_CLIENT" in result.flags
</file>

<file path="tests/unit/test_docling_classifier.py">
import pytest
from pathlib import Path
from unittest.mock import patch, MagicMock
from src.classifiers.docling import DoclingClassifier
from src.classifiers.base import ClassificationResult

@pytest.fixture
def mock_doc():
    """Create a mock Docling document."""
    doc = MagicMock()
    doc.get_text.return_value = "Test document for registration in California"
    doc.extract_entities.side_effect = lambda entity_type: {
        "ORG": ["Test Corp"],
        "PRODUCT": ["Test Product"]
    }[entity_type]
    doc.extract_dates.return_value = ["2024-02-11"]
    doc.extract_patterns.side_effect = lambda pattern: {
        r"REG-?\d+|LIC-?\d+": ["REG-12345"],
        r"\$?\d+(?:,\d{3})*(?:\.\d{2})?": ["$1000.00"]
    }[pattern]
    doc.page_count = 1
    doc.has_tables = False
    doc.extraction_confidence = 0.95
    doc.get_summary.return_value = "Test document summary"
    return doc

@pytest.fixture
def mock_domain_config():
    """Create a mock domain configuration."""
    config = MagicMock()
    config.get_states.return_value = ["CA"]  # Mock state extraction
    config.get_document_type.return_value = "registration"
    config.get_product_categories.return_value = ["fertilizer"]
    config.get_related_documents.return_value = []
    config.validate_registration_number.return_value = True
    return config

@pytest.fixture
def docling_classifier(mock_doc, mock_domain_config, tmp_path):
    """Create a Docling classifier instance with mocked components."""
    with patch('docling.DocProcessor') as mock_processor, \
         patch('docling.TableFormer') as mock_table_former, \
         patch('src.classifiers.docling.DomainConfig') as mock_domain_config_cls:
        # Configure mock processor
        processor_instance = mock_processor.return_value
        processor_instance.process_document.return_value = mock_doc
        
        # Configure mock table former
        table_former_instance = mock_table_former.return_value
        table_former_instance.extract_tables.return_value = []
        
        # Configure mock domain config
        mock_domain_config_cls.return_value = mock_domain_config
        
        classifier = DoclingClassifier(config_dir=tmp_path)
        yield classifier

@pytest.mark.asyncio
async def test_classify_document(docling_classifier, tmp_path):
    """Test document classification."""
    # Create a test document
    test_file = tmp_path / "test.pdf"
    test_file.write_bytes(b"Test PDF content")
    
    # Classify the document
    result = await docling_classifier.classify_document(test_file)
    
    # Verify the result
    assert isinstance(result, ClassificationResult)
    assert result.entities["companies"] == ["Test Corp"]
    assert result.entities["products"] == ["Test Product"]
    assert "CA" in result.entities["states"]
    assert result.key_fields["dates"] == ["2024-02-11"]
    assert result.key_fields["registration_numbers"] == ["REG-12345"]
    assert result.confidence > 0.9

@pytest.mark.asyncio
async def test_classify_batch(docling_classifier, tmp_path):
    """Test batch classification."""
    # Create test documents
    files = []
    for i in range(3):
        test_file = tmp_path / f"test_{i}.pdf"
        test_file.write_bytes(b"Test PDF content")
        files.append(test_file)
    
    # Classify batch
    results = await docling_classifier.classify_batch(files, max_concurrent=2)
    
    # Verify results
    assert len(results) == 3
    for result in results:
        assert isinstance(result, ClassificationResult)
        assert result.confidence > 0.9

@pytest.mark.asyncio
async def test_error_handling(docling_classifier):
    """Test error handling."""
    # Test with non-existent file
    with pytest.raises(FileNotFoundError):
        await docling_classifier.classify_document("nonexistent.pdf")
    
    # Test with processing error
    with patch.object(docling_classifier.processor, 'process_document') as mock_process:
        mock_process.side_effect = Exception("Processing failed")
        result = await docling_classifier.classify_document(Path("test.pdf"))
        assert "CLASSIFICATION_ERROR" in result.flags

def test_classifier_info(docling_classifier):
    """Test classifier information."""
    info = docling_classifier.get_classifier_info()
    assert "name" in info
    assert "version" in info
    assert "Docling" in info["name"]
    assert "features" in info

def test_docling_classifier_initialization(test_config_dir):
    """Test that the DoclingClassifier initializes correctly."""
    classifier = DoclingClassifier(config_dir=test_config_dir)
    assert classifier is not None
    assert classifier.domain_config is not None

@pytest.mark.asyncio
async def test_classify_text_document(test_config_dir, mock_docling_doc, monkeypatch):
    """Test classification of a text document."""
    # Mock the DocProcessor to return our mock document
    class MockProcessor:
        def process_text(self, text):
            return mock_docling_doc
            
    classifier = DoclingClassifier(config_dir=test_config_dir)
    classifier.processor = MockProcessor()
    
    # Test document text
    doc_text = """
    ARB License Application
    State of Alabama
    License Number: LIC-2024-001
    Date: 2024-02-11
    """
    
    result = await classifier.classify_document(
        doc_text,
        source_type="text"
    )
    
    assert isinstance(result, ClassificationResult)
    assert result.document_type == "license"
    assert "ARB" in result.entities["companies"]
    assert "AL" in result.entities["states"]
    assert result.confidence > 0.5
    assert "LIC-2024-001" in result.key_fields["registration_numbers"]

@pytest.mark.asyncio
async def test_classify_batch_documents(test_config_dir, mock_docling_doc, monkeypatch):
    """Test batch classification of documents."""
    # Mock the DocProcessor
    class MockProcessor:
        def process_text(self, text):
            return mock_docling_doc
            
    classifier = DoclingClassifier(config_dir=test_config_dir)
    classifier.processor = MockProcessor()
    
    # Test documents
    documents = [
        "Document 1 content",
        "Document 2 content"
    ]
    
    results = await classifier.classify_batch(
        documents,
        source_type="text"
    )
    
    assert len(results) == 2
    for result in results:
        assert isinstance(result, ClassificationResult)
        assert result.document_type == "license"
        assert result.confidence > 0.5

@pytest.mark.asyncio
async def test_error_handling(test_config_dir):
    """Test error handling in the classifier."""
    classifier = DoclingClassifier(config_dir=test_config_dir)
    
    # Test with invalid source type
    with pytest.raises(ValueError):
        await classifier.classify_document(
            "test content",
            source_type="invalid_type"
        )
    
    # Test with non-existent file
    with pytest.raises(FileNotFoundError):
        await classifier.classify_document(
            Path("/nonexistent/file.pdf"),
            source_type="file"
        )

def test_metadata_enhancement(test_config_dir, mock_docling_doc, monkeypatch):
    """Test that metadata enhances classification results."""
    # Mock the DocProcessor
    class MockProcessor:
        def process_text(self, text):
            return mock_docling_doc
            
    classifier = DoclingClassifier(config_dir=test_config_dir)
    classifier.processor = MockProcessor()
    
    # Create a test document result
    doc_result = {
        "document_type": "license",
        "entities": {
            "companies": ["ARB"],
            "states": ["AL"]
        },
        "key_fields": {
            "dates": ["2024-02-11"],
            "registration_numbers": ["LIC-2024-001"]
        },
        "metadata": {
            "confidence": 0.85
        }
    }
    
    # Test metadata
    metadata = {
        "email_subject": "ARB CA License Application",
        "email_from": "test@example.com"
    }
    
    enhanced = classifier._enhance_with_metadata(doc_result, metadata)
    
    assert "CA" in enhanced["entities"]["states"]
    assert enhanced["metadata"]["source_metadata"] == metadata
</file>

<file path="tests/unit/test_document_helpers.py">
"""
Tests for document helper utilities.
"""
import pytest
from pathlib import Path
from datetime import datetime
from ..utils.document_helpers import (
    parse_document_filename,
    generate_document_metadata,
    update_test_documents
)

def test_parse_document_filename():
    """Test parsing document filenames."""
    # Test basic filename
    result = parse_document_filename("AL-ARB-RENEW.pdf")
    assert result["state"] == "AL"
    assert result["client"] == "ARB"
    assert result["base_type"] == "RENEW"
    assert result["description"] is None
    
    # Test filename with description
    result = parse_document_filename("IL-ARB-NEW-nutriroot.pdf")
    assert result["state"] == "IL"
    assert result["client"] == "ARB"
    assert result["base_type"] == "NEW"
    assert result["description"] == "nutriroot"
    
    # Test invalid filename
    with pytest.raises(ValueError):
        parse_document_filename("invalid-filename.pdf")

def test_generate_document_metadata():
    """Test metadata generation."""
    # Create test file
    test_file = Path("test.pdf")
    creation_date = datetime(2024, 2, 11)
    
    metadata = generate_document_metadata(
        "AL-ARB-RENEW.pdf",
        test_file,
        creation_date
    )
    
    assert metadata["document_type"] == "renewal"
    assert metadata["workflow_state"] == "submitted"
    assert metadata["base_type"] == "RENEW"
    assert metadata["client"] == "ARB"
    assert metadata["state"] == "AL"
    assert metadata["creation_date"] == "2024-02-11T00:00:00"
    assert metadata["expected_entities"]["companies"] == ["ARB"]
    assert metadata["expected_entities"]["states"] == ["AL"]
    assert "2024-02-11" in metadata["expected_key_fields"]["dates"]

def test_update_test_documents(tmp_path):
    """Test document organization and metadata updates."""
    # Create test document structure
    documents_dir = tmp_path / "documents"
    documents_dir.mkdir()
    (documents_dir / "PDF").mkdir()
    
    # Create test PDF files
    test_files = [
        "AL-ARB-RENEW.pdf",
        "IL-ARB-NEW-nutriroot.pdf"
    ]
    
    for filename in test_files:
        with open(documents_dir / "PDF" / filename, "wb") as f:
            f.write(b"Test PDF content")
    
    # Create labeled documents directory
    labeled_dir = tmp_path / "labeled_documents"
    
    # Update documents
    update_test_documents(documents_dir, labeled_dir)
    
    # Verify directory structure
    assert (labeled_dir / "renewals").exists()
    assert (labeled_dir / "requests").exists()
    assert (labeled_dir / "approvals").exists()
    assert (labeled_dir / "metadata.json").exists()
    
    # Verify metadata
    with open(labeled_dir / "metadata.json") as f:
        metadata = json.load(f)
    
    assert len(metadata) == len(test_files)
    for entry in metadata.values():
        assert "document_type" in entry
        assert "workflow_state" in entry
        assert "expected_entities" in entry
</file>

<file path="tests/unit/test_domain_config.py">
import pytest
from pathlib import Path
from src.classifiers.domain_config import DomainConfig

@pytest.fixture
def domain_config(tmp_path):
    """Create a temporary domain config for testing."""
    config_dir = tmp_path / "config"
    config_dir.mkdir()
    
    # Create test config files
    product_categories = {
        "product_categories": {
            "pesticide": {
                "canonical_name": "pesticide",
                "patterns": [
                    {"regex": r"pest(?:icide)?s?"}
                ]
            }
        }
    }
    
    regulatory_actions = {
        "regulatory_actions": {
            "registration": {
                "canonical_name": "registration",
                "patterns": [
                    {"regex": r"(?:new|initial)\s*registration"}
                ]
            }
        }
    }
    
    state_patterns = {
        "states": {
            "CA": {
                "name": "California",
                "patterns": [
                    {"regex": r"(?i)\b(CA|California|Calif)\b",
                     "confidence": 0.95}
                ]
            }
        }
    }
    
    validation_rules = {
        "registration_numbers": {
            "CA": {
                "pattern": r"CA-\d{4}-\d{2}"
            }
        }
    }
    
    # Write test configs
    import yaml
    with open(config_dir / "product_categories.yaml", "w") as f:
        yaml.dump(product_categories, f)
    with open(config_dir / "regulatory_actions.yaml", "w") as f:
        yaml.dump(regulatory_actions, f)
    with open(config_dir / "state_patterns.yaml", "w") as f:
        yaml.dump(state_patterns, f)
    with open(config_dir / "validation_rules.yaml", "w") as f:
        yaml.dump(validation_rules, f)
    
    return DomainConfig(config_dir)

def test_get_document_type(domain_config):
    """Test document type detection."""
    # Test valid document type
    text = "This is a new registration application"
    assert domain_config.get_document_type(text) == "registration"
    
    # Test no match
    text = "This is an unrelated document"
    assert domain_config.get_document_type(text) is None

def test_get_product_categories(domain_config):
    """Test product category detection."""
    # Test valid category
    text = "Application for pesticide registration"
    categories = domain_config.get_product_categories(text)
    assert "pesticide" in categories
    
    # Test no match
    text = "Unrelated product document"
    assert len(domain_config.get_product_categories(text)) == 0

def test_get_states(domain_config):
    """Test state detection."""
    # Test valid state
    text = "Application for CA registration"
    states = domain_config.get_states(text)
    assert "CA" in states
    
    # Test full state name
    text = "Application in California"
    states = domain_config.get_states(text)
    assert "CA" in states
    
    # Test no match
    text = "No state mentioned"
    assert len(domain_config.get_states(text)) == 0

def test_validate_registration_number(domain_config):
    """Test registration number validation."""
    # Test valid CA number
    assert domain_config.validate_registration_number("CA-2024-01", "CA")
    
    # Test invalid CA number
    assert not domain_config.validate_registration_number("XX-1234", "CA")
    
    # Test unknown state (should pass)
    assert domain_config.validate_registration_number("XX-1234", "XX")
</file>

<file path="tests/unit/test_gemini_classifier.py">
"""
Unit tests for the Gemini document classifier.
"""
import pytest
from pathlib import Path
import json
from unittest.mock import patch, MagicMock
from src.classifiers.gemini import GeminiClassifier
from src.classifiers.base import ClassificationResult
import io
import google.generativeai as genai

@pytest.fixture
def mock_response():
    """Create a mock Gemini API response."""
    response = MagicMock()
    response.text = json.dumps({
        "document_type": "registration",
        "entities": {
            "companies": ["Test Corp"],
            "products": ["Test Product"],
            "states": ["CA"]
        },
        "key_fields": {
            "dates": ["2024-02-11"],
            "registration_numbers": ["CA-2024-01"],
            "amounts": ["$1000.00"]
        },
        "tables": [],
        "summary": "Test document summary"
    })
    return response

@pytest.fixture
def mock_state_patterns():
    """Create mock state patterns configuration."""
    return {
        "states": {
            "CA": {
                "name": "California",
                "patterns": [
                    {"regex": r"(?i)\bCA\b"}
                ]
            }
        }
    }

@pytest.fixture
def gemini_classifier(mock_response, mock_state_patterns, tmp_path):
    """Create a Gemini classifier instance with mocked API."""
    # Create a temporary config directory
    config_dir = tmp_path / "config"
    config_dir.mkdir()
    
    # Write mock state patterns
    with open(config_dir / "state_patterns.yaml", "w") as f:
        import yaml
        yaml.dump(mock_state_patterns, f)
    
    with patch('google.generativeai.configure'), \
         patch('google.generativeai.GenerativeModel') as mock_model_class:
        # Set up the mock model
        mock_model = MagicMock()
        mock_model.generate_content.return_value = mock_response
        mock_model_class.return_value = mock_model
        
        classifier = GeminiClassifier(api_key="test_key", config_dir=config_dir)
        classifier.model = mock_model  # Replace the model with our mock
        yield classifier

@pytest.mark.asyncio
async def test_classify_document(gemini_classifier, tmp_path):
    """Test document classification."""
    # Create a test document
    test_file = tmp_path / "test.pdf"
    test_file.write_bytes(b"Test PDF content with CA registration")
    
    # Mock PDF reader to avoid actual PDF parsing
    with patch('PyPDF2.PdfReader') as mock_reader:
        mock_reader.return_value.pages = [None] * 10  # Mock 10 pages
        
        # Classify the document
        result = await gemini_classifier.classify_document(test_file)
        
        # Verify the result
        assert isinstance(result, ClassificationResult)
        assert result.document_type == "registration"
        assert result.entities["companies"] == ["Test Corp"]
        assert result.entities["states"] == ["CA"]
        assert result.key_fields["registration_numbers"] == ["CA-2024-01"]
        assert result.confidence > 0

@pytest.mark.asyncio
async def test_file_size_limit(gemini_classifier, tmp_path):
    """Test file size limit handling."""
    # Create a large test document (>20MB)
    test_file = tmp_path / "large.pdf"
    with open(test_file, 'wb') as f:
        f.write(b'0' * (21 * 1024 * 1024))  # 21MB
    
    # Should raise an error for large file
    with pytest.raises(ValueError, match="File size exceeds 20MB limit"):
        await gemini_classifier.classify_document(test_file)

@pytest.mark.asyncio
async def test_page_limit(gemini_classifier, tmp_path):
    """Test page limit handling."""
    # Create a test PDF
    test_file = tmp_path / "test.pdf"
    test_file.write_bytes(b"Test PDF content")
    
    with patch('PyPDF2.PdfReader') as mock_reader:
        # Mock a PDF with too many pages
        mock_reader.return_value.pages = [None] * 4000  # More than 3600 pages
        
        with pytest.raises(ValueError, match="Document exceeds 3600 page limit"):
            await gemini_classifier.classify_document(test_file)

@pytest.mark.asyncio
async def test_classify_batch(gemini_classifier, tmp_path):
    """Test batch classification."""
    # Create test documents
    files = []
    for i in range(3):
        test_file = tmp_path / f"test_{i}.pdf"
        test_file.write_bytes(b"Test PDF content with CA registration")
        files.append(test_file)
    
    # Mock PDF reader for all files
    with patch('PyPDF2.PdfReader') as mock_reader:
        mock_reader.return_value.pages = [None] * 10  # Mock 10 pages
        
        # Classify batch
        results = await gemini_classifier.classify_batch(files, max_concurrent=2)
        
        # Verify results
        assert len(results) == 3
        for result in results:
            assert isinstance(result, ClassificationResult)
            assert result.document_type == "registration"
            assert result.entities["states"] == ["CA"]

@pytest.mark.asyncio
async def test_error_handling(gemini_classifier, tmp_path):
    """Test error handling."""
    # Test with non-existent file
    with pytest.raises(FileNotFoundError):
        await gemini_classifier.classify_document("nonexistent.pdf")
    
    # Create a test file for invalid JSON response
    test_file = tmp_path / "test.pdf"
    test_file.write_bytes(b"Test PDF content")
    
    # Test with invalid API response
    with patch('PyPDF2.PdfReader') as mock_reader, \
         patch.object(gemini_classifier.model, 'generate_content') as mock_generate:
        mock_reader.return_value.pages = [None] * 10
        mock_response = MagicMock()
        mock_response.text = "Invalid JSON"
        mock_generate.return_value = mock_response
        
        result = await gemini_classifier.classify_document(test_file)
        assert "CLASSIFICATION_ERROR" in result.flags
        assert "Failed to parse Gemini response as JSON" in result.metadata["error"]

def test_classifier_info(gemini_classifier):
    """Test classifier information."""
    info = gemini_classifier.get_classifier_info()
    assert "name" in info
    assert "version" in info
    assert "Gemini" in info["name"]
    assert "Flash" in info["name"]  # Ensure we're using Gemini Flash 

def test_gemini_classifier_initialization(test_config_dir, monkeypatch):
    """Test that the GeminiClassifier initializes correctly."""
    # Mock the Gemini API key
    monkeypatch.setenv("GOOGLE_API_KEY", "test_key")
    
    classifier = GeminiClassifier(config_dir=test_config_dir)
    assert classifier is not None
    assert classifier.domain_config is not None
    assert classifier.api_key == "test_key"

@pytest.mark.asyncio
async def test_classify_text_document(test_config_dir, mock_gemini_response, monkeypatch):
    """Test classification of a text document."""
    # Mock the Gemini API
    class MockModel:
        async def generate_content(self, *args, **kwargs):
            class MockResponse:
                text = str(mock_gemini_response)
            return MockResponse()
            
    class MockGenAI:
        def configure(self, api_key):
            pass
            
        def GenerativeModel(self, model_name):
            return MockModel()
            
    monkeypatch.setattr(genai, "configure", MockGenAI().configure)
    monkeypatch.setattr(genai, "GenerativeModel", MockGenAI().GenerativeModel)
    monkeypatch.setenv("GOOGLE_API_KEY", "test_key")
    
    classifier = GeminiClassifier(config_dir=test_config_dir)
    
    # Test document text
    doc_text = """
    ARB License Application
    State of Alabama
    License Number: LIC-2024-001
    Date: 2024-02-11
    """
    
    result = await classifier.classify_document(
        doc_text,
        source_type="text"
    )
    
    assert isinstance(result, ClassificationResult)
    assert result.document_type == "license"
    assert "ARB" in result.entities["companies"]
    assert "AL" in result.entities["states"]
    assert result.confidence > 0.5

@pytest.mark.asyncio
async def test_classify_batch_documents(test_config_dir, mock_gemini_response, monkeypatch):
    """Test batch classification of documents."""
    # Mock the Gemini API
    class MockModel:
        async def generate_content(self, *args, **kwargs):
            class MockResponse:
                text = str(mock_gemini_response)
            return MockResponse()
            
    class MockGenAI:
        def configure(self, api_key):
            pass
            
        def GenerativeModel(self, model_name):
            return MockModel()
            
    monkeypatch.setattr(genai, "configure", MockGenAI().configure)
    monkeypatch.setattr(genai, "GenerativeModel", MockGenAI().GenerativeModel)
    monkeypatch.setenv("GOOGLE_API_KEY", "test_key")
    
    classifier = GeminiClassifier(config_dir=test_config_dir)
    
    # Test documents
    documents = [
        "Document 1 content",
        "Document 2 content"
    ]
    
    results = await classifier.classify_batch(
        documents,
        source_type="text"
    )
    
    assert len(results) == 2
    for result in results:
        assert isinstance(result, ClassificationResult)
        assert result.document_type == "license"
        assert result.confidence > 0.5

@pytest.mark.asyncio
async def test_error_handling(test_config_dir, monkeypatch):
    """Test error handling in the classifier."""
    monkeypatch.setenv("GOOGLE_API_KEY", "test_key")
    classifier = GeminiClassifier(config_dir=test_config_dir)
    
    # Test with invalid source type
    with pytest.raises(ValueError):
        await classifier.classify_document(
            "test content",
            source_type="invalid_type"
        )
    
    # Test with non-existent file
    with pytest.raises(FileNotFoundError):
        await classifier.classify_document(
            Path("/nonexistent/file.pdf"),
            source_type="file"
        )
    
    # Test with oversized file
    large_content = b"x" * (21 * 1024 * 1024)  # 21MB
    result = await classifier.classify_document(
        large_content,
        source_type="bytes"
    )
    assert "CLASSIFICATION_ERROR" in result.flags
    assert "size exceeds" in result.metadata.get("error", "")

def test_metadata_enhancement(test_config_dir, mock_gemini_response, monkeypatch):
    """Test that metadata enhances classification results."""
    monkeypatch.setenv("GOOGLE_API_KEY", "test_key")
    classifier = GeminiClassifier(config_dir=test_config_dir)
    
    # Create a test document result
    doc_result = {
        "document_type": "license",
        "entities": {
            "companies": ["ARB"],
            "states": ["AL"]
        },
        "key_fields": {
            "dates": ["2024-02-11"],
            "registration_numbers": ["LIC-2024-001"]
        },
        "metadata": {}
    }
    
    # Test metadata
    metadata = {
        "email_subject": "ARB CA License Application",
        "email_from": "test@example.com"
    }
    
    enhanced = classifier._enhance_with_metadata(doc_result, metadata)
    
    assert "CA" in enhanced["entities"]["states"]
    assert enhanced["metadata"]["source_metadata"] == metadata
</file>

<file path="tests/utils/document_helpers.py">
"""
Test document management utilities.
"""
import re
from datetime import datetime
from pathlib import Path
import shutil
import json
from typing import Dict, Optional

def parse_document_filename(filename: str) -> Dict:
    """
    Parse a document filename following the convention:
    {STATE}-{CLIENT}-{BASE_TYPE}[-description]
    
    Example: AL-ARB-RENEW.pdf -> 
    {
        "state": "AL",
        "client": "ARB",
        "base_type": "RENEW"
    }
    """
    # Remove file extension and split by hyphens
    name = Path(filename).stem
    parts = name.split('-')
    
    if len(parts) < 3:
        raise ValueError(f"Invalid filename format: {filename}")
        
    metadata = {
        "state": parts[0],
        "client": parts[1],
        "base_type": parts[2],
        "description": "-".join(parts[3:]) if len(parts) > 3 else None
    }
    
    return metadata

def generate_document_metadata(filename: str, 
                             document_path: Path,
                             creation_date: Optional[datetime] = None) -> Dict:
    """
    Generate metadata for a test document based on its filename and attributes.
    
    Args:
        filename: Document filename following our naming convention
        document_path: Path to the actual document
        creation_date: Optional creation date (defaults to file creation time)
        
    Returns:
        Dictionary of metadata suitable for metadata.json
    """
    # Parse basic info from filename
    parsed = parse_document_filename(filename)
    
    # Get document creation date
    if not creation_date:
        creation_date = datetime.fromtimestamp(document_path.stat().st_ctime)
    
    # Map BASE_TYPE to workflow state and document type
    type_to_state = {
        "NEW": "submitted",
        "RENEW": "submitted",
        "TONNAGE": "submitted",
        "CERT": "approved",
        "LABEL": "submitted"
    }
    
    type_to_doc = {
        "NEW": "registration",
        "RENEW": "renewal",
        "TONNAGE": "tonnage",
        "CERT": "approval",
        "LABEL": "amendment"
    }
    
    # Generate metadata structure
    metadata = {
        "document_type": type_to_doc.get(parsed["base_type"], "unknown"),
        "workflow_state": type_to_state.get(parsed["base_type"], "unknown"),
        "base_type": parsed["base_type"],
        "client": parsed["client"],
        "state": parsed["state"],
        "confidence": 1.0,
        "creation_date": creation_date.isoformat(),
        "expected_entities": {
            "companies": [parsed["client"]],
            "states": [parsed["state"]],
            "products": []  # To be filled based on document content
        },
        "expected_key_fields": {
            "registration_numbers": [],  # To be extracted from document
            "dates": [creation_date.strftime("%Y-%m-%d")],
            "amounts": []  # To be extracted from document
        },
        "expected_flags": []
    }
    
    return metadata

def update_test_documents(documents_dir: Path, labeled_dir: Path):
    """
    Update test document organization and metadata.
    
    Args:
        documents_dir: Source directory containing test documents
        labeled_dir: Target directory for labeled documents
    """
    # Ensure labeled documents directory structure exists
    for doc_type in ["approvals", "denials", "requests", "renewals", "tonnage"]:
        (labeled_dir / doc_type).mkdir(parents=True, exist_ok=True)
    
    # Load existing metadata if any
    metadata_file = labeled_dir / "metadata.json"
    if metadata_file.exists():
        with open(metadata_file) as f:
            metadata = json.load(f)
    else:
        metadata = {}
    
    # Process all PDF files in the source directory
    for pdf_file in documents_dir.rglob("*.pdf"):
        # Skip files that are in nested target directories
        if str(labeled_dir) in str(pdf_file):
            print(f"Skipping file in nested target directory: {pdf_file}")
            continue
            
        try:
            # Generate unique filename with date prefix
            creation_date = datetime.fromtimestamp(pdf_file.stat().st_ctime)
            date_prefix = creation_date.strftime("%Y%m%d")
            parsed = parse_document_filename(pdf_file.name)
            
            new_filename = f"{date_prefix}-{pdf_file.name}"
            doc_type_dir = {
                "NEW": "requests",
                "RENEW": "renewals",
                "CERT": "approvals",
                "TONNAGE": "tonnage"
            }.get(parsed["base_type"], "requests")
            
            # Copy file to appropriate directory
            target_path = labeled_dir / doc_type_dir / new_filename
            shutil.copy2(pdf_file, target_path)
            
            # Generate and store metadata
            metadata[new_filename] = generate_document_metadata(
                new_filename, 
                target_path,
                creation_date
            )
            
        except Exception as e:
            print(f"Error processing {pdf_file}: {e}")
            continue
    
    # Save updated metadata
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
</file>

<file path="tests/conftest.py">
"""
Test configuration and fixtures for the document classification system.
"""
import os
import pytest
from pathlib import Path
from typing import Dict, List, Optional
import yaml
import sys
import shutil
import json
from unittest.mock import MagicMock
from .utils.document_helpers import update_test_documents

# Add mocks directory to Python path
MOCKS_DIR = Path(__file__).parent / "mocks"
sys.path.insert(0, str(MOCKS_DIR))

# Mock external dependencies
try:
    import google.generativeai as genai
except ImportError:
    genai = MagicMock()
    sys.modules['google.generativeai'] = genai

@pytest.fixture
def labeled_documents_dir(workspace_root) -> Path:
    """
    Access the labeled documents directory containing real PDFs and their expected classifications.
    The directory structure should be:
    
    tests/fixtures/labeled_documents/
    ├── metadata.json            # Contains expected classifications for each document
    ├── approvals/              # Documents organized by type
    │   ├── doc1.pdf
    │   └── doc2.pdf
    ├── denials/
    │   └── doc3.pdf
    └── requests/
        └── doc4.pdf
    """
    # Set up directories
    documents_dir = workspace_root / "tests" / "fixtures" / "documents"
    labeled_dir = workspace_root / "tests" / "fixtures" / "labeled_documents"
    
    if not labeled_dir.exists():
        labeled_dir.mkdir(parents=True)
        
    # Update test documents and metadata
    update_test_documents(documents_dir, labeled_dir)
    
    return labeled_dir

@pytest.fixture
def document_metadata(labeled_documents_dir) -> Dict:
    """Get the metadata for labeled test documents."""
    metadata_file = labeled_documents_dir / "metadata.json"
    if metadata_file.exists():
        with open(metadata_file) as f:
            return json.load(f)
    return {}

@pytest.fixture
def test_config_dir(tmp_path) -> Path:
    """Create a temporary config directory with actual configuration files."""
    config_dir = tmp_path / "config"
    config_dir.mkdir()
    
    # Get the actual config directory path
    src_config_dir = Path(__file__).parents[1] / "src" / "config"
    
    # Copy all YAML files from the actual config directory
    for yaml_file in src_config_dir.glob("*.yaml"):
        if yaml_file.is_file():
            # Read the original file
            with open(yaml_file) as f:
                config = yaml.safe_load(f) or {}
            
            # Add version if it doesn't exist
            if "version" not in config:
                config["version"] = "1.0.0"
            
            # Write to the test directory
            with open(config_dir / yaml_file.name, 'w') as f:
                yaml.dump(config, f)
    
    # Create version control file
    version_control = {
        "version_control": {
            "min_compatible_version": "1.0.0",
            "current_versions": {
                "document_types": "1.0.0",
                "state_patterns": "1.0.0",
                "company_codes": "1.0.0",
                "regulatory_actions": "1.0.0",
                "product_categories": "1.0.0",
                "validation_rules": "1.0.0",
                "relationships": "1.0.0",
                "status_workflows": "1.0.0",
                "email_classification": "1.0.0",
                "clients": "1.0.0"
            },
            "migrations_required": {}
        }
    }
    
    with open(config_dir / "version_control.yaml", 'w') as f:
        yaml.dump(version_control, f)
            
    return config_dir

@pytest.fixture
def test_documents_dir(tmp_path) -> Path:
    """Create a temporary directory with test documents."""
    docs_dir = tmp_path / "documents"
    docs_dir.mkdir()
    
    # Create test documents
    documents = {
        "test_license.txt": """
        ARB License Application
        State of Alabama
        License Number: LIC-2024-001
        Date: 2024-02-11
        """,
        "test_registration.txt": """
        BIN Registration Form
        California Department
        Registration: REG-2024-002
        Amount: $500.00
        """
    }
    
    for filename, content in documents.items():
        with open(docs_dir / filename, 'w') as f:
            f.write(content)
            
    return docs_dir

@pytest.fixture
def mock_email_message():
    """Create a mock email message for testing."""
    from email.message import EmailMessage
    
    msg = EmailMessage()
    msg["Subject"] = "ARB AL License Renewal 2024-25"
    msg["From"] = "test@example.com"
    msg["Date"] = "Mon, 11 Feb 2024 15:30:00 -0500"
    msg["Message-ID"] = "<test123@example.com>"
    msg.set_content("""
    Please find attached our license renewal application for Alabama.
    
    Best regards,
    Test Company
    """)
    
    # Add a test attachment
    attachment_content = """
    ARB License Renewal
    State of Alabama
    License Number: LIC-2024-003
    Date: 2024-02-11
    """
    msg.add_attachment(
        attachment_content.encode(),
        maintype="application",
        subtype="pdf",
        filename="renewal.pdf"
    )
    
    return msg

@pytest.fixture
def mock_gemini_response():
    """Create a mock Gemini API response."""
    return {
        "document_type": "license",
        "entities": {
            "companies": ["ARB"],
            "products": ["Fertilizer"],
            "states": ["AL"]
        },
        "key_fields": {
            "dates": ["2024-02-11"],
            "registration_numbers": ["LIC-2024-001"],
            "amounts": []
        },
        "summary": "License application for ARB in Alabama",
        "text": "Full document text would go here"
    }

@pytest.fixture
def mock_docling_doc():
    """Create a mock Docling document object."""
    class MockDoc:
        def __init__(self):
            self.page_count = 1
            self.has_tables = False
            self.extraction_confidence = 0.85
            
        def get_text(self):
            return """
            ARB License Application
            State of Alabama
            License Number: LIC-2024-001
            Date: 2024-02-11
            """
            
        def get_summary(self):
            return "License application for ARB in Alabama"
            
        def extract_entities(self, entity_type):
            if entity_type == "ORG":
                return ["ARB"]
            elif entity_type == "PRODUCT":
                return ["Fertilizer"]
            return []
            
        def extract_dates(self):
            return ["2024-02-11"]
            
        def extract_patterns(self, pattern):
            if "REG-" in pattern or "LIC-" in pattern:
                return ["LIC-2024-001"]
            elif r"\$" in pattern:
                return []
            return []
            
    return MockDoc()

@pytest.fixture(scope="session")
def workspace_root():
    """Get the workspace root directory."""
    return Path(__file__).parents[1]

@pytest.fixture(autouse=True)
def mock_env(monkeypatch):
    """Mock environment variables."""
    monkeypatch.setenv("GOOGLE_API_KEY", "test_key")
    monkeypatch.setenv("TESTING", "true")
</file>

<file path="tests/main.py">
import pytest

def test_sample():
    assert True
</file>

<file path="tests/README.md">
# Document Classification Test Suite

This directory contains the test suite for the document classification system. The tests are organized into unit tests and integration tests to ensure both individual components and the system as a whole work correctly.

## Directory Structure

```
tests/
├── unit/                 # Unit tests for individual components
│   ├── test_base_classifier.py
│   ├── test_domain_config.py
│   ├── test_classifier_factory.py
│   ├── test_gemini_classifier.py
│   └── test_docling_classifier.py
├── integration/          # Integration tests for end-to-end functionality
│   └── test_classifier_integration.py
├── fixtures/            # Test data and fixtures
│   └── documents/       # Test PDF documents
└── conftest.py         # Shared test configuration and fixtures
```

## Test Data

To run the integration tests, you need to add test PDF documents to the `fixtures/documents/` directory. These documents should represent various types of regulatory documents that the system needs to classify.

Example document types:
- Registration applications
- Renewal forms
- Amendments
- Tonnage reports

## Running Tests

### Prerequisites

1. Install test dependencies:
```bash
pip install -r requirements/requirements-dev.txt
```

2. Set up environment variables:
```bash
export GOOGLE_API_KEY=your_api_key  # Required for Gemini classifier tests
```

### Running All Tests

```bash
pytest tests/
```

### Running Specific Test Categories

Run unit tests only:
```bash
pytest tests/unit/
```

Run integration tests only:
```bash
pytest tests/integration/
```

Run tests for a specific classifier:
```bash
pytest tests/unit/test_gemini_classifier.py
pytest tests/unit/test_docling_classifier.py
```

### Test Configuration

The test suite uses a temporary configuration directory with test-specific domain rules and patterns. You can modify these configurations in `conftest.py` if needed.

## Adding New Tests

When adding new test cases:

1. Unit tests:
   - Add test files to the `unit/` directory
   - Focus on testing individual components in isolation
   - Use mocking for external dependencies

2. Integration tests:
   - Add test files to the `integration/` directory
   - Test end-to-end workflows
   - Use real PDF documents from the fixtures directory

3. Test data:
   - Add test documents to `fixtures/documents/`
   - Include a variety of document types
   - Document the expected classification results

## Test Coverage

To generate a test coverage report:

```bash
pytest --cov=src tests/
```

## Continuous Integration

The test suite is integrated with our CI/CD pipeline. The following checks are performed:

1. All unit tests must pass
2. Integration tests must pass when API keys are available
3. Code coverage must meet minimum thresholds
4. Code style must follow project guidelines

## Contributing

When contributing new features or bug fixes:

1. Add appropriate test cases
2. Ensure all tests pass locally
3. Maintain or improve code coverage
4. Follow existing test patterns and naming conventions
</file>

<file path=".codecov.yml">
coverage:
  status:
    project:
      default:
        target: 80%    # the required coverage value
        threshold: 1%  # the leniency in hitting the target
    patch:
      default:
        target: 80%
        threshold: 1%

comment:
  layout: "reach, diff, flags, files"
  behavior: default
  require_changes: false

ignore:
  - "tests/**/*"
  - "setup.py"
</file>

<file path=".env.example">
DEBUG=True
API_KEY=your_key_here

# Gmail API Credentials
GMAIL_CLIENT_ID=your_client_id
GMAIL_CLIENT_SECRET=your_client_secret
GMAIL_REFRESH_TOKEN=your_refresh_token
GMAIL_TOKEN_URI=https://oauth2.googleapis.com/token
CODECOV_TOKEN=your_codecov_token
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg

# Virtual Environment
.env
.venv
.venv-3.11
env/
venv/
ENV/

# IDE
.idea/
.vscode/
*.swp
*.swo

# System and Cache Files
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db
*.log
*.cache

# Testing
.coverage
.pytest_cache/
htmlcov/
coverage.xml
junit.xml
test-results/

# Project specific
output/*
!output/.gitkeep

# Google API
secrets/*

# notes
/notes/*

.op/

/data
/data/clients/*.numbers
/data/clients
data/clients/delta-client-labels.numbers

# Secrets
secrets/
*.json
secrets/*.json
credentials/*.json

#notebooks
notebooks/*

vertex-ai-samples/

.cursorrules
/context/**
/src/config/**
tests/fixtures/documents/**
docs/4-schemas/**

# Documentation
/docs/
/docs/**

# Additional directories to ignore
/context/**
/src/config/**
tests/fixtures/documents/**
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v3.2.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-added-large-files

  - repo: local
    hooks:
      - id: branch-check
        name: branch name check
        entry: python scripts/check_branch.py
        language: system
        pass_filenames: false
</file>

<file path=".python-version">
3.11.7
</file>

<file path=".repomixignore">
# Add patterns to ignore here, one per line
# Example:
# *.log
# tmp/
</file>

<file path=".sourcery.yaml">
version: '1'
ignore:
  - .git
  - venv
  - env
rules:
  - name: use-fstring
    enabled: true
  - name: merge-duplicate-blocks
    enabled: true
  - name: inline-immediately-returned-variable
    enabled: true
  - name: move-assign-in-if-to-if-exp
    enabled: true
  - name: use-assigned-variable
    enabled: true
  - name: no-loop-in-tests
    enabled: false
</file>

<file path="base.txt">
# Base requirements
requests>=2.28.0
pyyaml>=6.0
structlog>=22.1.0
google-api-python-client>=1.7.3
google-cloud-aiplatform>=1.35.0
beautifulsoup4>=4.0.0
python-dateutil>=2.8.1
oauth2client>=4.1.3
lxml>=4.4.2
simplegmail>=3.1.0
Pillow>=10.1.0
python-dotenv>=1.0.0
matplotlib>=3.8.0

# Add to existing requirements
google-generativeai==0.3.2
python-multipart==0.0.9

# Add Docling and its dependencies
docling>=1.0.0  # Note: Update version as needed
torch>=2.0.0    # Required for Docling's ML components
</file>

<file path="dev.txt">
# Dev requirements
-r base.txt
black>=23.12.0
flake8>=4.0.1
mypy>=1.8.0
pre-commit>=2.20.0
types-Pillow>=10.1.0
types-requests>=2.31.0
types-python-dateutil>=2.8.19
</file>

<file path="Makefile">
# Makefile for project management

.PHONY: setup test lint format clean

setup:
	pip install -r requirements/dev.txt

test:
	pytest tests/ --cov=src

lint:
	flake8 src/
	mypy src/

format:
	black src/ tests/

clean:
	find . -type d -name "__pycache__" -exec rm -rf {} +
	find . -type f -name "*.pyc" -delete
	find . -type f -name "*.pyo" -delete
	find . -type f -name "*.pyd" -delete
	find . -type f -name ".coverage" -delete
	find . -type d -name "*.egg-info" -exec rm -rf {} +
	find . -type d -name "*.egg" -exec rm -rf {} +
	find . -type d -name ".pytest_cache" -exec rm -rf {} +
	find . -type d -name ".mypy_cache" -exec rm -rf {} +
</file>

<file path="mypy.ini">
[mypy]
ignore_missing_imports = True
disallow_untyped_defs = True
check_untyped_defs = True

[mypy.plugins.django.*]
init_typed = True
</file>

<file path="plan.md">
# Delta Inbox
## Email Parsing and AI Labeling Integration Plan

### Project Scope and Requirements

Objectives:

	•	Email Monitoring: Develop an application that continuously monitors the biofertregistration@delta-ac.com inbox.
	•	Security Vetting: Implement a mechanism to vet emails for suspicious intent (e.g., phishing, malware).
	•	Content Extraction: Extract relevant information from email bodies and attachments.
	•	Classification and Categorization: Classify/label/tag emails and attachments based on their content and map them to the appropriate entities our Airtable schema.
	•	Data Integration: Store or update records in Airtable and an SQL database based on the extracted information. Update Gmail and Gmelius labels based on the classification.
	•	Notifications: Send alerts or notifications to relevant team members when specific types of emails are received.

Tech Stack:
- Phase 1: 
	•	Gmail API
	•	Gmelius API
	•	Airtable API
- Phase 2: 
	•	SQL Alchemy ORM
	•	SQL Database (Postgres)

### Email Content and Types

Common Email Types:

	•	Regulatory Communications: Updates from state and federal agencies.
	•	Client Correspondence: Emails from clients regarding products, registrations, renewals, etc.
	•	Invoices and Payments: Financial documents and confirmations.
	•	Attachments: Forms, certificates, regulatory documents, product labels, etc.

Key Data to Extract:

	•	Client Information: Company name, contact details, product codes.
	•	Product Details: Product names, versions, registration numbers.
	•	Regulatory Data: Registration statuses, renewal dates, compliance requirements.
	•	Financial Information: Invoice numbers, payment amounts, dates.

3. Map Email Data to Schema Entities

Entities to Consider:

	•	Client List: For client-related information and updates.
	•	Products: For product-specific details and changes.
	•	Registration Tracking: For updates on product registrations and compliance statuses.
	•	Payments: For financial transactions and payment confirmations.
	•	Invoices: For billing and invoicing records.
	•	Reg Reqs (Regulatory Requirements): For changes in regulatory policies or requirements.

Mapping Strategy:

	•	Email Sender/Recipient: Determine if the email is from a known client or regulatory body.
	•	Subject Line Keywords: Use keywords to categorize the email (e.g., “Renewal Notice,” “Payment Confirmation”).
	•	Attachment Types: Identify attachment formats to determine processing method (e.g., PDFs for forms, Excel files for data).

4. Design the Application Architecture

Components:

	1.	Email Retrieval Module:
	•	Technology: Use Gmail API with OAuth 2.0 authentication.
	•	Functionality: Fetch new emails from the inbox at regular intervals.
	2.	Security Vetting Module:
	•	Spam Detection: Utilize libraries like SpamAssassin or APIs like Google’s Safe Browsing.
	•	Malware Scanning: Integrate with antivirus APIs or services (e.g., VirusTotal).
	3.	Content Processing Module:
	•	Email Parsing: Extract text from email bodies using MIME parsers.
	•	Attachment Handling: Use file type-specific libraries (e.g., PyPDF2 for PDFs, openpyxl for Excel files).
	•	Natural Language Processing (NLP): Use NLP libraries like spaCy or NLTK to extract entities.
	4.	Classification Module:
	•	Rule-Based Classification: Define rules based on keywords, sender, and content patterns.
	•	Machine Learning (Optional): Train a model if email patterns are complex.
	5.	Data Integration Module:
	•	Airtable API Interaction: Use Airtable’s API to create or update records.
	•	Data Validation: Ensure data matches expected formats and types in the schema.
	6.	Logging and Error Handling Module:
	•	Logging: Record processing steps, errors, and actions taken.
	•	Alerts: Notify administrators of critical issues.
	7.	User Interface (Optional):
	•	Dashboard: Display processing status, recent activities, and analytics.
	•	Manual Overrides: Allow users to review and correct data before it’s committed.

5. Select Technologies and Tools

Programming Language: Python is recommended due to its rich ecosystem and library support.

Libraries and Frameworks:

	•	Email Access: google-api-python-client for Gmail API.
	•	Email Parsing: tbd. considering options... email module, imaplib, mail-parser.
	•	Gmelius API: existing framework for sharing emails and labels with team members within the gmail account. 
	•	NLP: tbd. considering options...
	•	PDF Processing: tbd. considering options...
	•	Excel Processing: openpyxl, pandas.
	•	HTTP Requests: requests library for API interactions.
	•	Airtable Integration: airtable-python-wrapper or direct API calls.

Security Tools:

	•	Spam Detection: scikit-learn for custom models, or integrate with services.
	•	Malware Scanning: APIs like VirusTotal.

1. Develop the Email Retrieval Module

Steps:

	1.	Set Up Gmail API Access:
	•	Create a Google Cloud project and enable the Gmail API.
	•	Configure OAuth 2.0 credentials.
	2.	Implement Email Fetching:
	•	Connect to the inbox and fetch unread emails.
	•	Mark emails as read or move them to a processed folder after handling.

7. Implement Security Vetting

Email Vetting Process:

	•	Check for Phishing Links: Scan email content for suspicious URLs.
	•	Verify Sender Authenticity: Check SPF, DKIM, and DMARC records (handled by Gmail but can add extra checks).
	•	Attachment Scanning:
	•	Only allow processing of specific file types.
	•	Scan attachments using antivirus software or services.

8. Develop Content Processing Logic

Email Parsing:

	•	Extract Metadata: Sender, recipient, subject, date.
	•	Extract Body Text: Handle both plain text and HTML content.
	•	Handle Encodings: Ensure correct text encoding (e.g., UTF-8).

Attachment Processing:

	•	PDFs:
	•	Use PyPDF2 or pdfminer.six to extract text.
	•	If PDFs are scanned images, use OCR with pytesseract.
	•	Word Documents:
	•	Use python-docx to read .docx files.
	•	Excel Files:
	•	Use openpyxl or pandas to read data.

9. Build the Classification Module

Rule-Based Classification:

	•	Define Rules:
	•	Use keyword matching in subject lines and body text.
	•	Example: If the subject contains “Renewal,” classify as a renewal notice.
	•	Implement Regex Patterns:
	•	Extract dates, registration numbers, client names.

Machine Learning Classification (Optional):

	•	Data Collection:
	•	Gather labeled examples of different email types.
	•	Model Training:
	•	Use scikit-learn or TensorFlow to train a classifier.

10. Integrate with Airtable Schema

Using Airtable API:

	•	Authentication:
	•	Use API keys to authenticate requests.
	•	Data Operations:
	•	Create Records: For new clients, products, or registrations.
	•	Update Records: Modify existing entries with new information.
	•	Search Records: Find records to update based on unique identifiers (e.g., client code).

Data Mapping:

	•	Define Field Mappings:
	•	Map extracted data fields to Airtable fields.
	•	Ensure data types are compatible (e.g., dates, numbers, text).
	•	Handle Linked Records:
	•	Use record IDs to link to related tables (e.g., linking a payment to a client).

Error Handling:

	•	API Rate Limits:
	•	Implement retry logic and respect Airtable’s rate limits.
	•	Data Validation Errors:
	•	Log and handle cases where data doesn’t meet schema requirements.

11. Test the Application Thoroughly

Testing Types:

	•	Unit Tests:
	•	Test individual functions and modules.
	•	Integration Tests:
	•	Test end-to-end processing from email retrieval to Airtable integration.
	•	Security Tests:
	•	Simulate malicious emails to test vetting mechanisms.
	•	User Acceptance Testing:
	•	Involve team members to validate the application’s performance with real-world data.

12. Deploy the Application

Deployment Options:

	•	Cloud Platforms:
	•	Use services like AWS (Lambda, EC2), Google Cloud Platform, or Azure.
	•	On-Premises Server:
	•	Deploy on a local server if required by company policy.

Continuous Integration/Continuous Deployment (CI/CD):

	•	Automate Deployments:
	•	Use tools like Jenkins, GitHub Actions, or GitLab CI/CD.
	•	Version Control:
	•	Use Git for code management and collaboration.

13. Monitor and Maintain the Application

Monitoring Tools:

	•	Logging:
	•	Use logging frameworks to capture application logs.
	•	Alerts:
	•	Set up email or SMS alerts for critical failures.
	•	Performance Monitoring:
	•	Use tools like Prometheus and Grafana for metrics.

Maintenance Tasks:

	•	Regular Updates:
	•	Keep dependencies and libraries up to date.
	•	Security Patches:
	•	Apply security updates promptly.
	•	Backup Data:
	•	Ensure that data in Airtable or your database is regularly backed up.

14. Document and Train

Documentation:

	•	Technical Documentation:
	•	Document code, APIs used, data flows, and configurations.
	•	User Guides:
	•	Provide instructions for team members on how to interact with the application.

Training:

	•	Workshops:
	•	Conduct sessions to demonstrate the application’s features.
	•	Support:
	•	Establish a support channel for questions and troubleshooting.

15. Plan for Future Enhancements

Potential Improvements:

	•	Automated Responses:
	•	Send acknowledgment emails or predefined replies.
	•	Advanced Analytics:
	•	Analyze email trends, common issues, and client interactions.
	•	Machine Learning Enhancements:
	•	Improve classification accuracy with more data and training.
	•	Integration with Other Systems:
	•	Connect with CRM systems, accounting software, or regulatory databases.

Next Steps

	1.	Project Kickoff:
	•	Assemble a development team.
	•	Assign roles and responsibilities.
	2.	Create a Project Plan:
	•	Define milestones, deliverables, and timelines.
	3.	Set Up Development Environment:
	•	Configure necessary tools and access permissions.
	4.	Start Development Iteratively:
	•	Follow agile methodologies to develop in sprints.
	•	Regularly review progress with stakeholders.
	5.	Engage Stakeholders:
	•	Keep communication open with end-users and management.
	•	Incorporate feedback early and often.

Additional Considerations

	•	Compliance and Data Privacy:
	•	Ensure compliance with data protection regulations (e.g., GDPR, CCPA).
	•	Implement data encryption and secure storage practices.
	•	Error Recovery:
	•	Design the system to handle failures gracefully.
	•	Implement retry mechanisms where appropriate.
	•	Scalability and Performance:
	•	Build the application to handle increased email volumes.
	•	Optimize for performance in parsing and processing.
</file>

<file path="pytest.ini">
[pytest]
# Set asyncio loop scope to function level
asyncio_mode = auto
asyncio_default_fixture_loop_scope = function

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Basic options
addopts = --verbose

# Environment variables
env =
    PYTHONPATH=.
    TESTING=true

# Markers
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Tests that take longer to run

# Filter warnings
filterwarnings =
    ignore::DeprecationWarning
    ignore::pytest.PytestDeprecationWarning

log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(message)s (%(filename)s:%(lineno)s)
log_cli_date_format = %Y-%m-%d %H:%M:%S
</file>

<file path="requirements-dev.txt">
# Test dependencies
pytest>=8.0.0
pytest-asyncio>=0.25.2
pytest-cov>=4.1.0
pytest-mock>=3.12.0
PyPDF2>=3.0.0

# Classifier dependencies
google-generativeai>=0.3.2
docling>=1.0.0
torch>=2.0.0  # Required for Docling

# Gmail API dependencies
google-api-python-client>=2.0.0
google-auth-oauthlib>=0.4.6
google-auth-httplib2>=0.1.0
simplegmail>=3.1.0

# Google Cloud dependencies
google-cloud-aiplatform>=1.0.0

# Development tools
black>=23.0.0
isort>=5.12.0
flake8>=6.1.0
mypy>=1.7.0
click>=8.1.0  # Required for CLI tools
rich>=10.0.0  # Required for CLI UI
questionary>=2.0.0  # Required for interactive prompts
pydantic>=2.0.0

# Type stubs
types-PyYAML
types-python-dateutil

# Additional dependencies
types-Pillow==10.1.0.2
types-requests==2.31.0.20240106
</file>

<file path="requirements-test.txt">
pytest==8.0.0
pytest-asyncio==0.23.5
pytest-cov==4.1.0
pytest-mock==3.12.0
pytest-timeout==2.2.0
pytest-xdist==3.5.0
click>=8.0.0
watchdog>=3.0.0
pyyaml>=6.0.0
google-generativeai>=0.3.0
</file>

<file path="requirements.txt">
click>=8.0.0
questionary>=2.0.0
rich>=13.0.0
PyYAML>=6.0.0
</file>

<file path="test.txt">
# Test requirements
-r base.txt
pytest>=8.0.0
pytest-cov>=4.1.0
pytest-mock>=3.12.0
pytest-asyncio>=0.23.0
coverage>=7.3.0
</file>

</files>
